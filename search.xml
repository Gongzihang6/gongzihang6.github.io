<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="/2024/04/10/jing-dian-juan-ji-shen-jing-wang-luo/"/>
      <url>/2024/04/10/jing-dian-juan-ji-shen-jing-wang-luo/</url>
      
        <content type="html"><![CDATA[<h3 id="经典卷积神经网络"><a href="#经典卷积神经网络" class="headerlink" title="经典卷积神经网络"></a>经典卷积神经网络</h3><h4 id="1、AlexNet（2012）"><a href="#1、AlexNet（2012）" class="headerlink" title="1、AlexNet（2012）"></a>1、AlexNet（2012）</h4><p>ImageNet Classification with Deep Convolutional Neural Networks</p><p><strong>主体贡献</strong>：</p><ul><li>提出了卷积层加全连接层的卷积神经网络结构</li><li>首次使用ReLU函数作为神经网络的激活函数</li><li>首次提出Dropout正则化来控制过拟合</li><li>使用加入动量的小批量梯度下降算法加速了训练过程的收敛</li><li>使用了数据增强策略极大地抑制了训练过程的过拟合</li><li>利用了GPU的并行计算能力，加速了网络的训练与推断</li></ul><p><img src="/2024/04/10/jing-dian-juan-ji-shen-jing-wang-luo/AlexNet.png" alt="AlexNet示意图,上下两部分表示使用两个GPU并行计算"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">输入为3*224*224输出：参数量：计算量(FLOPS)：</span><br><span class="line">conv111*11*3*96s496*55*5596*(3*11*11+1)96*55*55*11*11*3</span><br><span class="line">maxpool13*3s296*27**27</span><br><span class="line">LRN14</span><br><span class="line">conv25*5*96*256s1 p2256*27*27256*(96*5*5+1)256*27*27*5*5*96</span><br><span class="line">maxpool23*3s2256*13*13</span><br><span class="line">LRN24</span><br><span class="line">conv33*3*256*384s1 p1384*13*13384*(256*3*3+1)384*13*13*3*3*256</span><br><span class="line">conv43*3*384*384s1 p1384*13*13384*(384*3*3+1)384*13*13*3*3*384</span><br><span class="line">conv53*3*384*256s1 p1256*13*13256*(384*3*3+1)256*13*13*3*3*384</span><br><span class="line">maxpool33*3s2256*6*6</span><br><span class="line">fc69216*40964096*19216*40969216*4096</span><br><span class="line">fc74096*40964096*14096*40964096*4096</span><br><span class="line">fc84096*10001000*14096*10004096*1000</span><br></pre></td></tr></table></figure><p>值得注意的一点：原图输入224 × 224，实际上进行了<strong>随机裁剪</strong>，实际大小为227 × 227。</p><p>卷积层参数量计算公式：卷积核高* 卷积核宽* 输入通道数* 输出通道数</p><p>卷积层计算量计算公式：输出feature map的高* 输出feature map的宽* 输出feature map的通道数* 卷积核高* 卷积核宽* 输入通道数</p><p>LRN，局部响应归一化层，作用是对局部神经元的活动创建竞争机制；响应比较大的值变得相对更大；抑制其他反馈较小的神经元；增强模型的泛化能力；计算公式如下：<br>$$<br>b_{xy}^i &#x3D; a_{xy}^i &#x2F; (k + α∑_{j&#x3D;max(0,i-n&#x2F;2)}^{min(N-1,i+n&#x2F;2)} (a_{xy}^j)^2) ^ β<br>$$<br>其中：</p><ul><li><p>$$\alpha_{xy}^i$$: ReLU处理后的神经元，作为LRN的输入</p></li><li><p>$$b_{xy}^i$$: LRN的输出，LRN处理后的神经元</p></li><li><p>N：kernal总数或通道数</p></li><li><p>$k$、$n$、$\alpha$、$\beta$：常量，超参数，k类似于bias，n对应于Caffe中的local_size，在论文中这几个值分别为2、5、1e-4、0.75。</p></li></ul><h4 id="2、ZFNet"><a href="#2、ZFNet" class="headerlink" title="2、ZFNet"></a>2、ZFNet</h4><p>主要改进：错误率由16.4%–&gt;11.7%</p><ul><li><p>将第一个卷积层（11* 11滤波器，s&#x3D;4）的卷积核大小改为了7* 7，s&#x3D;2；</p></li><li><p>将第二、第三个卷积层的卷积步长都设置为2</p></li><li><p>conv3、4、5的卷积核个数改为512、1024、512</p><img src="/2024/04/10/jing-dian-juan-ji-shen-jing-wang-luo/ZFNet.png" alt="ZFNet示意图(图中conv3、4、5的通道数不正确，已经改为了512、1024、512)" style="zoom:150%;"></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">输入为3*224*224输出：参数量：计算量(FLOPS)：</span><br><span class="line">conv17*7*3*96s296*110*1107*7*3*9696*110*110*7*7*3</span><br><span class="line">maxpool13*3s296*55*55</span><br><span class="line">LRN196*55*554</span><br><span class="line">conv25*5*96*256s2 p1256*26*265*5*96*256256*26*26*5*5*96</span><br><span class="line">maxpool23*3s2256*13*13</span><br><span class="line">LRN2256*13*134</span><br><span class="line">conv33*3*256*512s1 p1512*13*133*3*256*512512*13*13*3*3*256</span><br><span class="line">conv43*3*512*1024 s1 p11024*13*133*3*512*10241024*13*13*3*3*512</span><br><span class="line">conv53*3*1024*512 s1 p1512*13*133*3*1024*512512*13*13*3*3*1024</span><br><span class="line">maxpool33*3s2512*6*6</span><br><span class="line">fc618432*40964096*118432*409618432*4096</span><br><span class="line">fc74096*40964096*14096*40964096*4096</span><br><span class="line">fc84096*10001000*14096*10004096*1000</span><br></pre></td></tr></table></figure><p>ZFNet的主要研究贡献：</p><p>（1）特征可视化：利用反卷积、反池化、反激活函数去反可视化feature map，通过feature map可以看出特征分层次体系结构，以及我们可以知道前面的层学习的是物理轮廓、边缘、颜色、纹理等特征，后面的层学习的是和类别相关的抽象特征。这一个非常重要的结论，我们后面很多的微调手段都是基于此理论成果。再次强调这个结论：</p><ul><li><p>结论一：CNN网络前面的层学习的是物理轮廓、边缘、颜色、纹理等特征，后面的层学习的是和类别相关的抽象特征</p></li><li><p>结论二：CNN学习到的特征具有平移和缩放不变性，但是，没有旋转不变性</p></li></ul><p>（2）特征提取的通用性：作者通过实验说明了，将使用ImageNet2012数据集训练得到的CNN,在保持前面七层不变的情况，只在小数据集上面重新训练softmax层，通过这样的实验，说明了使用ImageNet2012数据集训练得到的CNN的特征提取功能就有通用性。</p><ul><li>结论三：CNN网络的特征提取具有通用性，这是后面微调的理论支持</li></ul><p>（3）对于遮挡的敏感性：通过遮挡，找出了决定图像类别的关键部位当，在输入图片中遮挡了学习到的特征时，分类效果会很差。同时根据遮挡图像局部对分类结果的影响来探讨对分类任务而言到底那部分输入信息更重要。</p><p>（4）特征的层次分析：作者通过实验证明了，不同深度的网络层学习到的特征对于分类的作用，说明了深度结构确实能够获得比浅层结构更好的效果。 通过实验，说明了深度增加时，网络可以学习到更具区分的特征。 底层特征在迭代次数比较少时就能收敛，高层需要的迭代次数比较多 越是高层的特征，其用来做分类的性能越好</p><h4 id="3、VGGNet-2014"><a href="#3、VGGNet-2014" class="headerlink" title="3、VGGNet(2014)"></a>3、VGGNet(2014)</h4><p>论文：<a href="https://arxiv.org/abs/1409.1556">Very deep convolutional networks for large-scale image recongnition</a></p><p><img src="/2024/04/10/jing-dian-juan-ji-shen-jing-wang-luo/VGG16.png" alt="以VGG16为例，13卷积层+3全连接层"></p><p>其中卷积核均为3*3的，步长为1，padding也为1，卷积不改变图像尺寸，只在最大池化层设置2 *2，步长为2，每经过一层pool，图像高宽减半，通道数不变；全连接层使用了dropout策略。</p><p>网络结构中全部采用3* 3卷积核级联，放弃采用大卷积核，理由是在获得相同大小的感受野的情况下，3* 3的小卷积核级联比大卷积核的参数更少、计算量更小，同时网络更深、更非线性。</p><p>为什么VGG网络前四段里，每经过一次池化操作，卷积核个数就增加一倍？</p><ol><li>池化操作可以减小特征图尺寸，降低显存占用</li><li>增加卷积核个数有助于学习更多的结构特征，但会增加网络参数量以及内存消耗（这也就是为什么第五段没有继续增加卷积核个数，减少参数，也防止过拟合）</li><li>一减一增的设计平衡了识别精度与存储、计算开销</li></ol><h4 id="4、GoogLeNet-2014"><a href="#4、GoogLeNet-2014" class="headerlink" title="4、GoogLeNet(2014)"></a>4、GoogLeNet(2014)</h4><p>主要创新点：</p><ul><li>提出了一种Inception结构，能够保留输入信号中的更多特征信息；</li><li>去掉了AlexNet的前两个全连接层，并采用了平均池化，这一设计使得GoogLeNet只有500万参数，比AlexNet少了12倍；</li><li>在网络的中部引入了辅助分类器，克服了训练过程中的梯度消失问题。</li></ul><p><img src="/2024/04/10/jing-dian-juan-ji-shen-jing-wang-luo/Googlenet.png" alt="GoogLeNet示意图"></p><p>网络主体部分由9个Inception模块构成，在第三个、第六个Inception模块分出两个辅助分类器，帮助底层神经网络参数更新，避免梯度消失。训练的时候，3个分类器一起参与训练，相当于有3个梯度流，最底层梯度为3个梯度流的和，有助于底层参数的更新。但推理时只采用最后一个分类器的输出。</p><img src="/2024/04/10/jing-dian-juan-ji-shen-jing-wang-luo/最初Inception模块.png" alt="最初Inception模块" style="zoom: 67%;"><img src="/2024/04/10/jing-dian-juan-ji-shen-jing-wang-luo/改进Inception模块.png" alt="改进Inception模块" style="zoom: 67%;"><p>Inception模块对输入分出四个分支，不改变图像高宽，只改变通道数，然后将四个分支在通道维拼接起来作为输出。对输入，采用1* 1卷积降维，再3* 3卷积升维。（1* 1卷积减少通道数并不会丢失信息）</p><p>GoogLeNet分类器最后采用的是平均池化层，丢失了语义结构的位置信息，有助于提升卷积层提取到的特征的平移不变性。</p><h4 id="5、ResNet（2015）"><a href="#5、ResNet（2015）" class="headerlink" title="5、ResNet（2015）"></a>5、ResNet（2015）</h4><p>论文： <a href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">Deep Residual Learning for Image Recognition, Identity Mappings in Deep Residual Networks</a></p><p>主要创新：</p><ul><li><p>提出了一种残差模块，通过堆叠残差模块可以构建任意深度的神经网络，而不会出现“退化”现象</p></li><li><p>提出了批归一化方法来对抗梯度消失，该方法降低了网络训练过程对于权重初始化的依赖</p></li><li><p>提出了一种针对ReLU激活函数的初始化方法</p><p><img src="/2024/04/10/jing-dian-juan-ji-shen-jing-wang-luo/ResNet.png" alt="ResNet示意图"></p></li></ul><p>残差结构，输入为$x$，输出为$H(x)$，而$H(x)&#x3D;F(x)+x$，所以$F(x)&#x3D;H(x)-x$，因此称$F(x)$为残差。</p><p>完整ResNet架构：初始卷积层、堆叠残差块、每个残差快有2个3* 3卷积层、周期性使用步长2的卷积，将特征响应图高宽减少一半、只有fc1000用于输出类别</p><p><img src="/2024/04/10/jing-dian-juan-ji-shen-jing-wang-luo/ResNet-Bottleneck.png" alt="ResNet-Bottleneck"></p><p><strong>为什么残差网络性能这么好？一种典型的解释：残差网络可以看作一种集成模型</strong></p><h4 id="6、R-CNN"><a href="#6、R-CNN" class="headerlink" title="6、R-CNN"></a>6、R-CNN</h4><p>实现流程示意如下：</p><p><img src="/2024/04/10/jing-dian-juan-ji-shen-jing-wang-luo/R-CNN.png" alt="R-CNN实现的示意图"></p><h5 id="6-1、区域建议：Selective-Search"><a href="#6-1、区域建议：Selective-Search" class="headerlink" title="6.1、区域建议：Selective Search"></a>6.1、区域建议：Selective Search</h5><p>对于region proposal的要求：</p><ul><li>找出所有潜在可能包含目标的区域</li><li>运行速度需要相对较快，比如，Selective Search在CPU上仅需要运行几秒钟就可以产生2000个候选区域</li></ul><p>这一步的实现方式有很多，最容易想到的就是穷举法，在图片上使用穷举法或者滑动窗口选出所有物体可能出现的区域框，就是在原始图片上进行不同尺度不同大小的滑窗，获取每个可能的位置。而这样做的缺点也显而易见，复杂度太高，产生了很多的冗余候选区域，而且由于不可能每个尺度都兼顾到，因此得到的目标位置也不可能那么准，在现实当中不可行。因此提出了Selective Serch算法，SS算法的具体流程如下：</p><ol><li>使用《Efficient Graph-Based Image Segmentation》中的方法<strong>初始化</strong>区域集 $R$ </li><li>计算 $R$ 中相邻区域的<strong>相似度</strong>，并以此构建相似度集$S$ （初始化区域中 $\forall r_i和r_j (\in R)$之间的相似度组成的集合;</li><li>如果 $S$ <strong>不为空</strong>，则执行以下7个子步骤，否则，跳至步骤4；<ol><li>获取 $S$ 中的最大值 $s(r_i,r_j)$ （表示 $r_i与r_j$之间的相似度）;</li><li>将 $r_i$ 与 $r_j$ 合并成一个新区域 $r_t$ ；</li><li>将 $S$ 中与 $r_i$ 有关的值 $s(r_i,r_*)$ 剔除掉；</li><li>将 $S$ 中与 $r_j$ 有关的值 $s(r_*,r_j)$ 剔除掉；</li><li>使用步骤2中的方法，构建 $S_t$ ，它是 $S$ 的元素与 $r_t$ 之间的相似度构成的集合；</li><li>将 $S_t$ 中的元素全部添加到 $S$ 中；</li><li>将 $r_t$ 放入 $R$ 中。</li></ol></li><li>将 $R$ 中的区域作为目标的位置框 $L$ ，这就是算法的执行结果。</li></ol><p><strong>相似度的计算</strong></p><ol><li>颜色相似度 $S_{color}(r_i,r_j)$：论文中将每个region的像素按不同颜色通道统计成直方图，其中，每个颜色通道的直方图为25 bins（比如，对于0-255的颜色通道来说，就每隔9（255&#x2F;9&#x3D;25 个数值统计像素数量）。这样，三个通道可以得到一个75维的直方图向量 $C_i&#x3D;c_i^1,\dots,c_i^n$ ,其中n&#x3D;75。之后，使用 $L1$ 范数（绝对值之和）对直方图进行归一化。由直方图我们就可以计算两个区域的颜色相似度：<br>$$<br>S_{color}(r_i,r_j)&#x3D;\sum\limits_{k&#x3D;1}^nmin(c_i^k,c_j^k)<br>$$</li></ol><p>其中n&#x3D;75，表示直方图中的bins，$c_i^k$ 表示 $r_i$ 中像素值为 $k$ 的像素个数，$c_j^k$ 表示 $r_j$ 中像素值为 $k$ 的像素个数。</p><p>2、纹理相似度（$S_{texture}(r_i,r_j)$ ：对每一个颜色通道，在8个方向上提取高斯倒数  ($\sigma$ &#x3D;1)（先对图像进行高斯滤波，然后在8个方向上进行梯度计算，得到边缘特征）。在每个颜色通道的每个方向上，提取一个bins为10的直方图（每个颜色通道的梯度图像划分为 10 个 bins，计算每个区间的梯度值数量，得到直方图），从而得到每个区域 $r_i$ 的纹理直方图向量 $T_i&#x3D;{t_i^1,\dots,t_i^n}$ ，其中n&#x3D;240（计算方式：$n_orientations\times bins \times n_channels&#x3D;8 \times 10 \times 3$ ）， $T_i$ 也是用区域的 $L_1$ 范数归一化后的向量。<strong>纹理相似度的计算公式：</strong><br>$$<br>S_{texture}(r_i,r_j)&#x3D;\sum\limits_{k&#x3D;1}^nmin(t_i^k,t_j^k)<br>$$<br>3、尺寸相似度（$S_{size}(r_i,r_j)$ ：优先合并小的区域，如果仅仅是通过颜色和纹理特征合并的话，很容易使得合并后的区域不断吞并周围的区域，后果就是多尺度只应用在了那个局部，而不是全局的多尺度。因此我们给小的区域更多的权重，这样保证在图像每个位置都是多尺度的在合并。<br>$$<br>S_{size}(r_i,r_j)&#x3D;1-\frac{size(r_i)+size(r_j)}{size(im)}<br>$$<br>4、填充相似度（$S_{fill}(r_i,r_j)$ ：填充相似度主要用来测量两个区域之间 fit 的程度，个人觉得这一点是要解决文章最开始提出的物体之间的包含关系（比如：轮胎包含在汽车上）。在给出填充相似度的公式前，我们需要定义一个矩形区域 $BB_{ij}$ ，它表示包含 $r_i$ 和 $r_j$ 的最小的 bounding box。基于此，我们给出填充相似度的计算公式为：<br>$$<br>S_{fill}(r_i,r_j)&#x3D;1-\frac{size(BB_{ij})-size(r_i)-size(r_j)}{size(im)}<br>$$<br>为了高效地计算 $BB_{ij}$ ，我们可以在计算每个region 的时候，都保存它们的 bounding box 的位置，这样，$BB_{ij}$ 就可以很快地由两个区域的 bounding box 推出来</p><p>5、总相似度计算公式：综合上面四个子公式，我们可以得到计算相似度的最终公式：<br>$$<br>S(r_i,r_j)&#x3D;a_1S_{color}(r_i,r_j)+a_2S_{texture}(r_i,r_j)+a_3S_{size}(r_i,r_j)+a_4S_{fill}(r_i,r_j)<br>$$<br>其中，$a_i$ 的取值为0或1，表示某个相似度是否被采纳。</p><h5 id="6-2、特征提取"><a href="#6-2、特征提取" class="headerlink" title="6.2、特征提取"></a>6.2、特征提取</h5><p>对6.1中提取出的ROI（region of interest），采用诸如AlexNet、VGG、ResNet等网络来提取特征，获得相应的feature map</p><h5 id="6-3、分类和边界框回归"><a href="#6-3、分类和边界框回归" class="headerlink" title="6.3、分类和边界框回归"></a>6.3、分类和边界框回归</h5><p>根据特征提取得到的feature map，使用SVM作为分类器（SVM分类器的个数由数据集类别数目决定），边界框位置和高宽等信息由Bbox reg得到。</p><p>问题在于，对于每一张图片，大约有2k个区域需要卷积网络进行特征提取，重复区域反复计算，计算效率低下。</p><h4 id="7、Fast-R-CNN"><a href="#7、Fast-R-CNN" class="headerlink" title="7、Fast R-CNN"></a>7、Fast R-CNN</h4><p>关键改进，放弃先对原始图像Selective Search，生成多个ROI，然后对每个ROI卷积进行特征提取，这样计算重复多，计算效率低，改为先对原始图像进行特征提取（如AlexNet、VGG、ResNet等），然后在得到的feature map上进行Selective Search，生成ROI，然后对生成的ROI进行裁剪+缩放特征来进行尺度的统一（比如暴力resize、按原始比例resize后pad等），然后送入全连接层，进行分类和边界框预测回归。</p><img src="/2024/04/10/jing-dian-juan-ji-shen-jing-wang-luo/Fast R-CNN.png" alt="Fast R-CNN流程示意图" style="zoom:67%;"><h5 id="裁剪-缩放特征的实现"><a href="#裁剪-缩放特征的实现" class="headerlink" title="裁剪+缩放特征的实现"></a>裁剪+缩放特征的实现</h5><img src="/2024/04/10/jing-dian-juan-ji-shen-jing-wang-luo/ROI pool.png" alt="ROI pool实现流程" style="zoom:60%;"><p>将候选区域投影到特征图上后，得到的区域大概率不规整，四个角点没有刚好落在像素点上，此时没有直接将四个角点规整到最近的像素点（这样可能导致后面最大池化时四个区域不相等，为池化操作带来麻烦），而是将该区域平均分为4个小区域，然后在每个小区域内等间距取4个像素点，当然这四个像素点也大概率不规整，因此采用双线性插值方法得到这些像素点的像素值，然后再对整个区域进行2* 2最大池化，然后进入全连接层。</p><h4 id="8、Faster-R-CNN"><a href="#8、Faster-R-CNN" class="headerlink" title="8、Faster R-CNN"></a>8、Faster R-CNN</h4><p>主要改进，引进了区域建议网络RPN (Region Proposal Network)来产生候选区域（原因是Selective Search算法提出候选区域占据了整个训练和测试的绝大部分时间），因此提出RPN，使用卷积神经网络来产生候选区域。其他部分与Fast R-CNN一致，即扣取每个候选区域的特征，然后对其进行分类。</p><img src="/2024/04/10/jing-dian-juan-ji-shen-jing-wang-luo/Faster R-CNN.png" alt="Faster R-CNN流程示意图" style="zoom:60%;"><h5 id="RPN"><a href="#RPN" class="headerlink" title="RPN"></a>RPN</h5><p>RPN在结构中是用来生成一系列proposals的一层，本质上它是Backbone的最后一层卷积层作为输入，proposal的具体位置作为输出的神经网络。RPN主要包含以下几步：</p><ul><li>生成anchor boxes；</li><li>判断anchor boxes包含的是前景还是背景；</li><li>回归学习anchor boxes和ground truth的标注的位置差，来精确定位物体。</li></ul><p>首先介绍下什么是anchor和anchor boxes。每个anchor点就是backbone网络最后一层卷积层feature map上的元素点。而anchor boxes是以anchor为中心点而生成的一系列框boxes。一个anchor对应的框的具体数量是由scales和aspect ratios 2个参数控制。scales指的是对于每种形状，框体最长边的像素大小，aspect ratios指的是具体由哪些形状，描述的是长宽比。所以scales[8,16,32] 和ratios [0.5,1,2]就代表一个anchor会生成9个anchor boxes。注意的是anchor boxes的坐标是对应在原图尺寸上的，而feature map相比原图是缩小很多倍的，比如VGG16,backbone网络出来的图片尺寸比原图缩小了16倍，如果在原图中采anchor boxes，就需要按16像素点跳着采样。</p><p>有了anchor boxes还不够，这些框体大多都都没有包含目标物体，所以我们需要模型学习判断box是否包含物体（即是前景图片还是背景图片），同时对于前景图片我们还要学习来预测box和GT的offset。这2个任务分别是用2个卷积网络实现的，如下图所示，</p><img src="/2024/04/10/jing-dian-juan-ji-shen-jing-wang-luo/RPN.png" alt="RPN" style="zoom:60%;"><p>RPN的损失函数如下，由分类损失和定位损失两部分组成<br>$$<br>L(p_i,t_i)&#x3D;\frac{1}{N_{cls}} \times \sum L_{cls}(p_i,p_i^*) + \frac{\lambda}{N_{reg}} \times \sum p_i^<em>L_{reg}(t_i,t_i^</em>)<br>$$<br>其中 $L_{cls}$ 常用交叉熵损失函数，$L_{reg}$ 常用平方误差损失函数</p><p>参考链接：<a href="https://blog.csdn.net/qq_27825451/article/details/88815490">ZF网络架构深度详解-CSDN博客</a></p><p>[[1409.1556] Very Deep Convolutional Networks for Large-Scale Image Recognition (arxiv.org)]: </p>]]></content>
      
      
      <categories>
          
          <category> deeplearing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deeplearning </tag>
            
            <tag> 卷积神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>正则表达式</title>
      <link href="/2024/04/08/zheng-ze-biao-da-shi/"/>
      <url>/2024/04/08/zheng-ze-biao-da-shi/</url>
      
        <content type="html"><![CDATA[<h3 id="一、正则表达式"><a href="#一、正则表达式" class="headerlink" title="一、正则表达式"></a>一、正则表达式</h3><p>​简单理解正则表达式就是用来匹配一段字符串中自己想要的部分的匹配规则，从一段长字符串中抽取想要的关键信息</p><h4 id="1、字符组"><a href="#1、字符组" class="headerlink" title="1、字符组"></a>1、字符组</h4><p>​字符组用[]括起来。在[]中出现的内容会被匹配，例如[abc]匹配a或b或c，字符组内容过多时还可以使用-，例如[a-z]表示a到z之间所有小写字母，[0-9]表示匹配0到9的所有阿拉伯数字</p><h4 id="2、简单元字符"><a href="#2、简单元字符" class="headerlink" title="2、简单元字符"></a>2、简单元字符</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">.匹配除了换行符以外的任意字符</span><br><span class="line">\w匹配字母、数字、下划线</span><br><span class="line">\s匹配任意的空白符</span><br><span class="line">\d匹配数字</span><br><span class="line">\n匹配一个换行符</span><br><span class="line">\t匹配一个制表符</span><br><span class="line"></span><br><span class="line">^匹配字符串的开始</span><br><span class="line">$匹配字符串的结尾</span><br><span class="line"></span><br><span class="line">\W匹配非 字母、数字、下划线（和\w相反）</span><br><span class="line">\D匹配非数字</span><br><span class="line">\S匹配非空白符</span><br><span class="line">a|b匹配字符a或字符b</span><br><span class="line">()匹配括号内的表达式，也表示一个组</span><br><span class="line">[...]匹配字符组中的字符</span><br><span class="line">[^...]匹配除了字符组中字符的所有字符</span><br></pre></td></tr></table></figure><h4 id="3、量词"><a href="#3、量词" class="headerlink" title="3、量词"></a>3、量词<a id="量词"></a></h4><p>控制前面的元字符出现的次数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">*重复零次或更多次</span><br><span class="line">+重复一次或更多次</span><br><span class="line">？重复零次或一次</span><br><span class="line">&#123;n&#125;重复n次</span><br><span class="line">&#123;n,&#125; 重复n次或更多次</span><br><span class="line">&#123;n,m&#125; 重复n到m次</span><br></pre></td></tr></table></figure><h4 id="4、惰性匹配与贪婪匹配"><a href="#4、惰性匹配与贪婪匹配" class="headerlink" title="4、惰性匹配与贪婪匹配"></a>4、惰性匹配与贪婪匹配</h4><p>​量词中的？、*、+、{}都属于贪婪匹配，就是尽可能多的匹配到结果</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">例如：</span><br><span class="line">str: 玩儿吃鸡游戏, 晚上一起上游戏, 干嘛呢? 打游戏啊</span><br><span class="line">reg: 玩儿.*游戏</span><br><span class="line"></span><br><span class="line">此时匹配的是: 玩儿吃鸡游戏, 晚上一起上游戏, 干嘛呢? 打游戏</span><br></pre></td></tr></table></figure><p>​在使用.*后面如果加了？则是表示尽可能少匹配，表示惰性匹配</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">str: 玩儿吃鸡游戏, 晚上一起上游戏, 干嘛呢? 打游戏啊</span><br><span class="line">reg: 玩儿.*?游戏</span><br><span class="line"></span><br><span class="line">此时匹配的是: 玩儿吃鸡游戏</span><br><span class="line"></span><br><span class="line">str: &lt;div&gt;胡辣汤&lt;/div&gt;</span><br><span class="line">reg: &lt;.*&gt;</span><br><span class="line">结果: &lt;div&gt;胡辣汤&lt;/div&gt;</span><br><span class="line"></span><br><span class="line">str: &lt;div&gt;胡辣汤&lt;/div&gt;</span><br><span class="line">reg: &lt;.*?&gt;</span><br><span class="line">结果: </span><br><span class="line">    &lt;div&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line"></span><br><span class="line">str: &lt;div&gt;胡辣汤&lt;/div&gt;&lt;span&gt;饭团&lt;/span&gt;</span><br><span class="line">reg: &lt;div&gt;.*?&lt;/div&gt;</span><br><span class="line">结果:</span><br><span class="line">    &lt;div&gt;胡辣汤&lt;/div&gt;</span><br></pre></td></tr></table></figure><p>.*?x的含义，找到最近的x</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">str: abcdefgxhijklmn</span><br><span class="line">reg: .*?x</span><br><span class="line">结果:abcdefgx</span><br></pre></td></tr></table></figure><h4 id="5、分组"><a href="#5、分组" class="headerlink" title="5、分组"></a>5、分组</h4><p>在正则中使用()进行分组. 比如. 我们要匹配一个相对复杂的身份证号. 身份证号分成两种. 老的身份证号有15位. 新的身份证号有18位. 并且新的身份证号结尾有可能是x.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">身份证号  15位, 18位</span><br><span class="line">^[1-9]\d&#123;13,16&#125;[0-9x]$</span><br><span class="line">^[1-9]\d&#123;14&#125;(\d&#123;2&#125;[0-9x])?$√</span><br><span class="line">^([1-9]\d&#123;16&#125;[0-9x]|[1-9]\d&#123;14&#125;)$</span><br></pre></td></tr></table></figure><p>最后，推荐一个可以练习正则表达式的网站<a href="https://tool.chinaz.com/regex/">正则表达式在线测试 - 站长工具 (chinaz.com)</a></p><p><a href="#%E9%87%8F%E8%AF%8D">量词</a></p>]]></content>
      
      
      <categories>
          
          <category> Markdown </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Typora </tag>
            
            <tag> Markdown </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/04/08/latex-ru-men-xue-xi/"/>
      <url>/2024/04/08/latex-ru-men-xue-xi/</url>
      
        <content type="html"><![CDATA[<h3 id="前言：排版工具与书写工具的讨论"><a href="#前言：排版工具与书写工具的讨论" class="headerlink" title="前言：排版工具与书写工具的讨论"></a>前言：排版工具与书写工具的讨论</h3><p>LaTeX是一种“非所见即所得”的排版系统，用户需要输入特定的代码，保存在后缀为.tex的文件中，通过编译得到所需的pdf文件，例如以下代码：</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\documentclass</span>&#123;article&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">\begin</span>&#123;document&#125;</span><br><span class="line"></span><br><span class="line">Hello, world!</span><br><span class="line"></span><br><span class="line"><span class="keyword">\end</span>&#123;document&#125;</span><br></pre></td></tr></table></figure><p>最后输出的结果是一个pdf文件，内容是”Hello, world!“。</p><p>如何理解“非所见即所得”呢？在这里举个“所见即所得”的例子：Word。Word的界面就是一张A4纸，输入的时候是什么样子，最后呈现出来就是什么样子。这给了我们极高的<strong>自由度</strong>，也非常容易上手，但是有如下问题： - 对于对细节不敏感的用户，Word的排版常常会在细节存在问题，比如两段话之间行间距不同、字体不同、标题样式不同等； - 对于撰写论文的用户，Word的标题、章节、图表、参考文献等无法自动标号，也很难在正文中引用； - 对于有公式输入需求的用户，Word自带的公式不稳定，而公式插件效果常常不好。</p><p>相比之下，使用LaTeX进行排版，就像是在铺好的轨道上驾驶火车一样。使用LaTeX没有办法像Word一样非常自由，但是可以保证<strong>规范性</strong>，这使得LaTeX非常适合用于论文的排版。在学习的过程中，也将会感受到这一点。</p><p>无论是LaTeX还是Word，其归根结底都只是<strong>排版工具</strong>，用Word也可以排出LaTeX的效果，用LaTeX也可以排出Word的效果。另外，笔者最建议的<strong>书写工具</strong>是Markdown，其书写的过程中可以不在意排版，也支持使用LaTeX语法输入公式，与LaTeX之间的转换非常方便。</p><p>- </p><h4 id="利用LaTeX编写文档"><a href="#利用LaTeX编写文档" class="headerlink" title="利用LaTeX编写文档"></a>利用LaTeX编写文档</h4><h4 id="文档类型"><a href="#文档类型" class="headerlink" title="文档类型"></a>文档类型</h4><p>TeX有多种文档类型可选，笔者较常用的有如下几种类型：</p><ul><li>对于英文，可以用<code>book</code>、<code>article</code>和<code>beamer</code>；</li><li>对于中文，可以用<code>ctexbook</code>、<code>ctexart</code>和<code>ctexbeamer</code>，这些类型自带了对中文的支持。</li></ul><p>不同的文件类型，编写的过程中也会有一定的差异，如果直接修改文件类型的话，甚至会报错。以下统一选用<code>ctexart</code>。在编辑框第一行，输入如下内容来设置文件类型：</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\documentclass</span>&#123;ctexart&#125;</span><br></pre></td></tr></table></figure><p>另外，一般也可以在<code>\documentclass</code>处设置基本参数，笔者通常设置默认字体大小为12pt，纸张大小为A4，单面打印。需要将第一行的内容替换为：</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\documentclass</span>[12pt, a4paper, oneside]&#123;ctexart&#125;</span><br></pre></td></tr></table></figure><p>文件的正文部分需要放入document环境中，在document环境外的部分不会出现在文件中。</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\documentclass</span>[12pt, a4paper, oneside]&#123;ctexart&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">\begin</span>&#123;document&#125;</span><br><span class="line"></span><br><span class="line">这里是正文. </span><br><span class="line"></span><br><span class="line"><span class="keyword">\end</span>&#123;document&#125;</span><br></pre></td></tr></table></figure><h4 id="宏包"><a href="#宏包" class="headerlink" title="宏包"></a>宏包</h4><p>为了完成一些功能（如定理环境），还需要在导言区，也即document环境之前加载宏包。加载宏包的代码是<code>\usepackage&#123;&#125;</code>。本份教程中，与数学公式与定理环境相关的宏包为<code>amsmath</code>、<code>amsthm</code>、<code>amssymb</code>，用于插入图片的宏包为<code>graphicx</code>，代码如下：</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\usepackage</span>&#123;amsmath, amsthm, amssymb, graphicx&#125;</span><br></pre></td></tr></table></figure><p>另外，在加载宏包时还可以设置基本参数，如使用超链接宏包<code>hyperref</code>，可以设置引用的颜色为黑色等，代码如下：</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\usepackage</span>[bookmarks=true, colorlinks, citecolor=blue, linkcolor=black]&#123;hyperref&#125;</span><br></pre></td></tr></table></figure><h4 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h4><p>标题可以用<code>\title&#123;&#125;</code>设置，作者可以用<code>\author</code>设置，日期可以用<code>\date&#123;&#125;</code>设置，这些都需要放在导言区。为了在文档中显示标题信息，需要使用<code>\maketitle</code>。例如：</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\documentclass</span>[12pt, a4paper, oneside]&#123;ctexart&#125;</span><br><span class="line"><span class="keyword">\usepackage</span>&#123;amsmath, amsthm, amssymb, graphicx&#125;</span><br><span class="line"><span class="keyword">\usepackage</span>[bookmarks=true, colorlinks, citecolor=blue, linkcolor=black]&#123;hyperref&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">% 导言区</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">\title</span>&#123;我的第一个<span class="keyword">\LaTeX</span> 文档&#125;</span><br><span class="line"><span class="keyword">\author</span>&#123;Dylaaan&#125;</span><br><span class="line"><span class="keyword">\date</span>&#123;<span class="keyword">\today</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">\begin</span>&#123;document&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">\maketitle</span></span><br><span class="line"></span><br><span class="line">这里是正文. </span><br><span class="line"></span><br><span class="line"><span class="keyword">\end</span>&#123;document&#125;</span><br></pre></td></tr></table></figure><h4 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h4><p>正文可以直接在document环境中书写，没有必要加入空格来缩进，因为文档默认会进行首行缩进。相邻的两行在编译时仍然会视为同一段。在LaTeX中，另起一段的方式是使用一行相隔，例如：</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">我是第一段. </span><br><span class="line"></span><br><span class="line">我是第二段.</span><br></pre></td></tr></table></figure><p>这样编译出来就是两个段落。在正文部分，多余的空格、回车等等都会被自动忽略，这保证了全文排版不会突然多出一行或者多出一个空格。另外，另起一页的方式是：</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\newpage</span></span><br></pre></td></tr></table></figure><p>笔者在编写文档时，为了保证美观，通常将中文标点符号替换为英文标点符号（需要注意的是英文标点符号后面还有一个空格），这比较适合数学类型的文档。</p><p>在正文中，还可以设置局部的特殊字体：</p><table><thead><tr><th>字体</th><th>命令</th></tr></thead><tbody><tr><td>直立</td><td>\textup{}</td></tr><tr><td>意大利</td><td>\textit{}</td></tr><tr><td>倾斜</td><td>\textsl{}</td></tr><tr><td>小型大写</td><td>\textsc{}</td></tr><tr><td>加宽加粗</td><td>\textbf{}</td></tr></tbody></table><h3 id="章节"><a href="#章节" class="headerlink" title="章节"></a>章节</h3><p>对于<code>ctexart</code>文件类型，章节可以用<code>\section&#123;&#125;</code>和<code>\subsection&#123;&#125;</code>命令来标记，例如：</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\documentclass</span>[12pt, a4paper, oneside]&#123;ctexart&#125;</span><br><span class="line"><span class="keyword">\usepackage</span>&#123;amsmath, amsthm, amssymb, graphicx&#125;</span><br><span class="line"><span class="keyword">\usepackage</span>[bookmarks=true, colorlinks, citecolor=blue, linkcolor=black]&#123;hyperref&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">% 导言区</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">\title</span>&#123;我的第一个<span class="keyword">\LaTeX</span> 文档&#125;</span><br><span class="line"><span class="keyword">\author</span>&#123;Dylaaan&#125;</span><br><span class="line"><span class="keyword">\date</span>&#123;<span class="keyword">\today</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">\begin</span>&#123;document&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">\maketitle</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">\section</span>&#123;一级标题&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">\subsection</span>&#123;二级标题&#125;</span><br><span class="line"></span><br><span class="line">这里是正文. </span><br><span class="line"></span><br><span class="line"><span class="keyword">\subsection</span>&#123;二级标题&#125;</span><br><span class="line"></span><br><span class="line">这里是正文. </span><br><span class="line"></span><br><span class="line"><span class="keyword">\end</span>&#123;document&#125;</span><br></pre></td></tr></table></figure><h4 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h4><p>在有了章节的结构之后，使用<code>\tableofcontents</code>命令就可以在指定位置生成目录。通常带有目录的文件需要编译两次，因为需要先在目录中生成.toc文件，再据此生成目录。</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\documentclass</span>[12pt, a4paper, oneside]&#123;ctexart&#125;</span><br><span class="line"><span class="keyword">\usepackage</span>&#123;amsmath, amsthm, amssymb, graphicx&#125;</span><br><span class="line"><span class="keyword">\usepackage</span>[bookmarks=true, colorlinks, citecolor=blue, linkcolor=black]&#123;hyperref&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">% 导言区</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">\title</span>&#123;我的第一个<span class="keyword">\LaTeX</span> 文档&#125;</span><br><span class="line"><span class="keyword">\author</span>&#123;Dylaaan&#125;</span><br><span class="line"><span class="keyword">\date</span>&#123;<span class="keyword">\today</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">\begin</span>&#123;document&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">\maketitle</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">\tableofcontents</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">\section</span>&#123;一级标题&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">\subsection</span>&#123;二级标题&#125;</span><br><span class="line"></span><br><span class="line">这里是正文. </span><br><span class="line"></span><br><span class="line"><span class="keyword">\subsection</span>&#123;二级标题&#125;</span><br><span class="line"></span><br><span class="line">这里是正文. </span><br><span class="line"></span><br><span class="line"><span class="keyword">\end</span>&#123;document&#125;</span><br></pre></td></tr></table></figure><h4 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h4><p>插入图片需要使用<code>graphicx</code>宏包，建议使用如下方式：</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;figure&#125;[htbp]</span><br><span class="line">    <span class="keyword">\centering</span></span><br><span class="line">    <span class="keyword">\includegraphics</span>[width=8cm]&#123;图片.jpg&#125;</span><br><span class="line">    <span class="keyword">\caption</span>&#123;图片标题&#125;</span><br><span class="line"><span class="keyword">\end</span>&#123;figure&#125;</span><br></pre></td></tr></table></figure><p>其中，<code>[htbp]</code>的作用是自动选择插入图片的最优位置，<code>\centering</code>设置让图片居中，<code>[width=8cm]</code>设置了图片的宽度为8cm，<code>\caption&#123;&#125;</code>用于设置图片的标题。</p><h4 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h4><p>LaTeX中表格的插入较为麻烦，可以直接使用<a href="https://link.zhihu.com/?target=https://www.tablesgenerator.com/%23">Create LaTeX tables online – TablesGenerator.com</a>来生成。建议使用如下方式：</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;table&#125;[htbp]</span><br><span class="line">    <span class="keyword">\centering</span></span><br><span class="line">    <span class="keyword">\caption</span>&#123;表格标题&#125;</span><br><span class="line">    <span class="keyword">\begin</span>&#123;tabular&#125;&#123;ccc&#125;</span><br><span class="line">        1 <span class="built_in">&amp;</span> 2 <span class="built_in">&amp;</span> 3 <span class="keyword">\\</span></span><br><span class="line">        4 <span class="built_in">&amp;</span> 5 <span class="built_in">&amp;</span> 6 <span class="keyword">\\</span></span><br><span class="line">        7 <span class="built_in">&amp;</span> 8 <span class="built_in">&amp;</span> 9</span><br><span class="line">    <span class="keyword">\end</span>&#123;tabular&#125;</span><br><span class="line"><span class="keyword">\end</span>&#123;table&#125;</span><br></pre></td></tr></table></figure><h4 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h4><p>LaTeX中的列表环境包含无序列表<code>itemize</code>、有序列表<code>enumerate</code>和描述<code>description</code>，以<code>enumerate</code>为例，用法如下：</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;enumerate&#125;</span><br><span class="line">    <span class="keyword">\item</span> 这是第一点; </span><br><span class="line">    <span class="keyword">\item</span> 这是第二点;</span><br><span class="line">    <span class="keyword">\item</span> 这是第三点. </span><br><span class="line"><span class="keyword">\end</span>&#123;enumerate&#125;</span><br></pre></td></tr></table></figure><p>另外，也可以自定义<code>\item</code>的样式：</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;enumerate&#125;</span><br><span class="line">    <span class="keyword">\item</span>[(1)] 这是第一点; </span><br><span class="line">    <span class="keyword">\item</span>[(2)] 这是第二点;</span><br><span class="line">    <span class="keyword">\item</span>[(3)] 这是第三点. </span><br><span class="line"><span class="keyword">\end</span>&#123;enumerate&#125;</span><br></pre></td></tr></table></figure><h4 id="定理环境"><a href="#定理环境" class="headerlink" title="定理环境"></a>定理环境</h4><p>定理环境需要使用<code>amsthm</code>宏包，首先在导言区加入：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\newtheorem&#123;theorem&#125;&#123;定理&#125;[section]</span><br></pre></td></tr></table></figure><p>其中<code>&#123;theorem&#125;</code>是环境的名称，<code>&#123;定理&#125;</code>设置了该环境显示的名称是“定理”，<code>[section]</code>的作用是让<code>theorem</code>环境在每个section中单独编号。在正文中，用如下方式来加入一条定理：</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;theorem&#125;[定理名称]</span><br><span class="line">    这里是定理的内容. </span><br><span class="line"><span class="keyword">\end</span>&#123;theorem&#125;</span><br></pre></td></tr></table></figure><p>其中<code>[定理名称]</code>不是必须的。另外，我们还可以建立新的环境，如果要让新的环境和<code>theorem</code>环境一起计数的话，可以用如下方式：</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\newtheorem</span>&#123;theorem&#125;&#123;定理&#125;[section]</span><br><span class="line"><span class="keyword">\newtheorem</span>&#123;definition&#125;[theorem]&#123;定义&#125;</span><br><span class="line"><span class="keyword">\newtheorem</span>&#123;lemma&#125;[theorem]&#123;引理&#125;</span><br><span class="line"><span class="keyword">\newtheorem</span>&#123;corollary&#125;[theorem]&#123;推论&#125;</span><br><span class="line"><span class="keyword">\newtheorem</span>&#123;example&#125;[theorem]&#123;例&#125;</span><br><span class="line"><span class="keyword">\newtheorem</span>&#123;proposition&#125;[theorem]&#123;命题&#125;</span><br></pre></td></tr></table></figure><p>另外，定理的证明可以直接用<code>proof</code>环境。</p><h4 id="页面"><a href="#页面" class="headerlink" title="页面"></a>页面</h4><p>最开始选择文件类型时，我们设置的页面大小是a4paper，除此之外，我们也可以修改页面大小为b5paper等等。</p><p>一般情况下，LaTeX默认的页边距很大，为了让每一页显示的内容更多一些，我们可以使用<code>geometry</code>宏包，并在导言区加入以下代码：</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\usepackage</span>&#123;geometry&#125;</span><br><span class="line"><span class="keyword">\geometry</span>&#123;left=2.54cm, right=2.54cm, top=3.18cm, bottom=3.18cm&#125;</span><br></pre></td></tr></table></figure><p>另外，为了设置行间距，可以使用如下代码：</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\linespread</span>&#123;1.5&#125;</span><br></pre></td></tr></table></figure><h4 id="页码"><a href="#页码" class="headerlink" title="页码"></a>页码</h4><p>默认的页码编码方式是阿拉伯数字，用户也可以自己设置为小写罗马数字：</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\pagenumbering</span>&#123;roman&#125;</span><br></pre></td></tr></table></figure><p>另外，<code>aiph</code>表示小写字母，<code>Aiph</code>表示大写字母，<code>Roman</code>表示大写罗马数字，<code>arabic</code>表示默认的阿拉伯数字。如果要设置页码的话，可以用如下代码来设置页码从0开始：</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\setcounter</span>&#123;page&#125;&#123;0&#125;</span><br></pre></td></tr></table></figure><h4 id="数学公式的输入方式"><a href="#数学公式的输入方式" class="headerlink" title="数学公式的输入方式"></a>数学公式的输入方式</h4><h4 id="行内公式"><a href="#行内公式" class="headerlink" title="行内公式"></a>行内公式</h4><p>行内公式通常使用<code>$..$</code>来输入，这通常被称为公式环境，例如：</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">若<span class="built_in">$</span>a&gt;0<span class="built_in">$</span>, <span class="built_in">$</span>b&gt;0<span class="built_in">$</span>, 则<span class="built_in">$</span>a+b&gt;0<span class="built_in">$</span>.</span><br></pre></td></tr></table></figure><p>若$a&gt;0$, $b&gt;0$, 则$a+b&gt;0$.</p><p>公式环境通常使用特殊的字体，并且默认为斜体。需要注意的是，只要是公式，就需要放入公式环境中。如果需要在行内公式中展现出行间公式的效果，可以在前面加入<code>\displaystyle</code>，例如</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">设<span class="built_in">$</span><span class="keyword">\displaystyle</span><span class="keyword">\lim</span><span class="built_in">_</span>&#123;n<span class="keyword">\to</span><span class="keyword">\infty</span>&#125;x<span class="built_in">_</span>n=x<span class="built_in">$</span>.</span><br></pre></td></tr></table></figure><p>设$\displaystyle\lim_{n\to\infty}x_n&#x3D;x$</p><h4 id="行间公式"><a href="#行间公式" class="headerlink" title="行间公式"></a>行间公式</h4><p>行间公式需要用<code>\[..\]</code>或者<code>$$..$$</code>来输入，推荐使用<code>\[..\]</code>，输入方式如下：</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">若<span class="built_in">$</span>a&gt;0<span class="built_in">$</span>, <span class="built_in">$</span>b&gt;0<span class="built_in">$</span>, 则</span><br><span class="line"><span class="keyword">\[</span></span><br><span class="line">a+b&gt;0.</span><br><span class="line"><span class="keyword">\]</span></span><br></pre></td></tr></table></figure><p>若$a&gt;0, b&gt;0$, 则</p><p>$a+b&gt;0.$</p><p>关于具体的输入方式，可以参考<a href="https://link.zhihu.com/?target=https://www.latexlive.com/">在线LaTeX公式编辑器-编辑器 (latexlive.com)</a>，在这里只列举一些需要注意的。</p><h4 id="上下标"><a href="#上下标" class="headerlink" title="上下标"></a>上下标</h4><p>上标可以用<code>^</code>输入，例如<code>a^n</code>，效果为 a^n ；下标可以用<code>_</code>来输入，例如<code>a_1</code>，效果为 a_1 。上下标只会读取第一个字符，如果上下标的内容较多的话，需要改成<code>^&#123;&#125;</code>或<code>_&#123;&#125;</code>。</p><p>$x_1$  $x_n$$x_n^m$$x_{i+j}^{m+n}$</p><h4 id="分式"><a href="#分式" class="headerlink" title="分式"></a>分式</h4><p>分式可以用<code>\dfrac&#123;&#125;&#123;&#125;</code>来输入，例如<code>\dfrac&#123;a&#125;&#123;b&#125;</code>，效果为 \dfrac{a}{b} 。为了在行间、分子、分母或者指数上输入较小的分式，可以改用<code>\frac&#123;&#125;&#123;&#125;</code>，例如<code>a^\frac&#123;1&#125;&#123;n&#125;</code>，效果为 $a^\frac{1}{n}$ 。</p><h4 id="括号"><a href="#括号" class="headerlink" title="括号"></a>括号</h4><p>括号可以直接用<code>(..)</code>输入，但是需要注意的是，有时候括号内的内容高度较大，需要改用<code>\left(..\right)</code>。例如<code>\left(1+\dfrac&#123;1&#125;&#123;n&#125;\right)^n</code>，效果是$ \left(1+\dfrac{1}{n}\right)^n$ 。</p><p>在中间需要隔开时，可以用<code>\left(..\middle|..\right)</code>。</p><p>另外，输入大括号{}时需要用<code>\&#123;..\&#125;</code>，其中<code>\</code>起到了转义作用。</p><h4 id="加粗"><a href="#加粗" class="headerlink" title="加粗"></a>加粗</h4><p>对于加粗的公式，建议使用<code>bm</code>宏包，并且用命令<code>\bm&#123;&#125;</code>来加粗，这可以保留公式的斜体。</p><h4 id="大括号"><a href="#大括号" class="headerlink" title="大括号"></a>大括号</h4><p>在这里可以使用<code>cases</code>环境，可以用于分段函数或者方程组，例如</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">$</span><span class="built_in">$</span></span><br><span class="line">f(x)=<span class="keyword">\begin</span>&#123;cases&#125;</span><br><span class="line">    x, <span class="built_in">&amp;</span> x&gt;0, <span class="keyword">\\</span></span><br><span class="line">    -x, <span class="built_in">&amp;</span> x<span class="keyword">\leq</span> 0.</span><br><span class="line"><span class="keyword">\end</span>&#123;cases&#125;</span><br><span class="line"><span class="built_in">$</span><span class="built_in">$</span></span><br></pre></td></tr></table></figure><p>效果为</p><p>$f(x)&#x3D;\begin{cases} x, &amp; x&gt;0, \ -x, &amp; x\leq 0. \end{cases}$</p><h4 id="多行公式"><a href="#多行公式" class="headerlink" title="多行公式"></a>多行公式</h4><p>多行公式通常使用<code>aligned</code>环境，例如</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">$</span><span class="built_in">$</span></span><br><span class="line"><span class="keyword">\begin</span>&#123;aligned&#125;</span><br><span class="line">a <span class="built_in">&amp;</span> =b+c <span class="keyword">\\</span></span><br><span class="line"><span class="built_in">&amp;</span> =d+e</span><br><span class="line"><span class="keyword">\end</span>&#123;aligned&#125;</span><br><span class="line"><span class="built_in">$</span><span class="built_in">$</span></span><br></pre></td></tr></table></figure><p>效果为</p><p>$\begin{aligned} a &amp; &#x3D;b+c \ &amp; &#x3D;d+e \end{aligned}$</p><h4 id="矩阵和行列式"><a href="#矩阵和行列式" class="headerlink" title="矩阵和行列式"></a>矩阵和行列式</h4><p>矩阵可以用<code>bmatrix</code>环境和<code>pmatrix</code>环境，分别为方括号和圆括号，例如</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">$</span><span class="built_in">$</span></span><br><span class="line"><span class="keyword">\begin</span>&#123;bmatrix&#125;</span><br><span class="line">    a <span class="built_in">&amp;</span> b <span class="keyword">\\</span></span><br><span class="line">    c <span class="built_in">&amp;</span> d</span><br><span class="line"><span class="keyword">\end</span>&#123;bmatrix&#125;</span><br><span class="line"><span class="built_in">$</span><span class="built_in">$</span></span><br></pre></td></tr></table></figure><p>效果为$ \begin{bmatrix} a &amp; b \ c &amp; d \end{bmatrix} $。如果要输入行列式的话，可以使用<code>vmatrix</code>环境，用法同上。</p><hr><h4 id="常用数学公式和符号"><a href="#常用数学公式和符号" class="headerlink" title="常用数学公式和符号"></a>常用数学公式和符号</h4><p>在latex中，字符 #、$、%、&amp;、~、^、n、_、{、} 的含义特殊，不能直接表示</p><table><thead><tr><th align="center">符号</th><th align="center">命令</th><th align="center">符号</th><th align="center">命令</th><th align="center">符号</th><th align="center">命令</th></tr></thead><tbody><tr><td align="center">$</td><td align="center">$</td><td align="center">%</td><td align="center">%</td><td align="center">{</td><td align="center">{</td></tr><tr><td align="center">_</td><td align="center">_</td><td align="center">}</td><td align="center">}</td><td align="center">#</td><td align="center">#</td></tr><tr><td align="center">&amp;</td><td align="center">&amp;</td><td align="center">^</td><td align="center">^{}</td><td align="center">~</td><td align="center">~{}</td></tr><tr><td align="center">\</td><td align="center">\backslash</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr></tbody></table><h5 id="公式中常用到的希腊字母"><a href="#公式中常用到的希腊字母" class="headerlink" title="公式中常用到的希腊字母"></a>公式中常用到的希腊字母</h5><table><thead><tr><th align="center">符号</th><th align="center">命令</th><th align="center">符号</th><th align="center">命令</th><th align="center">符号</th><th align="center">命令</th></tr></thead><tbody><tr><td align="center">$\alpha$</td><td align="center">\alpha</td><td align="center">$\beta$</td><td align="center">\beta</td><td align="center">$\gamma$  $\Gamma$</td><td align="center">\gamma  \Gamma</td></tr><tr><td align="center">$\delta$  $\Delta$</td><td align="center">\delta  \Delta</td><td align="center">$\epsilon$  $\varepsilon$</td><td align="center">\epsilon  \varepsilon</td><td align="center">$\zeta$</td><td align="center">\zeta</td></tr><tr><td align="center">$\eta$</td><td align="center">\eta</td><td align="center">$\theta$  $\vartheta$</td><td align="center">\theta  \vartheta</td><td align="center">$\iota$</td><td align="center">\iota</td></tr><tr><td align="center">$\kappa$</td><td align="center">\kappa</td><td align="center">$\Theta$</td><td align="center">\Theta</td><td align="center">$\lambda$</td><td align="center">\lambda</td></tr><tr><td align="center">$\mu$</td><td align="center">\mu</td><td align="center">$\nu$</td><td align="center">\nu</td><td align="center">$\xi$  $\Xi$</td><td align="center">\xi  \Xi</td></tr><tr><td align="center">$\pi$  $\Pi$</td><td align="center">\pi  \Pi</td><td align="center">$o$  $O$</td><td align="center">o  O</td><td align="center">$\rho$  $\varrho$</td><td align="center">\rho  \varrho</td></tr><tr><td align="center">$\sigma$  $\Sigma$</td><td align="center">\sigma  \Sigma</td><td align="center">$\tau$</td><td align="center">\tau</td><td align="center">$\upsilon$</td><td align="center">\upsilon</td></tr><tr><td align="center">$\phi$  $\varphi$</td><td align="center">\phi  \varphi</td><td align="center">$\chi$</td><td align="center">\chi</td><td align="center">$\psi$  $\Psi$</td><td align="center">\psi  \Psi</td></tr><tr><td align="center">$\omega$  $\Omega$</td><td align="center">\omega  \Omega</td><td align="center">$\Phi$</td><td align="center">\Phi</td><td align="center">$\Upsilon$</td><td align="center">\Upsilon</td></tr></tbody></table><h5 id="各种运算符号"><a href="#各种运算符号" class="headerlink" title="各种运算符号"></a>各种运算符号</h5><table><thead><tr><th align="center">符号</th><th align="center">命令</th><th align="center">符号</th><th align="center">命令</th><th align="center">符号</th><th align="center">命令</th></tr></thead><tbody><tr><td align="center">$\times$</td><td align="center">\times</td><td align="center">$\div$</td><td align="center">\div</td><td align="center">$\pm$  ($\mp$)</td><td align="center">\pm  (\mp)</td></tr><tr><td align="center">$\otimes$</td><td align="center">\otimes</td><td align="center">$\oplus$</td><td align="center">\oplus</td><td align="center">$\odot$</td><td align="center">\odot</td></tr><tr><td align="center">$\oslash$</td><td align="center">\oslash</td><td align="center">$\triangle$</td><td align="center">\triangle</td><td align="center">$\neq$</td><td align="center">\neq</td></tr><tr><td align="center">$\equiv$</td><td align="center">\equiv</td><td align="center">$\pm$</td><td align="center">\pm</td><td align="center">$\ominus$</td><td align="center">\ominus</td></tr><tr><td align="center">$\le$</td><td align="center">\le</td><td align="center">$&lt;$</td><td align="center">&lt;</td><td align="center">$&gt;$</td><td align="center">&gt;</td></tr><tr><td align="center">$\ge$</td><td align="center">\ge</td><td align="center">$\cup$</td><td align="center">\cup</td><td align="center">$\bigcup$</td><td align="center">\bigcup</td></tr><tr><td align="center">$\bigotimes$</td><td align="center">\bigotimes</td><td align="center">$\bigcirc$</td><td align="center">\bigcirc</td><td align="center">$\vee$</td><td align="center">\vee</td></tr></tbody></table><table><thead><tr><th align="center">符号</th><th align="center">命令</th><th align="center">符号</th><th align="center">命令</th><th align="center">符号</th><th align="center">命令</th></tr></thead><tbody><tr><td align="center">$\bigvee$</td><td align="center">\bigvee</td><td align="center">$\sqcap$</td><td align="center">\sqcap</td><td align="center">$\subset$</td><td align="center">\subset</td></tr><tr><td align="center">$\subseteq$</td><td align="center">\subseteq</td><td align="center">$\setminus$</td><td align="center">\setminus</td><td align="center">$\parallel$</td><td align="center">\parallel</td></tr><tr><td align="center">$\propto$</td><td align="center">\propto</td><td align="center">$\forall$</td><td align="center">\forall</td><td align="center">$\aleph$</td><td align="center">\aleph</td></tr><tr><td align="center">$\ell$</td><td align="center">\ell</td><td align="center">$\uplus$</td><td align="center">\uplus</td><td align="center">$\cap$</td><td align="center">\cap</td></tr><tr><td align="center">$\bigcap$</td><td align="center">\bigcap</td><td align="center">$\bigoplus$</td><td align="center">\bigoplus</td><td align="center">$\amalg$</td><td align="center">\amalg</td></tr><tr><td align="center">$\wedge$</td><td align="center">\wedge</td><td align="center">$\bigwedge$</td><td align="center">\bigwedge</td><td align="center">$\sqcup$</td><td align="center">\sqcup</td></tr><tr><td align="center">$\supset$</td><td align="center">\supset</td><td align="center">$\supseteq$</td><td align="center">\supseteq</td><td align="center">$\mid$</td><td align="center">\mid</td></tr><tr><td align="center">$\neg$</td><td align="center">\neg</td><td align="center">$\exists$</td><td align="center">\exists</td><td align="center">$\nabla$</td><td align="center">\nabla</td></tr><tr><td align="center">$\partial$</td><td align="center">\partial</td><td align="center">$\biguplus$</td><td align="center">\biguplus</td><td align="center">$\ast$</td><td align="center">\ast</td></tr><tr><td align="center">$\circ$</td><td align="center">\circ</td><td align="center">$\to$</td><td align="center">\to</td><td align="center">$\lhd$</td><td align="center">\lhd</td></tr><tr><td align="center">$\unlhd$</td><td align="center">\unlhd</td><td align="center">$\prec$</td><td align="center">\prec</td><td align="center">$\sim$</td><td align="center">\sim</td></tr><tr><td align="center">$\cong$</td><td align="center">\cong</td><td align="center">$\ll$</td><td align="center">\ll</td><td align="center">$\in$</td><td align="center">\in</td></tr><tr><td align="center">$\ldots$</td><td align="center">\ldots</td><td align="center">$\vdots$</td><td align="center">\vdots</td><td align="center">$\imath$</td><td align="center">\imath</td></tr><tr><td align="center">$\int$</td><td align="center">\int</td><td align="center">$\star$</td><td align="center">\star</td><td align="center">$\bullet$</td><td align="center">\bullet</td></tr><tr><td align="center">$\infty$</td><td align="center">\infty</td><td align="center">$\rhd$</td><td align="center">\rhd</td><td align="center">$\unrhd$</td><td align="center">\unrhd</td></tr><tr><td align="center">$\succ$</td><td align="center">\succ</td><td align="center">$\approx$</td><td align="center">\approx</td><td align="center">$\doteq$</td><td align="center">\doteq</td></tr><tr><td align="center">$\gg$</td><td align="center">\gg</td><td align="center">$\notin$</td><td align="center">\notin</td><td align="center">$\cdots$</td><td align="center">\cdots</td></tr><tr><td align="center">$\ddots$</td><td align="center">\ddots</td><td align="center">$\jmath$</td><td align="center">\jmath</td><td align="center">$\oint$</td><td align="center">\oint</td></tr><tr><td align="center">$\triangleleft$</td><td align="center">\triangleleft</td><td align="center">$\bigtriangleup$</td><td align="center">\bigtriangle</td><td align="center">$\uparrow$</td><td align="center">\uparrow</td></tr><tr><td align="center">$\leftarrow$</td><td align="center">\leftarrow</td><td align="center">$\Leftarrow$</td><td align="center">\Leftarrow</td><td align="center">$\longleftarrow$</td><td align="center">\longleftarrow</td></tr><tr><td align="center">$\Longleftarrow$</td><td align="center">\Longleftarrow</td><td align="center">$\leftrightarrow$</td><td align="center">\leftrightarrow</td><td align="center">$\searrow$</td><td align="center">\searrow</td></tr><tr><td align="center">$\leftharpoonup$</td><td align="center">\leftharpoonup</td><td align="center">$\leftharpoondown$</td><td align="center">\leftharpoondown</td><td align="center">$\swarrow$</td><td align="center">\swarrow</td></tr><tr><td align="center">$\nwarrow$</td><td align="center">\nwarrow</td><td align="center">$\rightleftharpoons$</td><td align="center">\rightleftharpoons</td><td align="center">$\triangle$</td><td align="center">\triangle</td></tr><tr><td align="center">$\diamond$</td><td align="center">\diamond</td><td align="center">$\heartsuit$</td><td align="center">\heartsuit</td><td align="center">$\spadesuit$</td><td align="center">\spadesuit</td></tr><tr><td align="center">$\triangleright$</td><td align="center">\triangleright</td><td align="center">$\bigtriangledown$</td><td align="center">\bigtriangledown</td><td align="center">$\downarrow$</td><td align="center">\downarrow</td></tr><tr><td align="center">$\rightarrow$</td><td align="center">\rightarrow</td><td align="center">$\Rightarrow$</td><td align="center">\Rightarrow</td><td align="center">$\nearrow$</td><td align="center">\nearrow</td></tr><tr><td align="center">$\Longrightarrow$</td><td align="center">\Longrightarrow</td><td align="center">$\longleftrightarrow$</td><td align="center">\longleftrightarrow</td><td align="center">$\S$</td><td align="center">\S</td></tr><tr><td align="center">$\rightharpoonup$</td><td align="center">\rightharpoonup</td><td align="center">$\rightharpoondown$</td><td align="center">\rightharpoondown</td><td align="center">$\diamondsuit$</td><td align="center">\diamondsuit</td></tr><tr><td align="center">$\longrightarrow$</td><td align="center">\longrightarrow</td><td align="center">$\Leftrightarrow$</td><td align="center">\Leftrightarrow</td><td align="center">$\angle$</td><td align="center">\angle</td></tr><tr><td align="center">$\clubsuit$</td><td align="center">\clubsuit</td><td align="center">$\Longleftrightarrow$</td><td align="center">\Longleftrightarrow</td><td align="center">$\because$</td><td align="center">\because</td></tr><tr><td align="center">$\therefore$</td><td align="center">\therefore</td><td align="center">$\log$</td><td align="center">\log</td><td align="center">$mod$</td><td align="center">mod</td></tr><tr><td align="center">$\bot$</td><td align="center">\bot</td><td align="center">$sin$</td><td align="center">sin</td><td align="center">$cos$</td><td align="center">cos</td></tr><tr><td align="center">$tan$</td><td align="center">tan</td><td align="center">$cot$</td><td align="center">cot</td><td align="center">$sec$</td><td align="center">sec</td></tr><tr><td align="center">$csc$</td><td align="center">csc</td><td align="center">$lg$</td><td align="center">lg</td><td align="center">$ln$</td><td align="center">ln</td></tr></tbody></table><h5 id="字形字体设置"><a href="#字形字体设置" class="headerlink" title="字形字体设置"></a>字形字体设置</h5><table><thead><tr><th align="center">命令</th><th align="center">实例</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">\boxed</td><td align="center">$\boxed{text}$</td><td align="center">斜体加上文本框</td></tr><tr><td align="center">\fbox</td><td align="center">$\fbox{text}$</td><td align="center">添加文本框</td></tr><tr><td align="center">\mathbf</td><td align="center">$\mathbf{text}$</td><td align="center">字体加粗</td></tr><tr><td align="center">\boldsymbol</td><td align="center">$\boldsymbol{text}$</td><td align="center">斜体再加粗</td></tr><tr><td align="center">A  \large{A}</td><td align="center">A  $\large{A}$</td><td align="center">加大字体</td></tr><tr><td align="center">A  \small{A}</td><td align="center">A  $\small{A}$</td><td align="center">缩小字体</td></tr></tbody></table><h5 id="公式中常出现的式子样式"><a href="#公式中常出现的式子样式" class="headerlink" title="公式中常出现的式子样式"></a>公式中常出现的式子样式</h5><table><thead><tr><th align="center">命令</th><th align="center">实例</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">a^{b}</td><td align="center">$a^{b}$</td><td align="center">上标（单字符可以省略{}）</td></tr><tr><td align="center">a_{b}</td><td align="center">$a_{b}$</td><td align="center">下标（单字符可以省略{}）</td></tr><tr><td align="center">a_{bb}</td><td align="center">$a_{bb}$</td><td align="center">下标（多字符，不可省略{}）</td></tr><tr><td align="center">\sqrt{ab}</td><td align="center">$\sqrt{ab}$</td><td align="center">开平方</td></tr><tr><td align="center">\sqrt[5]{ab}</td><td align="center">$\sqrt[5]{ab}$</td><td align="center">开5次根号，根号下多个字符时用{}</td></tr><tr><td align="center">\sideset{^1_2}{^3_4}\bigotimes</td><td align="center">$\sideset{^1_2}{^3_4}\bigotimes$</td><td align="center">左右都有上下标</td></tr><tr><td align="center">{}^{12}_{\phantom{1}6}\textrm{C}</td><td align="center">${}^{12}_{\phantom{1}6}\textrm{C}$</td><td align="center">上下标都在左边</td></tr><tr><td align="center">\frac{a}{b}</td><td align="center">$\frac{a}{b}$</td><td align="center">分数</td></tr><tr><td align="center">1+\frac{a}{1+\frac{b}{c}}</td><td align="center">$1+\frac{a}{1+\frac{b}{c}}$</td><td align="center">分数，字体逐级变小</td></tr><tr><td align="center">1+\frac{a}{1+\dfrac{b}{c}}</td><td align="center">$1+\frac{a}{1+\dfrac{b}{c}}$</td><td align="center">分数，字号为独立公式的大小</td></tr><tr><td align="center">\binom{a}{b^2}</td><td align="center">$\binom{a}{b^2}$</td><td align="center">组合数</td></tr><tr><td align="center">\dbinom{a}{b^2}</td><td align="center">$\dbinom{a}{b^2}$</td><td align="center">组合数</td></tr><tr><td align="center">\tbinom{a}{b^2}</td><td align="center">$\tbinom{a}{b^2}$</td><td align="center">组合数</td></tr><tr><td align="center">\stackrel{a}{b}</td><td align="center">$\stackrel{a}{b}$</td><td align="center">下面字符大，上面字符小</td></tr><tr><td align="center">{a \atop b+c}</td><td align="center">$a \atop b+c$</td><td align="center">上下符号等大</td></tr><tr><td align="center">{a \choose b+c}</td><td align="center">${a \choose b+c}$</td><td align="center">上下符号等大，带括号</td></tr><tr><td align="center">\sum_{i&#x3D;a}^{b}c_i</td><td align="center">$\sum_{i&#x3D;a}^{b}c_i$</td><td align="center">求和公式  $\Sigma_{i&#x3D;a}^{b}c_i$</td></tr><tr><td align="center">\sum\nolimits_{i&#x3D;a}^{b}c_i</td><td align="center">$\sum\nolimits_{i&#x3D;a}^{b}c_i$</td><td align="center">limits和nolimits是否压缩</td></tr><tr><td align="center">\prod_{i&#x3D;a}^{b}c_i</td><td align="center">$\prod_{i&#x3D;a}^{b}ci$</td><td align="center">求积公式</td></tr><tr><td align="center">\prod\nolimits_{i&#x3D;a}^{b}_c_i</td><td align="center">$\prod\nolimits_{i&#x3D;a}^{b}ci$</td><td align="center">limits和nolimits是否压缩</td></tr><tr><td align="center">\int_{a}^{b}f(x)dx</td><td align="center">$\int_{a}^{b}f(x)dx$</td><td align="center">求积分</td></tr><tr><td align="center">\int\nolimits_{a}^{b}f(x)dx</td><td align="center">$\int\nolimits_{a}^{b}f(x)dx$</td><td align="center">limits和nolimits是否压缩</td></tr><tr><td align="center">\iint</td><td align="center">$\iint$</td><td align="center">二重积分</td></tr><tr><td align="center">\iiint</td><td align="center">$\iiint$</td><td align="center">三重积分</td></tr><tr><td align="center">\idotsint</td><td align="center">$\idotsint$</td><td align="center">积分形式</td></tr><tr><td align="center">\xleftarrow[x+y]{x}</td><td align="center">$\xleftarrow[x+y]{x}$</td><td align="center">可自行调整</td></tr><tr><td align="center">\xrightarrow[x+y]{x}</td><td align="center">$\xrightarrow[x+y]{x}$</td><td align="center">可自行调整</td></tr><tr><td align="center">\overset{x+y}{\rightarrow}</td><td align="center">$\overset{x+y}{\rightarrow}$</td><td align="center">长度固定，适用单字符</td></tr><tr><td align="center">\overrightarrow{x+y}</td><td align="center">$\overrightarrow{x+y}$</td><td align="center">长度不固定，适用多字符</td></tr><tr><td align="center">\underrightarrow{x+y}</td><td align="center">$\underrightarrow{x+y}$</td><td align="center">长度不固定，适用多字符</td></tr><tr><td align="center">\bar{a}</td><td align="center">$\bar{a}$</td><td align="center">单个字母上面加上横线</td></tr><tr><td align="center">\vec{x}</td><td align="center">$\vec{x}$</td><td align="center">向量，单个字母</td></tr><tr><td align="center">\overrightarrow{AB}</td><td align="center">$\overrightarrow{AB}$</td><td align="center">向量，多个字母</td></tr><tr><td align="center">\overleftarrow{AB}</td><td align="center">$\overleftarrow{AB}$</td><td align="center">向量，多个字母</td></tr><tr><td align="center">\tilde{x}</td><td align="center">$\tilde{x}$</td><td align="center">波浪线，单个字母</td></tr><tr><td align="center">\widetilde{xyz}</td><td align="center">$\widetilde{xyz}$</td><td align="center">波浪线，多个字符</td></tr><tr><td align="center">\dot{x}</td><td align="center">$\dot{x}$</td><td align="center">点</td></tr><tr><td align="center">\hat{x}</td><td align="center">$\hat{x}$</td><td align="center">尖帽</td></tr><tr><td align="center">\widehat{x}</td><td align="center">$\widehat{xyz}$</td><td align="center">大尖帽</td></tr><tr><td align="center">\grave{x}</td><td align="center">$\grave{x}$</td><td align="center">声调</td></tr><tr><td align="center">\mathring{x}</td><td align="center">$\mathring{x}$</td><td align="center">声调</td></tr><tr><td align="center">\ddot{x}</td><td align="center">$\ddot{x}$</td><td align="center">声调</td></tr><tr><td align="center">\check{x}</td><td align="center">$\check{x}$</td><td align="center">声调</td></tr><tr><td align="center">\breve{x}</td><td align="center">$\breve{x}$</td><td align="center">声调</td></tr><tr><td align="center">\dddot{x}</td><td align="center">$\dddot{x}$</td><td align="center">声调</td></tr><tr><td align="center">(a^b)</td><td align="center">$(a^b)$</td><td align="center">括号</td></tr><tr><td align="center">\left(a^b\right)</td><td align="center">$\left(a^b\right)$</td><td align="center">括号，可变大小</td></tr><tr><td align="center">{a^b}</td><td align="center">${a^b}$</td><td align="center">括号</td></tr><tr><td align="center">\left\lbrace a^b \right\rbrace</td><td align="center">$\left\lbrace a^b \right\rbrace$</td><td align="center">括号，可变大小</td></tr><tr><td align="center">[a^b]</td><td align="center">$[a^b]$</td><td align="center">括号</td></tr><tr><td align="center">\left[ a^b \right]</td><td align="center">$\left[ a^b \right]$</td><td align="center">括号，可变大小</td></tr><tr><td align="center">\lfloor a^b \rfloor</td><td align="center">$\lfloor a^b \rfloor$</td><td align="center">括号</td></tr><tr><td align="center">\lceil a^b \rceil</td><td align="center">$\lceil a^b \rceil$</td><td align="center">括号</td></tr><tr><td align="center">\overline{a+b}</td><td align="center">$\overline{a+b}$</td><td align="center">多个字母上面加横线</td></tr><tr><td align="center">\overbrace{a\dots a}^{n}</td><td align="center">$\overbrace{a \dots a}^{n}$</td><td align="center">括号在上面</td></tr><tr><td align="center">\underbrace{a \dots a}_{n}</td><td align="center">$\underbrace{a \dots a}_{n}$</td><td align="center">括号在下面</td></tr><tr><td align="center">a \quad b</td><td align="center">$a \quad b$</td><td align="center">一个m的宽度</td></tr><tr><td align="center">a &lt;!–qquad–&gt; b</td><td align="center">$a \qquad b$</td><td align="center">两个m的宽度</td></tr><tr><td align="center">a : b</td><td align="center">$a : b$</td><td align="center">1&#x2F;3个m的宽度</td></tr><tr><td align="center">a : b</td><td align="center">$a ; b$</td><td align="center">2&#x2F;7个m的宽度</td></tr><tr><td align="center">a , b</td><td align="center">$a , b$</td><td align="center">1&#x2F;6个m的宽度</td></tr><tr><td align="center">ab</td><td align="center">$ab$</td><td align="center">没有空格</td></tr><tr><td align="center">a ! b</td><td align="center">$a ! b$</td><td align="center">缩进1&#x2F;6个m的宽度</td></tr></tbody></table><p>公式中括号的应用，可以用一系列命令 (\big, \Big, \bigg, \Bigg) 改变括号大小，例如</p>\Bigg( \bigg( \Big( \big((x) \big) \Big) \bigg) \Bigg) $$ \Bigg\{ \bigg\{ \Big\{ \big\{\{x\} \big\} \Big\} \bigg\} \Bigg\}$\Bigg( \bigg( \Big( \big((x) \big) \Big) \bigg) \Bigg) $$ \Bigg\{ \bigg\{ \Big\{ \big\{\{x\} \big\} \Big\} \bigg\} \Bigg\}$ {{sensitive}}<p>也可以用自动模式自动调节大小<br>$$<br>f(x,y,z) &#x3D; 3y^2z \left( 3+\frac{7x+5}{1+y^2} \right)<br>$$</p><p>$$<br>f\left(\left[\frac{1+\left{x,y\right}}{\left(\frac{x}{y}+\frac{y}{x}\right)\left(u+1\right)}+a\right]^{3&#x2F;2}\right)<br>$$</p><p>其中“\begin{aligned}”与“\end{aligned}”开辟一个环境，可以换行。<br>$$<br>\begin{aligned}a&#x3D;&amp;\left(1+2+3+ \cdots \right. \&amp; \cdots+ \left. \infty-2+\infty-1+\infty\right)\end{aligned}<br>$$<br>分隔符\middle的作用，以及\可以公式换行<br>$$<br>\begin{aligned} P&#x3D;\left(A&#x3D;2|\frac{A^2}{B}&gt;4\right) \  P&#x3D;\left(A&#x3D;2\middle|\frac{A^2}{B}&gt;4\right) \end{aligned}<br>$$<br>在单行文本中，不是只能写一行公式，而是整个公式占用一行，这里用到了 cases 环境，把多个情况放在一个公式中，每个情况用\换行<br>$$<br>L(Y,f(X))&#x3D;\begin{cases}1,\quad &amp;Y\neq f(X)\0,\quad &amp;Y&#x3D;f(X)\end{cases}<br>$$<br>在公式环境下编写公式，公式环境有很多种，这里列举一些常用环境。例如 equation<br>环境，公式放在这个环境中，自动居中对齐，带有公式编号<br>$$<br>\begin{equation}f(x)&#x3D;3x^2+6(x-2)-1\end{equation}<br>$$<br>$\begin{equation}f(x)&#x3D;3x^{2}+6(x-2)-1 \quad\tag{1}\end{equation} $做不到居中，typora中还是得ctrl+shift+m，行间公式且自动带编号（偏好设置里加上）。</p><p>\begin{equation}\begin{aligned}x&#x3D;&amp;\left(a+b+c \right. \&amp;\left. +d+e+f \right)\end{aligned}\end{equation}两个&amp;标明了换行后对齐的位置</p><p>$\begin{equation}\begin{aligned}x&#x3D;&amp;\left(a+b+c \right. \&amp;\left. +d+e+f \right)\end{aligned} \quad \tag{2}\end{equation}$   </p><p>\left.\begin{aligned}x+y &amp;&gt; 5 \ x-y &amp;&gt; 11 \end{aligned}\ \right}\Rightarrow x^2-y^2&gt;55</p><p>还可以把括号放在左边，只需要换一下“影子括号”位置就可以了。<br>$$<br>\left.\begin{aligned}x+y &amp;&gt; 5 \ x-y &amp;&gt; 11 \end{aligned}\ \right}\Rightarrow x^2-y^2&gt;55<br>$$<br>在 equation 环境中添加 array 环境，就可以实现数组或者表格的形式，其中每个元素用 &amp; 分隔， \hline表示横线。公式中如果有中文，就要用\text{}或者\mbox{}装载，否则不能正常输出中文。<br>$$<br>\begin{equation}\begin{array}{c|l|c|r}<br>n &amp; \text{左对齐} &amp; \text{居中对齐} &amp; \text{右对齐} \<br>\hline1 &amp; 0.24 &amp; 1 &amp; 125 \<br>\hline2 &amp; -1 &amp; 189 &amp; -8 \<br>\hline3 &amp; -20 &amp; 2000 &amp; 1+10i<br>\end{array}\end{equation}<br>$$<br>单行文本也可以表示矩阵和公式数组。</p><p>$$<br>\left(\begin{array}{ccc|c}<br>a11 &amp; a12 &amp; a13 &amp; b1 \<br>a21 &amp; a22 &amp; a23 &amp; b2 \<br>a31 &amp; a32 &amp; a33 &amp; b3 \<br>\end{array}\right)<br>$$<br><code>\left\&#123;</code> 表示一个左大括号，它会自动调整大小以适应其后的内容。通常，<code>\left\&#123;</code> 配合 <code>\right\&#125;</code> 使用，<code>\right\&#125;</code> 表示相应的右大括号，以确保左右括号大小一致且适应所包裹内容的大小。\right.表示不显示右边的大括号。\begin{array}和\end{array}表示数组、表格等环境，{ccc|c}表示表格格式，比如有几列等等。<br>$$<br>\left{<br>\begin{array}{c}<br>a_1x+b_1y+c_1z&#x3D;d_1 \<br>a_2x+b_2y+c_2z&#x3D;d_2 \<br>a_3x+b_3y+c_3z&#x3D;d_3<br>\end{array}<br>\right.$<br>$$<br><img src="/2024/04/08/latex-ru-men-xue-xi/矩阵.png" alt="latex公式矩阵表达" style="zoom:60%;"><br>$$<br>\begin{matrix}<br>a &amp;b &amp;c \<br>d &amp;e &amp;f \<br>g &amp;h &amp;j<br>\end{matrix}<br>$$</p><p>$$<br>\begin{bmatrix}<br>a &amp;b &amp;c \<br>d &amp;e &amp;f \<br>g &amp;h &amp;j<br>\end{bmatrix}<br>$$</p><p>$$<br>\left(<br>\begin{matrix}<br>a &amp;b &amp;c \<br>d &amp;e &amp;f \<br>g &amp;h &amp;j<br>\end{matrix}<br>\right)<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> Markdown </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Typora </tag>
            
            <tag> Markdown </tag>
            
            <tag> Latex </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/04/03/yolov5s-wang-luo-jie-gou/"/>
      <url>/2024/04/03/yolov5s-wang-luo-jie-gou/</url>
      
        <content type="html"><![CDATA[<h5 id="yolov5s-网络结构"><a href="#yolov5s-网络结构" class="headerlink" title="yolov5s 网络结构"></a>yolov5s 网络结构</h5><p>Parameter name: epoch    Parameter value:-1<br>Parameter name: best_fitness     Parameter value:[    0.65574]<br>Parameter name: training_results         Parameter value:None<br>Parameter name: model    Parameter value:Model(<br>  (model): Sequential(<br>    (0): Focus(<br>      (conv): Conv(<br>        (conv): Conv2d(12, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(32, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>    )<br>    (1): Conv(<br>      (conv): Conv2d(32, 64, kernel_size&#x3D;(3, 3), stride&#x3D;(2, 2), padding&#x3D;(1, 1), bias&#x3D;False)<br>      (bn): BatchNorm2d(64, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>      (act): SiLU()<br>    )<br>    (2): C3(<br>      (cv1): Conv(<br>        (conv): Conv2d(64, 32, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(32, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv2): Conv(<br>        (conv): Conv2d(64, 32, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(32, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv3): Conv(<br>        (conv): Conv2d(64, 64, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(64, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (m): Sequential(<br>        (0): Bottleneck(<br>          (cv1): Conv(<br>            (conv): Conv2d(32, 32, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(32, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>          (cv2): Conv(<br>            (conv): Conv2d(32, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(32, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>        )<br>      )<br>    )<br>    (3): Conv(<br>      (conv): Conv2d(64, 128, kernel_size&#x3D;(3, 3), stride&#x3D;(2, 2), padding&#x3D;(1, 1), bias&#x3D;False)<br>      (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>      (act): SiLU()<br>    )<br>    (4): C3(<br>      (cv1): Conv(<br>        (conv): Conv2d(128, 64, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(64, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv2): Conv(<br>        (conv): Conv2d(128, 64, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(64, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv3): Conv(<br>        (conv): Conv2d(128, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (m): Sequential(<br>        (0): Bottleneck(<br>          (cv1): Conv(<br>            (conv): Conv2d(64, 64, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(64, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>          (cv2): Conv(<br>            (conv): Conv2d(64, 64, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(64, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>        )<br>        (1): Bottleneck(<br>          (cv1): Conv(<br>            (conv): Conv2d(64, 64, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(64, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>          (cv2): Conv(<br>            (conv): Conv2d(64, 64, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(64, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>        )<br>        (2): Bottleneck(<br>          (cv1): Conv(<br>            (conv): Conv2d(64, 64, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(64, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>          (cv2): Conv(<br>            (conv): Conv2d(64, 64, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(64, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>        )<br>      )<br>    )<br>    (5): Conv(<br>      (conv): Conv2d(128, 256, kernel_size&#x3D;(3, 3), stride&#x3D;(2, 2), padding&#x3D;(1, 1), bias&#x3D;False)<br>      (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>      (act): SiLU()<br>    )<br>    (6): C3(<br>      (cv1): Conv(<br>        (conv): Conv2d(256, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv2): Conv(<br>        (conv): Conv2d(256, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv3): Conv(<br>        (conv): Conv2d(256, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (m): Sequential(<br>        (0): Bottleneck(<br>          (cv1): Conv(<br>            (conv): Conv2d(128, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>          (cv2): Conv(<br>            (conv): Conv2d(128, 128, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>        )<br>        (1): Bottleneck(<br>          (cv1): Conv(<br>            (conv): Conv2d(128, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>          (cv2): Conv(<br>            (conv): Conv2d(128, 128, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>        )<br>        (2): Bottleneck(<br>          (cv1): Conv(<br>            (conv): Conv2d(128, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>          (cv2): Conv(<br>            (conv): Conv2d(128, 128, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>        )<br>      )<br>    )<br>    (7): Conv(<br>      (conv): Conv2d(256, 512, kernel_size&#x3D;(3, 3), stride&#x3D;(2, 2), padding&#x3D;(1, 1), bias&#x3D;False)<br>      (bn): BatchNorm2d(512, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>      (act): SiLU()<br>    )<br>    (8): SPP(<br>      (cv1): Conv(<br>        (conv): Conv2d(512, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv2): Conv(<br>        (conv): Conv2d(1024, 512, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(512, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (m): ModuleList(<br>        (0): MaxPool2d(kernel_size&#x3D;5, stride&#x3D;1, padding&#x3D;2, dilation&#x3D;1, ceil_mode&#x3D;False)<br>        (1): MaxPool2d(kernel_size&#x3D;9, stride&#x3D;1, padding&#x3D;4, dilation&#x3D;1, ceil_mode&#x3D;False)<br>        (2): MaxPool2d(kernel_size&#x3D;13, stride&#x3D;1, padding&#x3D;6, dilation&#x3D;1, ceil_mode&#x3D;False)<br>      )<br>    )<br>    (9): C3(<br>      (cv1): Conv(<br>        (conv): Conv2d(512, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv2): Conv(<br>        (conv): Conv2d(512, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv3): Conv(<br>        (conv): Conv2d(512, 512, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(512, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (m): Sequential(<br>        (0): Bottleneck(<br>          (cv1): Conv(<br>            (conv): Conv2d(256, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>          (cv2): Conv(<br>            (conv): Conv2d(256, 256, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>        )<br>      )<br>    )<br>    (10): Conv(<br>      (conv): Conv2d(512, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>      (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>      (act): SiLU()<br>    )<br>    (11): Upsample(scale_factor&#x3D;2.0, mode&#x3D;’nearest’)<br>    (12): Concat()<br>    (13): C3(<br>      (cv1): Conv(<br>        (conv): Conv2d(512, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv2): Conv(<br>        (conv): Conv2d(512, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv3): Conv(<br>        (conv): Conv2d(256, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (m): Sequential(<br>        (0): Bottleneck(<br>          (cv1): Conv(<br>            (conv): Conv2d(128, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>          (cv2): Conv(<br>            (conv): Conv2d(128, 128, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>        )<br>      )<br>    )<br>    (14): Conv(<br>      (conv): Conv2d(256, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>      (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>      (act): SiLU()<br>    )<br>    (15): Upsample(scale_factor&#x3D;2.0, mode&#x3D;’nearest’)<br>    (16): Concat()<br>    (17): C3(<br>      (cv1): Conv(<br>        (conv): Conv2d(256, 64, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(64, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv2): Conv(<br>        (conv): Conv2d(256, 64, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(64, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv3): Conv(<br>        (conv): Conv2d(128, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (m): Sequential(<br>        (0): Bottleneck(<br>          (cv1): Conv(<br>            (conv): Conv2d(64, 64, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(64, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>          (cv2): Conv(<br>            (conv): Conv2d(64, 64, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(64, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>        )<br>      )<br>    )<br>    (18): Conv(<br>      (conv): Conv2d(128, 128, kernel_size&#x3D;(3, 3), stride&#x3D;(2, 2), padding&#x3D;(1, 1), bias&#x3D;False)<br>      (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>      (act): SiLU()<br>    )<br>    (19): Concat()<br>    (20): C3(<br>      (cv1): Conv(<br>        (conv): Conv2d(256, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv2): Conv(<br>        (conv): Conv2d(256, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv3): Conv(<br>        (conv): Conv2d(256, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (m): Sequential(<br>        (0): Bottleneck(<br>          (cv1): Conv(<br>            (conv): Conv2d(128, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>          (cv2): Conv(<br>            (conv): Conv2d(128, 128, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>        )<br>      )<br>    )<br>    (21): Conv(<br>      (conv): Conv2d(256, 256, kernel_size&#x3D;(3, 3), stride&#x3D;(2, 2), padding&#x3D;(1, 1), bias&#x3D;False)<br>      (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>      (act): SiLU()<br>    )<br>    (22): Concat()<br>    (23): C3(<br>      (cv1): Conv(<br>        (conv): Conv2d(512, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv2): Conv(<br>        (conv): Conv2d(512, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv3): Conv(<br>        (conv): Conv2d(512, 512, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(512, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (m): Sequential(<br>        (0): Bottleneck(<br>          (cv1): Conv(<br>            (conv): Conv2d(256, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>          (cv2): Conv(<br>            (conv): Conv2d(256, 256, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>        )<br>      )<br>    )<br>    (24): Detect(<br>      (m): ModuleList(<br>        (0): Conv2d(128, 21, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1))<br>        (1): Conv2d(256, 21, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1))<br>        (2): Conv2d(512, 21, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1))<br>      )<br>    )<br>  )<br>)<br>Parameter name: ema      Parameter value:None<br>Parameter name: updates          Parameter value:None<br>Parameter name: optimizer        Parameter value:None<br>Parameter name: wandb_id         Parameter value:None</p><h5 id="各层参数的形状"><a href="#各层参数的形状" class="headerlink" title="各层参数的形状"></a>各层参数的形状</h5><p>model.0.conv.conv.weight         torch.Size([32, 12, 3, 3])<br>model.0.conv.conv.bias   torch.Size([32])<br>model.1.conv.weight      torch.Size([64, 32, 3, 3])<br>model.1.conv.bias        torch.Size([64])<br>model.2.cv1.conv.weight          torch.Size([32, 64, 1, 1])<br>model.2.cv1.conv.bias    torch.Size([32])<br>model.2.cv2.conv.weight          torch.Size([32, 64, 1, 1])<br>model.2.cv2.conv.bias    torch.Size([32])<br>model.2.cv3.conv.weight          torch.Size([64, 64, 1, 1])<br>model.2.cv3.conv.bias    torch.Size([64])<br>model.2.m.0.cv1.conv.weight      torch.Size([32, 32, 1, 1])<br>model.2.m.0.cv1.conv.bias        torch.Size([32])<br>model.2.m.0.cv2.conv.weight      torch.Size([32, 32, 3, 3])<br>model.2.m.0.cv2.conv.bias        torch.Size([32])<br>model.3.conv.weight      torch.Size([128, 64, 3, 3])<br>model.3.conv.bias        torch.Size([128])<br>model.4.cv1.conv.weight          torch.Size([64, 128, 1, 1])<br>model.4.cv1.conv.bias    torch.Size([64])<br>model.4.cv2.conv.weight          torch.Size([64, 128, 1, 1])<br>model.4.cv2.conv.bias    torch.Size([64])<br>model.4.cv3.conv.weight          torch.Size([128, 128, 1, 1])<br>model.4.cv3.conv.bias    torch.Size([128])<br>model.4.m.0.cv1.conv.weight      torch.Size([64, 64, 1, 1])<br>model.4.m.0.cv1.conv.bias        torch.Size([64])<br>model.4.m.0.cv2.conv.weight      torch.Size([64, 64, 3, 3])<br>model.4.m.0.cv2.conv.bias        torch.Size([64])<br>model.4.m.1.cv1.conv.weight      torch.Size([64, 64, 1, 1])<br>model.4.m.1.cv1.conv.bias        torch.Size([64])<br>model.4.m.1.cv2.conv.weight      torch.Size([64, 64, 3, 3])<br>model.4.m.1.cv2.conv.bias        torch.Size([64])<br>model.4.m.2.cv1.conv.weight      torch.Size([64, 64, 1, 1])<br>model.4.m.2.cv1.conv.bias        torch.Size([64])<br>model.4.m.2.cv2.conv.weight      torch.Size([64, 64, 3, 3])<br>model.4.m.2.cv2.conv.bias        torch.Size([64])<br>model.5.conv.weight      torch.Size([256, 128, 3, 3])<br>model.5.conv.bias        torch.Size([256])<br>model.6.cv1.conv.weight          torch.Size([128, 256, 1, 1])<br>model.6.cv1.conv.bias    torch.Size([128])<br>model.6.cv2.conv.weight          torch.Size([128, 256, 1, 1])<br>model.6.cv2.conv.bias    torch.Size([128])<br>model.6.cv3.conv.weight          torch.Size([256, 256, 1, 1])<br>model.6.cv3.conv.bias    torch.Size([256])<br>model.6.m.0.cv1.conv.weight      torch.Size([128, 128, 1, 1])<br>model.6.m.0.cv1.conv.bias        torch.Size([128])<br>model.6.m.0.cv2.conv.weight      torch.Size([128, 128, 3, 3])<br>model.6.m.0.cv2.conv.bias        torch.Size([128])<br>model.6.m.1.cv1.conv.weight      torch.Size([128, 128, 1, 1])<br>model.6.m.1.cv1.conv.bias        torch.Size([128])<br>model.6.m.1.cv2.conv.weight      torch.Size([128, 128, 3, 3])<br>model.6.m.1.cv2.conv.bias        torch.Size([128])<br>model.6.m.2.cv1.conv.weight      torch.Size([128, 128, 1, 1])<br>model.6.m.2.cv1.conv.bias        torch.Size([128])<br>model.6.m.2.cv2.conv.weight      torch.Size([128, 128, 3, 3])<br>model.6.m.2.cv2.conv.bias        torch.Size([128])<br>model.7.conv.weight      torch.Size([512, 256, 3, 3])<br>model.7.conv.bias        torch.Size([512])<br>model.8.cv1.conv.weight          torch.Size([256, 512, 1, 1])<br>model.8.cv1.conv.bias    torch.Size([256])<br>model.8.cv2.conv.weight          torch.Size([512, 1024, 1, 1])<br>model.8.cv2.conv.bias    torch.Size([512])<br>model.9.cv1.conv.weight          torch.Size([256, 512, 1, 1])<br>model.9.cv1.conv.bias    torch.Size([256])<br>model.9.cv2.conv.weight          torch.Size([256, 512, 1, 1])<br>model.9.cv2.conv.bias    torch.Size([256])<br>model.9.cv3.conv.weight          torch.Size([512, 512, 1, 1])<br>model.9.cv3.conv.bias    torch.Size([512])<br>model.9.m.0.cv1.conv.weight      torch.Size([256, 256, 1, 1])<br>model.9.m.0.cv1.conv.bias        torch.Size([256])<br>model.9.m.0.cv2.conv.weight      torch.Size([256, 256, 3, 3])<br>model.9.m.0.cv2.conv.bias        torch.Size([256])<br>model.10.conv.weight     torch.Size([256, 512, 1, 1])<br>model.10.conv.bias       torch.Size([256])<br>model.13.cv1.conv.weight         torch.Size([128, 512, 1, 1])<br>model.13.cv1.conv.bias   torch.Size([128])<br>model.13.cv2.conv.weight         torch.Size([128, 512, 1, 1])<br>model.13.cv2.conv.bias   torch.Size([128])<br>model.13.cv3.conv.weight         torch.Size([256, 256, 1, 1])<br>model.13.cv3.conv.bias   torch.Size([256])<br>model.13.m.0.cv1.conv.weight     torch.Size([128, 128, 1, 1])<br>model.13.m.0.cv1.conv.bias       torch.Size([128])<br>model.13.m.0.cv2.conv.weight     torch.Size([128, 128, 3, 3])<br>model.13.m.0.cv2.conv.bias       torch.Size([128])<br>model.14.conv.weight     torch.Size([128, 256, 1, 1])<br>model.14.conv.bias       torch.Size([128])<br>model.17.cv1.conv.weight         torch.Size([64, 256, 1, 1])<br>model.17.cv1.conv.bias   torch.Size([64])<br>model.17.cv2.conv.weight         torch.Size([64, 256, 1, 1])<br>model.17.cv2.conv.bias   torch.Size([64])<br>model.17.cv3.conv.weight         torch.Size([128, 128, 1, 1])<br>model.17.cv3.conv.bias   torch.Size([128])<br>model.17.m.0.cv1.conv.weight     torch.Size([64, 64, 1, 1])<br>model.17.m.0.cv1.conv.bias       torch.Size([64])<br>model.17.m.0.cv2.conv.weight     torch.Size([64, 64, 3, 3])<br>model.17.m.0.cv2.conv.bias       torch.Size([64])<br>model.18.conv.weight     torch.Size([128, 128, 3, 3])<br>model.18.conv.bias       torch.Size([128])<br>model.20.cv1.conv.weight         torch.Size([128, 256, 1, 1])<br>model.20.cv1.conv.bias   torch.Size([128])<br>model.20.cv2.conv.weight         torch.Size([128, 256, 1, 1])<br>model.20.cv2.conv.bias   torch.Size([128])<br>model.20.cv3.conv.weight         torch.Size([256, 256, 1, 1])<br>model.20.cv3.conv.bias   torch.Size([256])<br>model.20.m.0.cv1.conv.weight     torch.Size([128, 128, 1, 1])<br>model.20.m.0.cv1.conv.bias       torch.Size([128])<br>model.20.m.0.cv2.conv.weight     torch.Size([128, 128, 3, 3])<br>model.20.m.0.cv2.conv.bias       torch.Size([128])<br>model.21.conv.weight     torch.Size([256, 256, 3, 3])<br>model.21.conv.bias       torch.Size([256])<br>model.23.cv1.conv.weight         torch.Size([256, 512, 1, 1])<br>model.23.cv1.conv.bias   torch.Size([256])<br>model.23.cv2.conv.weight         torch.Size([256, 512, 1, 1])<br>model.23.cv2.conv.bias   torch.Size([256])<br>model.23.cv3.conv.weight         torch.Size([512, 512, 1, 1])<br>model.23.cv3.conv.bias   torch.Size([512])<br>model.23.m.0.cv1.conv.weight     torch.Size([256, 256, 1, 1])<br>model.23.m.0.cv1.conv.bias       torch.Size([256])<br>model.23.m.0.cv2.conv.weight     torch.Size([256, 256, 3, 3])<br>model.23.m.0.cv2.conv.bias       torch.Size([256])<br>model.24.m.0.weight      torch.Size([21, 128, 1, 1])<br>model.24.m.0.bias        torch.Size([21])<br>model.24.m.1.weight      torch.Size([21, 256, 1, 1])<br>model.24.m.1.bias        torch.Size([21])<br>model.24.m.2.weight      torch.Size([21, 512, 1, 1])<br>model.24.m.2.bias        torch.Size([21])</p><h5 id="输入为640-640-3时，各层输出形状"><a href="#输入为640-640-3时，各层输出形状" class="headerlink" title="输入为640* 640* 3时，各层输出形状"></a>输入为640* 640* 3时，各层输出形状</h5><h6 id="Layer-type-depth-idx-Output-Shape-Param"><a href="#Layer-type-depth-idx-Output-Shape-Param" class="headerlink" title="Layer (type:depth-idx)                        Output Shape              Param"></a>Layer (type:depth-idx)                        Output Shape              Param</h6><p>Model                                         [1, 25200, 7]             –<br>├─Sequential: 1-1                             –                        –<br>│    └─Focus: 2-1                             [1, 32, 320, 320]         –<br>│    │    └─Conv: 3-1                         [1, 32, 320, 320]         (3,488)<br>│    └─Conv: 2-2                              [1, 64, 160, 160]         –<br>│    │    └─Conv2d: 3-2                       [1, 64, 160, 160]         (18,496)<br>│    │    └─SiLU: 3-3                         [1, 64, 160, 160]         –<br>│    └─C3: 2-3                                [1, 64, 160, 160]         –<br>│    │    └─Conv: 3-4                         [1, 32, 160, 160]         (2,080)<br>│    │    └─Sequential: 3-5                   [1, 32, 160, 160]         (10,304)<br>│    │    └─Conv: 3-6                         [1, 32, 160, 160]         (2,080)<br>│    │    └─Conv: 3-7                         [1, 64, 160, 160]         (4,160)<br>│    └─Conv: 2-4                              [1, 128, 80, 80]          –<br>│    │    └─Conv2d: 3-8                       [1, 128, 80, 80]          (73,856)<br>│    │    └─SiLU: 3-9                         [1, 128, 80, 80]          –<br>│    └─C3: 2-5                                [1, 128, 80, 80]          –<br>│    │    └─Conv: 3-10                        [1, 64, 80, 80]           (8,256)<br>│    │    └─Sequential: 3-11                  [1, 64, 80, 80]           (123,264)<br>│    │    └─Conv: 3-12                        [1, 64, 80, 80]           (8,256)<br>│    │    └─Conv: 3-13                        [1, 128, 80, 80]          (16,512)<br>│    └─Conv: 2-6                              [1, 256, 40, 40]          –<br>│    │    └─Conv2d: 3-14                      [1, 256, 40, 40]          (295,168)<br>│    │    └─SiLU: 3-15                        [1, 256, 40, 40]          –<br>│    └─C3: 2-7                                [1, 256, 40, 40]          –<br>│    │    └─Conv: 3-16                        [1, 128, 40, 40]          (32,896)<br>│    │    └─Sequential: 3-17                  [1, 128, 40, 40]          (492,288)<br>│    │    └─Conv: 3-18                        [1, 128, 40, 40]          (32,896)<br>│    │    └─Conv: 3-19                        [1, 256, 40, 40]          (65,792)<br>│    └─Conv: 2-8                              [1, 512, 20, 20]          –<br>│    │    └─Conv2d: 3-20                      [1, 512, 20, 20]          (1,180,160)<br>│    │    └─SiLU: 3-21                        [1, 512, 20, 20]          –<br>│    └─SPP: 2-9                               [1, 512, 20, 20]          –<br>│    │    └─Conv: 3-22                        [1, 256, 20, 20]          (131,328)<br>│    │    └─ModuleList: 3-23                  –                        –<br>│    │    └─Conv: 3-24                        [1, 512, 20, 20]          (524,800)<br>│    └─C3: 2-10                               [1, 512, 20, 20]          –<br>│    │    └─Conv: 3-25                        [1, 256, 20, 20]          (131,328)<br>│    │    └─Sequential: 3-26                  [1, 256, 20, 20]          (655,872)<br>│    │    └─Conv: 3-27                        [1, 256, 20, 20]          (131,328)<br>│    │    └─Conv: 3-28                        [1, 512, 20, 20]          (262,656)<br>│    └─Conv: 2-11                             [1, 256, 20, 20]          –<br>│    │    └─Conv2d: 3-29                      [1, 256, 20, 20]          (131,328)<br>│    │    └─SiLU: 3-30                        [1, 256, 20, 20]          –<br>│    └─Upsample: 2-12                         [1, 256, 40, 40]          –<br>│    └─Concat: 2-13                           [1, 512, 40, 40]          –<br>│    └─C3: 2-14                               [1, 256, 40, 40]          –<br>│    │    └─Conv: 3-31                        [1, 128, 40, 40]          (65,664)<br>│    │    └─Sequential: 3-32                  [1, 128, 40, 40]          (164,096)<br>│    │    └─Conv: 3-33                        [1, 128, 40, 40]          (65,664)<br>│    │    └─Conv: 3-34                        [1, 256, 40, 40]          (65,792)<br>│    └─Conv: 2-15                             [1, 128, 40, 40]          –<br>│    │    └─Conv2d: 3-35                      [1, 128, 40, 40]          (32,896)<br>│    │    └─SiLU: 3-36                        [1, 128, 40, 40]          –<br>│    └─Upsample: 2-16                         [1, 128, 80, 80]          –<br>│    └─Concat: 2-17                           [1, 256, 80, 80]          –<br>│    └─C3: 2-18                               [1, 128, 80, 80]          –<br>│    │    └─Conv: 3-37                        [1, 64, 80, 80]           (16,448)<br>│    │    └─Sequential: 3-38                  [1, 64, 80, 80]           (41,088)<br>│    │    └─Conv: 3-39                        [1, 64, 80, 80]           (16,448)<br>│    │    └─Conv: 3-40                        [1, 128, 80, 80]          (16,512)<br>│    └─Conv: 2-19                             [1, 128, 40, 40]          –<br>│    │    └─Conv2d: 3-41                      [1, 128, 40, 40]          (147,584)<br>│    │    └─SiLU: 3-42                        [1, 128, 40, 40]          –<br>│    └─Concat: 2-20                           [1, 256, 40, 40]          –<br>│    └─C3: 2-21                               [1, 256, 40, 40]          –<br>│    │    └─Conv: 3-43                        [1, 128, 40, 40]          (32,896)<br>│    │    └─Sequential: 3-44                  [1, 128, 40, 40]          (164,096)<br>│    │    └─Conv: 3-45                        [1, 128, 40, 40]          (32,896)<br>│    │    └─Conv: 3-46                        [1, 256, 40, 40]          (65,792)<br>│    └─Conv: 2-22                             [1, 256, 20, 20]          –<br>│    │    └─Conv2d: 3-47                      [1, 256, 20, 20]          (590,080)<br>│    │    └─SiLU: 3-48                        [1, 256, 20, 20]          –<br>│    └─Concat: 2-23                           [1, 512, 20, 20]          –<br>│    └─C3: 2-24                               [1, 512, 20, 20]          –<br>│    │    └─Conv: 3-49                        [1, 256, 20, 20]          (131,328)<br>│    │    └─Sequential: 3-50                  [1, 256, 20, 20]          (655,872)<br>│    │    └─Conv: 3-51                        [1, 256, 20, 20]          (131,328)<br>│    │    └─Conv: 3-52                        [1, 512, 20, 20]          (262,656)<br>│    └─Detect: 2-25                           [1, 25200, 7]             –</p><h6 id="│-│-└─ModuleList-3-53-–-18-879"><a href="#│-│-└─ModuleList-3-53-–-18-879" class="headerlink" title="│    │    └─ModuleList: 3-53                  –                        (18,879)"></a>│    │    └─ModuleList: 3-53                  –                        (18,879)</h6>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/04/03/hello-world/"/>
      <url>/2024/04/03/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>YOLOv5网络结构详解</title>
      <link href="/2023/04/06/understanding-of-yolo-network-structure-1/"/>
      <url>/2023/04/06/understanding-of-yolo-network-structure-1/</url>
      
        <content type="html"><![CDATA[<h3 id="yolo网络结构详解"><a href="#yolo网络结构详解" class="headerlink" title="yolo网络结构详解"></a>yolo网络结构详解</h3><p>$$<br>\frac{x}{y}&#x3D;z<br>$$</p><p>先来放上整个网络结构的示意图，图中所示为yolov5l的网络结构图</p><p><img src="/2023/04/06/understanding-of-yolo-network-structure-1/yolov5.jpeg" alt="yolov5l整体网络结构(部分参数不正确，以文中介绍为准)"></p><hr><h4 id="一、Backbone结构"><a href="#一、Backbone结构" class="headerlink" title="一、Backbone结构"></a>一、Backbone结构</h4><h5 id="1、Focus模块"><a href="#1、Focus模块" class="headerlink" title="1、Focus模块"></a>1、Focus模块</h5><p>​进入backbone之前，对原始输入图像进行切片操作，每隔一个像素取一个值，图像高宽减半，通道数变为原来4倍，信息基本没有丢失，以yolov5s为例，输入图像640 *640 *3，经过Focus结构，变为  320 *320 *12，如图：640 *640 *3</p><p><img src="/2023/04/06/understanding-of-yolo-network-structure-1/Focus.png" alt="focus示意图"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 展现在网络结构中就是先对输入图像进行切片操作，原始输入640* 640 *3，</span></span><br><span class="line"><span class="comment"># 切片后变为320* 320* 12</span></span><br><span class="line"><span class="comment"># 然后在经过两次卷积，640*640*3--&gt;320*320*12--&gt;320*320*64--&gt;160*160*128</span></span><br><span class="line">(<span class="number">0</span>): Focus(</span><br><span class="line">   (conv): Conv(</span><br><span class="line">     (conv): Conv2d(<span class="number">12</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">(<span class="number">1</span>): Conv(</span><br><span class="line">   (conv): Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line"><span class="comment"># 实现Focus模块的代码</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Focus</span>(nn.Module):</span><br><span class="line">    <span class="comment"># Focus wh information into c-space</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, c1, c2, k=<span class="number">1</span>, s=<span class="number">1</span>, p=<span class="literal">None</span>, g=<span class="number">1</span>, act=<span class="literal">True</span></span>):  <span class="comment"># ch_in, ch_out, kernel, stride, padding, groups</span></span><br><span class="line">        <span class="built_in">super</span>(Focus, self).__init__()</span><br><span class="line">        self.conv = Conv(c1 * <span class="number">4</span>, c2, k, s, p, g, act)</span><br><span class="line">        <span class="comment"># self.contract = Contract(gain=2)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):  <span class="comment"># x(b,c,w,h) -&gt; y(b,4c,w/2,h/2)</span></span><br><span class="line">        <span class="comment"># 对于4维张量x，...表示省略的维度，::2表示步长为2的切片。</span></span><br><span class="line">        <span class="comment"># 步长为2的切片后，在通道维度拼接起来，图像高宽减半，通道数变为原来4倍</span></span><br><span class="line">        <span class="keyword">return</span> self.conv(torch.cat([x[..., ::<span class="number">2</span>, ::<span class="number">2</span>], x[..., <span class="number">1</span>::<span class="number">2</span>, ::<span class="number">2</span>], x[..., ::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>], x[..., <span class="number">1</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>]], <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># return self.conv(self.contract(x))</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="2、BottleNeck模块"><a href="#2、BottleNeck模块" class="headerlink" title="2、BottleNeck模块"></a>2、BottleNeck模块</h5><p>下面两种BottleNeck模块分别用在模型的Backbone和neck部分，BottleNeck1用于Backbone，BottleNeck2用于neck，BottleNeck结构不改变图像高宽和通道数量。</p><p><img src="/2023/04/06/understanding-of-yolo-network-structure-1/BottleNeck.png" alt="BottleNeck示意图"></p><p>由一个1* 1卷积和3* 3卷积，再加上残差连接组成</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Bottleneck</span>(nn.Module):</span><br><span class="line">    <span class="comment"># Standard bottleneck</span></span><br><span class="line">    <span class="comment"># 残差链接块，shortcut表示是否包含捷径路线,即BottleNeck1和即BottleNeck2</span></span><br><span class="line">    <span class="comment"># 由1*1卷积、3*3卷积和残差连接组成</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, c1, c2, shortcut=<span class="literal">True</span>, g=<span class="number">1</span>, e=<span class="number">0.5</span></span>):  <span class="comment"># ch_in, ch_out, shortcut, groups, expansion</span></span><br><span class="line">        <span class="built_in">super</span>(Bottleneck, self).__init__()</span><br><span class="line">        c_ = <span class="built_in">int</span>(c2 * e)  <span class="comment"># hidden channels</span></span><br><span class="line">        self.cv1 = Conv(c1, c_, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.cv2 = Conv(c_, c2, <span class="number">3</span>, <span class="number">1</span>, g=g)</span><br><span class="line">        self.add = shortcut <span class="keyword">and</span> c1 == c2            <span class="comment"># 确保残差连接shortcut前后的通道数一致，保证能够可以相加</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 逐元素相加</span></span><br><span class="line">        <span class="keyword">return</span> x + self.cv2(self.cv1(x)) <span class="keyword">if</span> self.add <span class="keyword">else</span> self.cv2(self.cv1(x))</span><br><span class="line"><span class="comment">### 输入为160*160*64时，输出为160*160*64</span></span><br></pre></td></tr></table></figure><p>第一次进入BottleNeck1，图像形状为160* 160* 64，输出160* 160* 64</p><h5 id="3、C3模块"><a href="#3、C3模块" class="headerlink" title="3、C3模块"></a>3、C3模块</h5><p>如图，C3模块由1* 1卷积和BottleNeck模块组成，构成了yolov5的核心组成部分。整体来说，C3模块先对输入图像做通道数减半的1*1卷积并分支，其中一个分支经过若干个BottleNeck, 然后两个分支在通道维concat，concat后得到的输出通道就和输入通道数一样了，最后再来一个通道数不变的</p><p><img src="/2023/04/06/understanding-of-yolo-network-structure-1/C3.png" alt="C3模块示意图"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">C3</span>(nn.Module):</span><br><span class="line">    <span class="comment"># CSP Bottleneck with 3 convolutions</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, c1, c2, n=<span class="number">1</span>, shortcut=<span class="literal">True</span>, g=<span class="number">1</span>, e=<span class="number">0.5</span></span>):  <span class="comment"># ch_in, ch_out, number, shortcut, groups, expansion</span></span><br><span class="line">        <span class="built_in">super</span>(C3, self).__init__()</span><br><span class="line">        c_ = <span class="built_in">int</span>(c2 * e)  <span class="comment"># hidden channels</span></span><br><span class="line">        self.cv1 = Conv(c1, c_, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.cv2 = Conv(c1, c_, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.cv3 = Conv(<span class="number">2</span> * c_, c2, <span class="number">1</span>)  <span class="comment"># act=FReLU(c2)</span></span><br><span class="line">        self.m = nn.Sequential(*[Bottleneck(c_, c_, shortcut, g, e=<span class="number">1.0</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)])</span><br><span class="line">        <span class="comment"># self.m = nn.Sequential(*[CrossConv(c_, c_, 3, 1, g, 1.0, shortcut) for _ in range(n)])</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 先对输入图像x进行cv1卷积，再来n个Bottleneck块，再和经过cv2卷积的输入图像x在通道维拼接，再经过cv3卷积</span></span><br><span class="line">        <span class="keyword">return</span> self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), dim=<span class="number">1</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;&#x27;&#x27;&#x27;</span></span><br><span class="line">经过C3模块，图像由之前的<span class="number">160</span>*<span class="number">160</span>*<span class="number">128</span>--&gt;(cv1)<span class="number">160</span>*<span class="number">160</span>*<span class="number">64</span>--&gt;(<span class="number">3</span>个BottleNeck)<span class="number">160</span>*<span class="number">160</span>*<span class="number">64</span></span><br><span class="line">  --&gt;(cv2)<span class="number">160</span>*<span class="number">160</span>*<span class="number">64</span>--&gt;</span><br><span class="line">    concat--&gt;<span class="number">160</span>*<span class="number">160</span>*<span class="number">128</span>--&gt;(cv3)<span class="number">160</span>*<span class="number">160</span>*<span class="number">128</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;&#x27;&#x27;&#x27;</span></span><br><span class="line">图像经过一个C3模块，大小和通道数都不发生改变</span><br></pre></td></tr></table></figure><h5 id="4、后续网络走向"><a href="#4、后续网络走向" class="headerlink" title="4、后续网络走向"></a>4、后续网络走向</h5><ul><li>经过第一个C3模块，输出图像为160* 160* 128，然后一个卷积，变为80* 80* 256，此时得到的特征图称为P3</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">3</span>): Conv(</span><br><span class="line">  (conv): Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (act): SiLU(inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><ul><li><p>然后第二个C3模块，输入图像是80* 80* 256，输出图像也为80* 80* 256，然后一个卷积，变为40* 40* 512，此时得到的特征图称为P4</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 第二个C3模块有9个BottleNeck</span><br><span class="line">    (5): Conv(</span><br><span class="line">      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))</span><br><span class="line">      (act): SiLU(inplace=True)</span><br></pre></td></tr></table></figure></li><li><p>然后第三个C3模块，输入图像是40* 40* 512，输出图像也是40* 40* 512，经过一个卷积，变为20* 20* 1024，此时得到的特征图称为P5</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第三个C3模块有9个BOttleNeck</span></span><br><span class="line">    (<span class="number">7</span>): Conv(</span><br><span class="line">      (conv): Conv2d(<span class="number">512</span>, <span class="number">1024</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (act): SiLU(inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></li><li><p>然后第四个C3模块（3个BottleNeck），输出图像是20* 20* 1024，输出图像也为20* 20* 1024，与之前不同的是这里并没有一个卷积使其图像高宽减半，通道数加倍，而是接一个<strong>SPPF</strong>模块</p></li></ul><h5 id="5、SPPF"><a href="#5、SPPF" class="headerlink" title="5、SPPF"></a>5、<strong>SPPF</strong></h5><p>​SSPF模块将经过CBS的x、一次池化后的y1、两次池化后的y2和3次池化后的self.m(y2)先进行拼接，然后再CBS提取特征。 仔细观察不难发现，虽然SSPF对特征图进行了多次池化，但是特征图尺寸并未发生变化，通道数更不会变化，所以后续的4个输出能够在channel维度进行融合。这一模块的主要作用是对高层特征进行提取并融合，在融合的过程中作者多次运用最大池化，尽可能多的去提取高层次的语义特征。</p><p><img src="/2023/04/06/understanding-of-yolo-network-structure-1/SSPF.jpg" alt="SSPF示意图"></p><p>​第三个C3模块结束后，进入SSPF模块，输入为20* 20* 20* 1024，先经过一个卷积cv1，       变为20* 20* 512，然后顺序经过kernel_size为5、9、13的最大池化层并依次输出，图像大小和高宽均不发生变化，最后将这四个输出在通道维concat起来，四个20* 20 <em>512，拼接起来得到20 * 20</em> 2048，然后经过cv2，变为20* 20* 1024</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">(8): SPP(</span><br><span class="line">  (cv1): Conv(</span><br><span class="line">    (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">    (act): SiLU(inplace=True)</span><br><span class="line">  )</span><br><span class="line">  (cv2): Conv(</span><br><span class="line">    (conv): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">    (act): SiLU(inplace=True)</span><br><span class="line">  )</span><br><span class="line">  (m): ModuleList(</span><br><span class="line">    (0): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)</span><br><span class="line">    (1): MaxPool2d(kernel_size=9, stride=1, padding=4, dilation=1, ceil_mode=False)</span><br><span class="line">    (2): MaxPool2d(kernel_size=13, stride=1, padding=6, dilation=1, ceil_mode=False)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>​至此，整个BackBone结构完毕，接下来分别是从第二个C3模块结束、第三个C3模块结束、SPPF模块结束引出的连接BackBone和Head(检测层)的Neck</p><h4 id="二、Neck结构"><a href="#二、Neck结构" class="headerlink" title="二、Neck结构"></a>二、Neck结构</h4><p>按照整体网络结构图中由下至上的顺序</p><h5 id="1、SPPF模块引出的neck"><a href="#1、SPPF模块引出的neck" class="headerlink" title="1、SPPF模块引出的neck"></a>1、SPPF模块引出的neck</h5><p>​经过SPPF模块后，输出图像为20* 20* 1024，先经过一个卷积，通道数减半，变为20* 20* 512（记为<strong>n1</strong>,后面会用到），然后最近邻上采样，高宽加倍变为40* 40* 512，然后和第三个C3模块结束后的输出concat后变为40* 40* 1024，然后经过neck部分左下的C3模块，这里的C3会使输出通道数减半，高宽不变，输出40* 40* 512</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">10</span>): Conv(</span><br><span class="line">      (conv): Conv2d(<span class="number">1024</span>, <span class="number">512</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">11</span>): Upsample(scale_factor=<span class="number">2.0</span>, mode=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line">    (<span class="number">12</span>): Concat()</span><br><span class="line">    (<span class="number">13</span>): C3(</span><br><span class="line">      (cv1): Conv(</span><br><span class="line">        (conv): Conv2d(<span class="number">1024</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (cv2): Conv(</span><br><span class="line">        (conv): Conv2d(<span class="number">1024</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (cv3): Conv(</span><br><span class="line">        (conv): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      <span class="comment"># 这里BottleNeck中的卷积是256，图中有误</span></span><br><span class="line">      (m): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Bottleneck(</span><br><span class="line">          (cv1): Conv(</span><br><span class="line">            (conv): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (cv2): Conv(</span><br><span class="line">            (conv): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">1</span>): Bottleneck(</span><br><span class="line">          (cv1): Conv(</span><br><span class="line">            (conv): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (cv2): Conv(</span><br><span class="line">            (conv): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">2</span>): Bottleneck(</span><br><span class="line">          (cv1): Conv(</span><br><span class="line">            (conv): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (cv2): Conv(</span><br><span class="line">            (conv): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>​得到输出40* 40* 512后，经过一个卷积，通道数减半，得到40* 40* 256（记为<strong>n2</strong>), 然后一个最近邻上采样，得到80* 80* 256，然后和第二个C3模块引出的neck拼接在一起，得到80* 80* 512</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">14</span>): Conv(</span><br><span class="line">  (conv): Conv2d(<span class="number">512</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">)</span><br><span class="line">(<span class="number">15</span>): Upsample(scale_factor=<span class="number">2.0</span>, mode=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line">(<span class="number">16</span>): Concat()</span><br></pre></td></tr></table></figure><p>​然后经过neck中左上角的C3模块，和左下角的C3模块一样，输出的图像高宽不变，通道数减半，关键是进入C3分支的两个卷积的输出通道都缩小了1&#x2F;2（相比BackBone里的C3模块），输入为80* 80* 512，得到输出&#x3D;&#x3D;80* 80* 256（记为<strong>n3</strong>)，同时也是<strong>head1</strong>, 进入检测层&#x3D;&#x3D;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">17</span>): C3(</span><br><span class="line">  (cv1): Conv(</span><br><span class="line">    (conv): Conv2d(<span class="number">512</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">  (cv2): Conv(</span><br><span class="line">    (conv): Conv2d(<span class="number">512</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">  (cv3): Conv(</span><br><span class="line">    (conv): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">  (m): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Bottleneck(</span><br><span class="line">      (cv1): Conv(</span><br><span class="line">        (conv): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (cv2): Conv(</span><br><span class="line">        (conv): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">1</span>): Bottleneck(</span><br><span class="line">      (cv1): Conv(</span><br><span class="line">        (conv): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (cv2): Conv(</span><br><span class="line">        (conv): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">2</span>): Bottleneck(</span><br><span class="line">      (cv1): Conv(</span><br><span class="line">        (conv): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (cv2): Conv(</span><br><span class="line">        (conv): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>​通过左上角的C3模块后，得到80* 80* 256的输出，然后进入一个卷积，通道数不变，高宽减半，输出为40* 40* 256，然后和<strong>n2</strong>的40* 40* 256concat，得到40* 40* 512，然后进入右上角的C3模块，右上角的C3模块和BackBone里的一样，不改变图像高宽和通道数，通过右上角的C3模块，得到输出为&#x3D;&#x3D;40* 40* 512，也即<strong>head2</strong>，进入检测层&#x3D;&#x3D;</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">(20): C3(</span><br><span class="line">      (cv1): Conv(</span><br><span class="line">        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">        (act): SiLU(inplace=True)</span><br><span class="line">      )</span><br><span class="line">      (cv2): Conv(</span><br><span class="line">        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">        (act): SiLU(inplace=True)</span><br><span class="line">      )</span><br><span class="line">      (cv3): Conv(</span><br><span class="line">        (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">        (act): SiLU(inplace=True)</span><br><span class="line">      )</span><br><span class="line">      (m): Sequential(</span><br><span class="line">        (0): Bottleneck(</span><br><span class="line">          (cv1): Conv(</span><br><span class="line">            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">            (act): SiLU(inplace=True)</span><br><span class="line">          )</span><br><span class="line">          (cv2): Conv(</span><br><span class="line">            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">            (act): SiLU(inplace=True)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">        (1): Bottleneck(</span><br><span class="line">          (cv1): Conv(</span><br><span class="line">            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">            (act): SiLU(inplace=True)</span><br><span class="line">          )</span><br><span class="line">          (cv2): Conv(</span><br><span class="line">            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">            (act): SiLU(inplace=True)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">        (2): Bottleneck(</span><br><span class="line">          (cv1): Conv(</span><br><span class="line">            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">            (act): SiLU(inplace=True)</span><br><span class="line">          )</span><br><span class="line">          (cv2): Conv(</span><br><span class="line">            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">            (act): SiLU(inplace=True)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>​通过右上角的C3模块后，输出为40* 40* 512，然后先经过一个卷积，使图像高宽减半、通道数不变，得到20* 20* 512，然后和<strong>n1</strong>的20* 20* 512concat得到20* 20* 1024，然后进入neck的右下角的C3模块，这个C3模块和BackBone里的一样，不改变图像高宽和通道数，通过C3模块后得到输出为&#x3D;&#x3D;20* 20* 1024，也即<strong>head3</strong>，进入检测层&#x3D;&#x3D;</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">(21): Conv(</span><br><span class="line">  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))</span><br><span class="line">  (act): SiLU(inplace=True)</span><br><span class="line">)</span><br><span class="line">(22): Concat()</span><br><span class="line">(23): C3(</span><br><span class="line">  (cv1): Conv(</span><br><span class="line">    (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">    (act): SiLU(inplace=True)</span><br><span class="line">  )</span><br><span class="line">  (cv2): Conv(</span><br><span class="line">    (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">    (act): SiLU(inplace=True)</span><br><span class="line">  )</span><br><span class="line">  (cv3): Conv(</span><br><span class="line">    (conv): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">    (act): SiLU(inplace=True)</span><br><span class="line">  )</span><br><span class="line">  (m): Sequential(</span><br><span class="line">    (0): Bottleneck(</span><br><span class="line">      (cv1): Conv(</span><br><span class="line">        (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">        (act): SiLU(inplace=True)</span><br><span class="line">      )</span><br><span class="line">      (cv2): Conv(</span><br><span class="line">        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        (act): SiLU(inplace=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Bottleneck(</span><br><span class="line">      (cv1): Conv(</span><br><span class="line">        (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">        (act): SiLU(inplace=True)</span><br><span class="line">      )</span><br><span class="line">      (cv2): Conv(</span><br><span class="line">        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        (act): SiLU(inplace=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (2): Bottleneck(</span><br><span class="line">      (cv1): Conv(</span><br><span class="line">        (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">        (act): SiLU(inplace=True)</span><br><span class="line">      )</span><br><span class="line">      (cv2): Conv(</span><br><span class="line">        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        (act): SiLU(inplace=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="三、Head结构"><a href="#三、Head结构" class="headerlink" title="三、Head结构"></a>三、Head结构</h4><p>​经过前面的BackBone结构和Neck结构，得到head1（80* 80* 256）、head2（40* 40* 512）、head3（20* 20* 1024），3个head分别通过3个卷积变为80* 80* 21，40* 40* 21、20* 20* 21。为什么是21？因为我的分类类别数nc&#x3D;2，这里的输出通道数应为3*（nc+5）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">    (<span class="number">24</span>): Detect(</span><br><span class="line">      (m): ModuleList(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">256</span>, <span class="number">21</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">1</span>): Conv2d(<span class="number">512</span>, <span class="number">21</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">2</span>): Conv2d(<span class="number">1024</span>, <span class="number">21</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">)&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Detect</span>(nn.Module):</span><br><span class="line">    stride = <span class="literal">None</span>  <span class="comment"># strides computed during build，特征图的缩放步长</span></span><br><span class="line">    export = <span class="literal">False</span>  <span class="comment"># onnx export，ONNX动态量化</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, nc=<span class="number">80</span>, anchors=(<span class="params"></span>), ch=(<span class="params"></span>)</span>):  <span class="comment"># detection layer</span></span><br><span class="line">        <span class="built_in">super</span>(Detect, self).__init__()</span><br><span class="line">        self.nc = nc  <span class="comment"># number of classes</span></span><br><span class="line">        self.no = nc + <span class="number">5</span>  <span class="comment"># number of outputs per anchor，每个类别的预测置信度+（预测类别+预测坐标）</span></span><br><span class="line">        self.nl = <span class="built_in">len</span>(anchors)  <span class="comment"># number of detection layers # nl: 表示预测层数，yolov5是3层预测</span></span><br><span class="line">        <span class="comment"># na: 表示anchors的数量，除以2是因为[10,13, 16,30, 33,23]这个长度是6，对应3个anchor</span></span><br><span class="line">        self.na = <span class="built_in">len</span>(anchors[<span class="number">0</span>]) // <span class="number">2</span>  <span class="comment"># number of anchors</span></span><br><span class="line">        <span class="comment"># grid: 表示初始化grid列表大小，下面会计算grid，grid就是每个格子的x，y坐标（整数，比如0-19），</span></span><br><span class="line">        <span class="comment"># 左上角为(1,1),右下角为(input.w/stride,input.h/stride)</span></span><br><span class="line">        self.grid = [torch.zeros(<span class="number">1</span>)] * self.nl  <span class="comment"># init grid</span></span><br><span class="line">        <span class="comment"># print(&quot;self.grid: &quot;, self.grid)</span></span><br><span class="line">        a = torch.tensor(anchors).<span class="built_in">float</span>().view(self.nl, -<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># shape(nl,na,2)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用register_buffer方法注册anchors和anchor_grid为模块的缓冲区（buffer），</span></span><br><span class="line">        <span class="comment"># 这样在模型进行训练时，这些参数将被包含在模型的状态中，并且在推理过程中不会被修改。</span></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;anchors&#x27;</span>, a)  <span class="comment"># shape(nl,na,2)</span></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;anchor_grid&#x27;</span>, a.clone().view(self.nl, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>))  <span class="comment"># shape(nl,1,na,1,1,2)</span></span><br><span class="line">        <span class="comment"># ch=(128,256,512),最后的3个1*1卷积</span></span><br><span class="line">        <span class="comment"># 每一张进行三次预测，每一个预测结果包含nc+5个值</span></span><br><span class="line">        <span class="comment"># (n, 255, 80, 80),(n, 255, 40, 40),(n, 255, 20, 20) --&gt; ch=(255, 255, 255)</span></span><br><span class="line">        <span class="comment"># 255 -&gt; (nc+5)*3 ===&gt; 为了提取出预测框的位置信息以及预测框尺寸信息</span></span><br><span class="line">        self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, <span class="number">1</span>) <span class="keyword">for</span> x <span class="keyword">in</span> ch)  <span class="comment"># output conv</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x = x.copy()  # for profiling</span></span><br><span class="line">        z = []  <span class="comment"># inference output</span></span><br><span class="line">        self.training |= self.export</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;self.nl&quot;</span>, self.nl)</span><br><span class="line">        <span class="comment"># 首先进行for循环，每次i的循环，产生一个z。</span></span><br><span class="line">        <span class="comment"># 维度重排列：(n, 255, , ) -&gt; (n, 3, nc+5, ny, nx) -&gt; (n, 3, ny, nx, nc+5)，</span></span><br><span class="line">        <span class="comment"># 三层分别预测了80*80、40*40、20*20次。</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.nl):</span><br><span class="line">            x[i] = self.m[i](x[i])  <span class="comment"># conv，3个output 1*1 conv</span></span><br><span class="line">            bs, _, ny, nx = x[i].shape  <span class="comment"># x(bs,255,20,20) to x(bs,3,20,20,85)</span></span><br><span class="line">            <span class="comment"># print(&quot;ny,nx :&quot;, ny, nx)</span></span><br><span class="line">            <span class="comment"># print(&quot;x[i]: &quot;, x[i].shape)   </span></span><br><span class="line">            <span class="comment"># 维度重排列: bs, 先验框组数, 检测框行数, 检测框列数, 属性数5 + 分类数</span></span><br><span class="line">            x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>).contiguous()      <span class="comment"># .contiguous()确保张量在储存中是连续的</span></span><br><span class="line">            <span class="comment"># print(&quot;x[i]: &quot;, x[i].shape)</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.training:  <span class="comment"># inference</span></span><br><span class="line">                <span class="keyword">if</span> self.grid[i].shape[<span class="number">2</span>:<span class="number">4</span>] != x[i].shape[<span class="number">2</span>:<span class="number">4</span>]:</span><br><span class="line">                    self.grid[i] = self._make_grid(nx, ny).to(x[i].device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># -------------------按损失函数的回归方式来转换坐标---------------------</span></span><br><span class="line">                y = x[i].sigmoid()</span><br><span class="line">                <span class="comment"># 对坐标进行解码，计算预测框的中心坐标。</span></span><br><span class="line">                y[..., <span class="number">0</span>:<span class="number">2</span>] = (y[..., <span class="number">0</span>:<span class="number">2</span>] * <span class="number">2.</span> - <span class="number">0.5</span> + self.grid[i]) * self.stride[i]  <span class="comment"># xy</span></span><br><span class="line">                <span class="comment"># 计算预测框的宽度和高度。</span></span><br><span class="line">                y[..., <span class="number">2</span>:<span class="number">4</span>] = (y[..., <span class="number">2</span>:<span class="number">4</span>] * <span class="number">2</span>) ** <span class="number">2</span> * self.anchor_grid[i]  <span class="comment"># wh</span></span><br><span class="line">                z.append(y.view(bs, -<span class="number">1</span>, self.no))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x <span class="keyword">if</span> self.training <span class="keyword">else</span> (torch.cat(z, <span class="number">1</span>), x)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_grid</span>(<span class="params">nx=<span class="number">20</span>, ny=<span class="number">20</span></span>):</span><br><span class="line">        yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)])</span><br><span class="line">        <span class="keyword">return</span> torch.stack((xv, yv), <span class="number">2</span>).view((<span class="number">1</span>, <span class="number">1</span>, ny, nx, <span class="number">2</span>)).<span class="built_in">float</span>()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>​分类和 bbox 检测等都是在同一个卷积的不同通道中完成，预测结果在通道维得到。</p>]]></content>
      
      
      <categories>
          
          <category> deep learing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> yolo </tag>
            
            <tag> yolov5 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
