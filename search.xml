<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>understanding of yolo network structure</title>
      <link href="/2024/04/05/understanding-of-yolo-network-structure/"/>
      <url>/2024/04/05/understanding-of-yolo-network-structure/</url>
      
        <content type="html"><![CDATA[<h3 id="yolo网络结构详解"><a href="#yolo网络结构详解" class="headerlink" title="yolo网络结构详解"></a>yolo网络结构详解</h3><p>先来放上整个网络结构的示意图，图中所示为yolov5l的网络结构图</p><p><img src="file:///C:/Users/gzh/AppData/Local/Packages/Microsoft.Windows.Photos_8wekyb3d8bbwe/TempState/ShareServiceTempFolder/yolov5.jpeg" alt="img"></p><hr><h4 id="一、Backbone结构"><a href="#一、Backbone结构" class="headerlink" title="一、Backbone结构"></a>一、Backbone结构</h4><h5 id="1、Focus模块"><a href="#1、Focus模块" class="headerlink" title="1、Focus模块"></a>1、Focus模块</h5><p>​进入backbone之前，对原始输入图像进行切片操作，每隔一个像素取一个值，图像高宽减半，通道数变为原来4倍，信息基本没有丢失，以yolov5s为例，输入图像640 *640 *3，经过Focus结构，变为  320 *320 *12，如图：640 *640 *3</p><p><img src="C:\Users\gzh\AppData\Roaming\Typora\typora-user-images\image-20240405130433868.png" alt="image-20240405130433868"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 展现在网络结构中就是先对输入图像进行切片操作，原始输入640* 640 *3，</span></span><br><span class="line"><span class="comment"># 切片后变为320* 320* 12</span></span><br><span class="line"><span class="comment"># 然后在经过两次卷积，640*640*3--&gt;320*320*12--&gt;320*320*64--&gt;160*160*128</span></span><br><span class="line">(<span class="number">0</span>): Focus(</span><br><span class="line">   (conv): Conv(</span><br><span class="line">     (conv): Conv2d(<span class="number">12</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">(<span class="number">1</span>): Conv(</span><br><span class="line">   (conv): Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line"><span class="comment"># 实现Focus模块的代码</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Focus</span>(nn.Module):</span><br><span class="line">    <span class="comment"># Focus wh information into c-space</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, c1, c2, k=<span class="number">1</span>, s=<span class="number">1</span>, p=<span class="literal">None</span>, g=<span class="number">1</span>, act=<span class="literal">True</span></span>):  <span class="comment"># ch_in, ch_out, kernel, stride, padding, groups</span></span><br><span class="line">        <span class="built_in">super</span>(Focus, self).__init__()</span><br><span class="line">        self.conv = Conv(c1 * <span class="number">4</span>, c2, k, s, p, g, act)</span><br><span class="line">        <span class="comment"># self.contract = Contract(gain=2)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):  <span class="comment"># x(b,c,w,h) -&gt; y(b,4c,w/2,h/2)</span></span><br><span class="line">        <span class="comment"># 对于4维张量x，...表示省略的维度，::2表示步长为2的切片。</span></span><br><span class="line">        <span class="comment"># 步长为2的切片后，在通道维度拼接起来，图像高宽减半，通道数变为原来4倍</span></span><br><span class="line">        <span class="keyword">return</span> self.conv(torch.cat([x[..., ::<span class="number">2</span>, ::<span class="number">2</span>], x[..., <span class="number">1</span>::<span class="number">2</span>, ::<span class="number">2</span>], x[..., ::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>], x[..., <span class="number">1</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>]], <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># return self.conv(self.contract(x))</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="2、BottleNeck模块"><a href="#2、BottleNeck模块" class="headerlink" title="2、BottleNeck模块"></a>2、BottleNeck模块</h5><p>下面两种BottleNeck模块分别用在模型的Backbone和neck部分，BottleNeck1用于Backbone，BottleNeck2用于neck，BottleNeck结构不改变图像高宽和通道数量。</p><p><img src="C:\Users\gzh\AppData\Roaming\Typora\typora-user-images\image-20240406173005623.png" alt="image-20240406173005623"></p><p>由一个1* 1卷积和3* 3卷积，再加上残差连接组成</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Bottleneck</span>(nn.Module):</span><br><span class="line">    <span class="comment"># Standard bottleneck</span></span><br><span class="line">    <span class="comment"># 残差链接块，shortcut表示是否包含捷径路线,即BottleNeck1和即BottleNeck2</span></span><br><span class="line">    <span class="comment"># 由1*1卷积、3*3卷积和残差连接组成</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, c1, c2, shortcut=<span class="literal">True</span>, g=<span class="number">1</span>, e=<span class="number">0.5</span></span>):  <span class="comment"># ch_in, ch_out, shortcut, groups, expansion</span></span><br><span class="line">        <span class="built_in">super</span>(Bottleneck, self).__init__()</span><br><span class="line">        c_ = <span class="built_in">int</span>(c2 * e)  <span class="comment"># hidden channels</span></span><br><span class="line">        self.cv1 = Conv(c1, c_, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.cv2 = Conv(c_, c2, <span class="number">3</span>, <span class="number">1</span>, g=g)</span><br><span class="line">        self.add = shortcut <span class="keyword">and</span> c1 == c2            <span class="comment"># 确保残差连接shortcut前后的通道数一致，保证能够可以相加</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 逐元素相加</span></span><br><span class="line">        <span class="keyword">return</span> x + self.cv2(self.cv1(x)) <span class="keyword">if</span> self.add <span class="keyword">else</span> self.cv2(self.cv1(x))</span><br><span class="line"><span class="comment">### 输入为160*160*64时，输出为160*160*64</span></span><br></pre></td></tr></table></figure><p>第一次进入BottleNeck1，图像形状为160* 160* 64，输出160* 160* 64</p><h5 id="3、C3模块"><a href="#3、C3模块" class="headerlink" title="3、C3模块"></a>3、C3模块</h5><p>如图，C3模块由1* 1卷积和BottleNeck模块组成，构成了yolov5的核心组成部分。整体来说，C3模块先对输入图像做通道数减半的1*1卷积并分支，其中一个分支经过若干个BottleNeck, 然后两个分支在通道维concat，concat后得到的输出通道就和输入通道数一样了，最后再来一个通道数不变的</p><p><img src="C:\Users\gzh\AppData\Roaming\Typora\typora-user-images\image-20240406173601482.png" alt="image-20240406173601482"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">C3</span>(nn.Module):</span><br><span class="line">    <span class="comment"># CSP Bottleneck with 3 convolutions</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, c1, c2, n=<span class="number">1</span>, shortcut=<span class="literal">True</span>, g=<span class="number">1</span>, e=<span class="number">0.5</span></span>):  <span class="comment"># ch_in, ch_out, number, shortcut, groups, expansion</span></span><br><span class="line">        <span class="built_in">super</span>(C3, self).__init__()</span><br><span class="line">        c_ = <span class="built_in">int</span>(c2 * e)  <span class="comment"># hidden channels</span></span><br><span class="line">        self.cv1 = Conv(c1, c_, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.cv2 = Conv(c1, c_, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.cv3 = Conv(<span class="number">2</span> * c_, c2, <span class="number">1</span>)  <span class="comment"># act=FReLU(c2)</span></span><br><span class="line">        self.m = nn.Sequential(*[Bottleneck(c_, c_, shortcut, g, e=<span class="number">1.0</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)])</span><br><span class="line">        <span class="comment"># self.m = nn.Sequential(*[CrossConv(c_, c_, 3, 1, g, 1.0, shortcut) for _ in range(n)])</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 先对输入图像x进行cv1卷积，再来n个Bottleneck块，再和经过cv2卷积的输入图像x在通道维拼接，再经过cv3卷积</span></span><br><span class="line">        <span class="keyword">return</span> self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), dim=<span class="number">1</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;&#x27;&#x27;&#x27;</span></span><br><span class="line">经过C3模块，图像由之前的<span class="number">160</span>*<span class="number">160</span>*<span class="number">128</span>--&gt;(cv1)<span class="number">160</span>*<span class="number">160</span>*<span class="number">64</span>--&gt;(<span class="number">3</span>个BottleNeck)<span class="number">160</span>*<span class="number">160</span>*<span class="number">64</span></span><br><span class="line">  --&gt;(cv2)<span class="number">160</span>*<span class="number">160</span>*<span class="number">64</span>--&gt;</span><br><span class="line">    concat--&gt;<span class="number">160</span>*<span class="number">160</span>*<span class="number">128</span>--&gt;(cv3)<span class="number">160</span>*<span class="number">160</span>*<span class="number">128</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;&#x27;&#x27;&#x27;</span></span><br><span class="line">图像经过一个C3模块，大小和通道数都不发生改变</span><br></pre></td></tr></table></figure><h5 id="4、后续网络走向"><a href="#4、后续网络走向" class="headerlink" title="4、后续网络走向"></a>4、后续网络走向</h5><ul><li>经过第一个C3模块，输出图像为160* 160* 128，然后一个卷积，变为80* 80* 256，此时得到的特征图称为P3</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">3</span>): Conv(</span><br><span class="line">  (conv): Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (act): SiLU(inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><ul><li><p>然后第二个C3模块，输入图像是80* 80* 256，输出图像也为80* 80* 256，然后一个卷积，变为40* 40* 512，此时得到的特征图称为P4</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 第二个C3模块有9个BottleNeck</span><br><span class="line">    (5): Conv(</span><br><span class="line">      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))</span><br><span class="line">      (act): SiLU(inplace=True)</span><br></pre></td></tr></table></figure></li><li><p>然后第三个C3模块，输入图像是40* 40* 512，输出图像也是40* 40* 512，经过一个卷积，变为20* 20* 1024，此时得到的特征图称为P5</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第三个C3模块有9个BOttleNeck</span></span><br><span class="line">    (<span class="number">7</span>): Conv(</span><br><span class="line">      (conv): Conv2d(<span class="number">512</span>, <span class="number">1024</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (act): SiLU(inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></li><li><p>然后第四个C3模块（3个BottleNeck），输出图像是20* 20* 1024，输出图像也为20* 20* 1024，与之前不同的是这里并没有一个卷积使其图像高宽减半，通道数加倍，而是接一个<strong>SPPF</strong>模块</p></li></ul><h5 id="5、SPPF"><a href="#5、SPPF" class="headerlink" title="5、SPPF"></a>5、<strong>SPPF</strong></h5><p>​SSPF模块将经过CBS的x、一次池化后的y1、两次池化后的y2和3次池化后的self.m(y2)先进行拼接，然后再CBS提取特征。 仔细观察不难发现，虽然SSPF对特征图进行了多次池化，但是特征图尺寸并未发生变化，通道数更不会变化，所以后续的4个输出能够在channel维度进行融合。这一模块的主要作用是对高层特征进行提取并融合，在融合的过程中作者多次运用最大池化，尽可能多的去提取高层次的语义特征。</p><p><img src="https://pic4.zhimg.com/v2-efd1f08422000773973327b63f427b57_r.jpg" alt="img"></p><p>​第三个C3模块结束后，进入SSPF模块，输入为20* 20* 20* 1024，先经过一个卷积cv1，       变为20* 20* 512，然后顺序经过kernel_size为5、9、13的最大池化层并依次输出，图像大小和高宽均不发生变化，最后将这四个输出在通道维concat起来，四个20* 20 <em>512，拼接起来得到20 * 20</em> 2048，然后经过cv2，变为20* 20* 1024</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">(8): SPP(</span><br><span class="line">  (cv1): Conv(</span><br><span class="line">    (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">    (act): SiLU(inplace=True)</span><br><span class="line">  )</span><br><span class="line">  (cv2): Conv(</span><br><span class="line">    (conv): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">    (act): SiLU(inplace=True)</span><br><span class="line">  )</span><br><span class="line">  (m): ModuleList(</span><br><span class="line">    (0): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)</span><br><span class="line">    (1): MaxPool2d(kernel_size=9, stride=1, padding=4, dilation=1, ceil_mode=False)</span><br><span class="line">    (2): MaxPool2d(kernel_size=13, stride=1, padding=6, dilation=1, ceil_mode=False)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>​至此，整个BackBone结构完毕，接下来分别是从第二个C3模块结束、第三个C3模块结束、SPPF模块结束引出的连接BackBone和Head(检测层)的Neck</p><h4 id="二、Neck结构"><a href="#二、Neck结构" class="headerlink" title="二、Neck结构"></a>二、Neck结构</h4><p>按照整体网络结构图中由下至上的顺序</p><h5 id="1、SPPF模块引出的neck"><a href="#1、SPPF模块引出的neck" class="headerlink" title="1、SPPF模块引出的neck"></a>1、SPPF模块引出的neck</h5><p>​经过SPPF模块后，输出图像为20* 20* 1024，先经过一个卷积，通道数减半，变为20* 20* 512（记为<strong>n1</strong>,后面会用到），然后最近邻上采样，高宽加倍变为40* 40* 512，然后和第三个C3模块结束后的输出concat后变为40* 40* 1024，然后经过neck部分左下的C3模块，这里的C3会使输出通道数减半，高宽不变，输出40* 40* 512</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">10</span>): Conv(</span><br><span class="line">      (conv): Conv2d(<span class="number">1024</span>, <span class="number">512</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">11</span>): Upsample(scale_factor=<span class="number">2.0</span>, mode=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line">    (<span class="number">12</span>): Concat()</span><br><span class="line">    (<span class="number">13</span>): C3(</span><br><span class="line">      (cv1): Conv(</span><br><span class="line">        (conv): Conv2d(<span class="number">1024</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (cv2): Conv(</span><br><span class="line">        (conv): Conv2d(<span class="number">1024</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (cv3): Conv(</span><br><span class="line">        (conv): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      <span class="comment"># 这里BottleNeck中的卷积是256，图中有误</span></span><br><span class="line">      (m): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Bottleneck(</span><br><span class="line">          (cv1): Conv(</span><br><span class="line">            (conv): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (cv2): Conv(</span><br><span class="line">            (conv): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">1</span>): Bottleneck(</span><br><span class="line">          (cv1): Conv(</span><br><span class="line">            (conv): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (cv2): Conv(</span><br><span class="line">            (conv): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">2</span>): Bottleneck(</span><br><span class="line">          (cv1): Conv(</span><br><span class="line">            (conv): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (cv2): Conv(</span><br><span class="line">            (conv): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>​得到输出40* 40* 512后，经过一个卷积，通道数减半，得到40* 40* 256（记为<strong>n2</strong>), 然后一个最近邻上采样，得到80* 80* 256，然后和第二个C3模块引出的neck拼接在一起，得到80* 80* 512</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">14</span>): Conv(</span><br><span class="line">  (conv): Conv2d(<span class="number">512</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">)</span><br><span class="line">(<span class="number">15</span>): Upsample(scale_factor=<span class="number">2.0</span>, mode=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line">(<span class="number">16</span>): Concat()</span><br></pre></td></tr></table></figure><p>​然后经过neck中左上角的C3模块，和左下角的C3模块一样，输出的图像高宽不变，通道数减半，关键是进入C3分支的两个卷积的输出通道都缩小了1&#x2F;2（相比BackBone里的C3模块），输入为80* 80* 512，得到输出&#x3D;&#x3D;80* 80* 256（记为<strong>n3</strong>)，同时也是<strong>head1</strong>, 进入检测层&#x3D;&#x3D;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">17</span>): C3(</span><br><span class="line">  (cv1): Conv(</span><br><span class="line">    (conv): Conv2d(<span class="number">512</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">  (cv2): Conv(</span><br><span class="line">    (conv): Conv2d(<span class="number">512</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">  (cv3): Conv(</span><br><span class="line">    (conv): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">  (m): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Bottleneck(</span><br><span class="line">      (cv1): Conv(</span><br><span class="line">        (conv): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (cv2): Conv(</span><br><span class="line">        (conv): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">1</span>): Bottleneck(</span><br><span class="line">      (cv1): Conv(</span><br><span class="line">        (conv): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (cv2): Conv(</span><br><span class="line">        (conv): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">2</span>): Bottleneck(</span><br><span class="line">      (cv1): Conv(</span><br><span class="line">        (conv): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (cv2): Conv(</span><br><span class="line">        (conv): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>​通过左上角的C3模块后，得到80* 80* 256的输出，然后进入一个卷积，通道数不变，高宽减半，输出为40* 40* 256，然后和<strong>n2</strong>的40* 40* 256concat，得到40* 40* 512，然后进入右上角的C3模块，右上角的C3模块和BackBone里的一样，不改变图像高宽和通道数，通过右上角的C3模块，得到输出为&#x3D;&#x3D;40* 40* 512，也即<strong>head2</strong>，进入检测层&#x3D;&#x3D;</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">(20): C3(</span><br><span class="line">      (cv1): Conv(</span><br><span class="line">        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">        (act): SiLU(inplace=True)</span><br><span class="line">      )</span><br><span class="line">      (cv2): Conv(</span><br><span class="line">        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">        (act): SiLU(inplace=True)</span><br><span class="line">      )</span><br><span class="line">      (cv3): Conv(</span><br><span class="line">        (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">        (act): SiLU(inplace=True)</span><br><span class="line">      )</span><br><span class="line">      (m): Sequential(</span><br><span class="line">        (0): Bottleneck(</span><br><span class="line">          (cv1): Conv(</span><br><span class="line">            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">            (act): SiLU(inplace=True)</span><br><span class="line">          )</span><br><span class="line">          (cv2): Conv(</span><br><span class="line">            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">            (act): SiLU(inplace=True)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">        (1): Bottleneck(</span><br><span class="line">          (cv1): Conv(</span><br><span class="line">            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">            (act): SiLU(inplace=True)</span><br><span class="line">          )</span><br><span class="line">          (cv2): Conv(</span><br><span class="line">            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">            (act): SiLU(inplace=True)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">        (2): Bottleneck(</span><br><span class="line">          (cv1): Conv(</span><br><span class="line">            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">            (act): SiLU(inplace=True)</span><br><span class="line">          )</span><br><span class="line">          (cv2): Conv(</span><br><span class="line">            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">            (act): SiLU(inplace=True)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>​通过右上角的C3模块后，输出为40* 40* 512，然后先经过一个卷积，使图像高宽减半、通道数不变，得到20* 20* 512，然后和<strong>n1</strong>的20* 20* 512concat得到20* 20* 1024，然后进入neck的右下角的C3模块，这个C3模块和BackBone里的一样，不改变图像高宽和通道数，通过C3模块后得到输出为&#x3D;&#x3D;20* 20* 1024，也即<strong>head3</strong>，进入检测层&#x3D;&#x3D;</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">(21): Conv(</span><br><span class="line">  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))</span><br><span class="line">  (act): SiLU(inplace=True)</span><br><span class="line">)</span><br><span class="line">(22): Concat()</span><br><span class="line">(23): C3(</span><br><span class="line">  (cv1): Conv(</span><br><span class="line">    (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">    (act): SiLU(inplace=True)</span><br><span class="line">  )</span><br><span class="line">  (cv2): Conv(</span><br><span class="line">    (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">    (act): SiLU(inplace=True)</span><br><span class="line">  )</span><br><span class="line">  (cv3): Conv(</span><br><span class="line">    (conv): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">    (act): SiLU(inplace=True)</span><br><span class="line">  )</span><br><span class="line">  (m): Sequential(</span><br><span class="line">    (0): Bottleneck(</span><br><span class="line">      (cv1): Conv(</span><br><span class="line">        (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">        (act): SiLU(inplace=True)</span><br><span class="line">      )</span><br><span class="line">      (cv2): Conv(</span><br><span class="line">        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        (act): SiLU(inplace=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Bottleneck(</span><br><span class="line">      (cv1): Conv(</span><br><span class="line">        (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">        (act): SiLU(inplace=True)</span><br><span class="line">      )</span><br><span class="line">      (cv2): Conv(</span><br><span class="line">        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        (act): SiLU(inplace=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (2): Bottleneck(</span><br><span class="line">      (cv1): Conv(</span><br><span class="line">        (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">        (act): SiLU(inplace=True)</span><br><span class="line">      )</span><br><span class="line">      (cv2): Conv(</span><br><span class="line">        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        (act): SiLU(inplace=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="三、Head结构"><a href="#三、Head结构" class="headerlink" title="三、Head结构"></a>三、Head结构</h4><p>​经过前面的BackBone结构和Neck结构，得到head1（80* 80* 256）、head2（40* 40* 512）、head3（20* 20* 1024），3个head分别通过3个卷积变为80* 80* 21，40* 40* 21、20* 20* 21。为什么是21？因为我的分类类别数nc&#x3D;2，这里的输出通道数应为3*（nc+5）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">    (<span class="number">24</span>): Detect(</span><br><span class="line">      (m): ModuleList(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">256</span>, <span class="number">21</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">1</span>): Conv2d(<span class="number">512</span>, <span class="number">21</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">2</span>): Conv2d(<span class="number">1024</span>, <span class="number">21</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">)&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Detect</span>(nn.Module):</span><br><span class="line">    stride = <span class="literal">None</span>  <span class="comment"># strides computed during build，特征图的缩放步长</span></span><br><span class="line">    export = <span class="literal">False</span>  <span class="comment"># onnx export，ONNX动态量化</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, nc=<span class="number">80</span>, anchors=(<span class="params"></span>), ch=(<span class="params"></span>)</span>):  <span class="comment"># detection layer</span></span><br><span class="line">        <span class="built_in">super</span>(Detect, self).__init__()</span><br><span class="line">        self.nc = nc  <span class="comment"># number of classes</span></span><br><span class="line">        self.no = nc + <span class="number">5</span>  <span class="comment"># number of outputs per anchor，每个类别的预测置信度+（预测类别+预测坐标）</span></span><br><span class="line">        self.nl = <span class="built_in">len</span>(anchors)  <span class="comment"># number of detection layers # nl: 表示预测层数，yolov5是3层预测</span></span><br><span class="line">        <span class="comment"># na: 表示anchors的数量，除以2是因为[10,13, 16,30, 33,23]这个长度是6，对应3个anchor</span></span><br><span class="line">        self.na = <span class="built_in">len</span>(anchors[<span class="number">0</span>]) // <span class="number">2</span>  <span class="comment"># number of anchors</span></span><br><span class="line">        <span class="comment"># grid: 表示初始化grid列表大小，下面会计算grid，grid就是每个格子的x，y坐标（整数，比如0-19），</span></span><br><span class="line">        <span class="comment"># 左上角为(1,1),右下角为(input.w/stride,input.h/stride)</span></span><br><span class="line">        self.grid = [torch.zeros(<span class="number">1</span>)] * self.nl  <span class="comment"># init grid</span></span><br><span class="line">        <span class="comment"># print(&quot;self.grid: &quot;, self.grid)</span></span><br><span class="line">        a = torch.tensor(anchors).<span class="built_in">float</span>().view(self.nl, -<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># shape(nl,na,2)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用register_buffer方法注册anchors和anchor_grid为模块的缓冲区（buffer），</span></span><br><span class="line">        <span class="comment"># 这样在模型进行训练时，这些参数将被包含在模型的状态中，并且在推理过程中不会被修改。</span></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;anchors&#x27;</span>, a)  <span class="comment"># shape(nl,na,2)</span></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;anchor_grid&#x27;</span>, a.clone().view(self.nl, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>))  <span class="comment"># shape(nl,1,na,1,1,2)</span></span><br><span class="line">        <span class="comment"># ch=(128,256,512),最后的3个1*1卷积</span></span><br><span class="line">        <span class="comment"># 每一张进行三次预测，每一个预测结果包含nc+5个值</span></span><br><span class="line">        <span class="comment"># (n, 255, 80, 80),(n, 255, 40, 40),(n, 255, 20, 20) --&gt; ch=(255, 255, 255)</span></span><br><span class="line">        <span class="comment"># 255 -&gt; (nc+5)*3 ===&gt; 为了提取出预测框的位置信息以及预测框尺寸信息</span></span><br><span class="line">        self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, <span class="number">1</span>) <span class="keyword">for</span> x <span class="keyword">in</span> ch)  <span class="comment"># output conv</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x = x.copy()  # for profiling</span></span><br><span class="line">        z = []  <span class="comment"># inference output</span></span><br><span class="line">        self.training |= self.export</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;self.nl&quot;</span>, self.nl)</span><br><span class="line">        <span class="comment"># 首先进行for循环，每次i的循环，产生一个z。</span></span><br><span class="line">        <span class="comment"># 维度重排列：(n, 255, , ) -&gt; (n, 3, nc+5, ny, nx) -&gt; (n, 3, ny, nx, nc+5)，</span></span><br><span class="line">        <span class="comment"># 三层分别预测了80*80、40*40、20*20次。</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.nl):</span><br><span class="line">            x[i] = self.m[i](x[i])  <span class="comment"># conv，3个output 1*1 conv</span></span><br><span class="line">            bs, _, ny, nx = x[i].shape  <span class="comment"># x(bs,255,20,20) to x(bs,3,20,20,85)</span></span><br><span class="line">            <span class="comment"># print(&quot;ny,nx :&quot;, ny, nx)</span></span><br><span class="line">            <span class="comment"># print(&quot;x[i]: &quot;, x[i].shape)   </span></span><br><span class="line">            <span class="comment"># 维度重排列: bs, 先验框组数, 检测框行数, 检测框列数, 属性数5 + 分类数</span></span><br><span class="line">            x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>).contiguous()      <span class="comment"># .contiguous()确保张量在储存中是连续的</span></span><br><span class="line">            <span class="comment"># print(&quot;x[i]: &quot;, x[i].shape)</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.training:  <span class="comment"># inference</span></span><br><span class="line">                <span class="keyword">if</span> self.grid[i].shape[<span class="number">2</span>:<span class="number">4</span>] != x[i].shape[<span class="number">2</span>:<span class="number">4</span>]:</span><br><span class="line">                    self.grid[i] = self._make_grid(nx, ny).to(x[i].device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># -------------------按损失函数的回归方式来转换坐标---------------------</span></span><br><span class="line">                y = x[i].sigmoid()</span><br><span class="line">                <span class="comment"># 对坐标进行解码，计算预测框的中心坐标。</span></span><br><span class="line">                y[..., <span class="number">0</span>:<span class="number">2</span>] = (y[..., <span class="number">0</span>:<span class="number">2</span>] * <span class="number">2.</span> - <span class="number">0.5</span> + self.grid[i]) * self.stride[i]  <span class="comment"># xy</span></span><br><span class="line">                <span class="comment"># 计算预测框的宽度和高度。</span></span><br><span class="line">                y[..., <span class="number">2</span>:<span class="number">4</span>] = (y[..., <span class="number">2</span>:<span class="number">4</span>] * <span class="number">2</span>) ** <span class="number">2</span> * self.anchor_grid[i]  <span class="comment"># wh</span></span><br><span class="line">                z.append(y.view(bs, -<span class="number">1</span>, self.no))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x <span class="keyword">if</span> self.training <span class="keyword">else</span> (torch.cat(z, <span class="number">1</span>), x)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_grid</span>(<span class="params">nx=<span class="number">20</span>, ny=<span class="number">20</span></span>):</span><br><span class="line">        yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)])</span><br><span class="line">        <span class="keyword">return</span> torch.stack((xv, yv), <span class="number">2</span>).view((<span class="number">1</span>, <span class="number">1</span>, ny, nx, <span class="number">2</span>)).<span class="built_in">float</span>()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>​分类和 bbox 检测等都是在同一个卷积的不同通道中完成，预测结果在通道维得到。</p>]]></content>
      
      
      
        <tags>
            
            <tag> yolo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/04/03/yolov5s-wang-luo-jie-gou/"/>
      <url>/2024/04/03/yolov5s-wang-luo-jie-gou/</url>
      
        <content type="html"><![CDATA[<h5 id="yolov5s-网络结构"><a href="#yolov5s-网络结构" class="headerlink" title="yolov5s 网络结构"></a>yolov5s 网络结构</h5><p>Parameter name: epoch    Parameter value:-1<br>Parameter name: best_fitness     Parameter value:[    0.65574]<br>Parameter name: training_results         Parameter value:None<br>Parameter name: model    Parameter value:Model(<br>  (model): Sequential(<br>    (0): Focus(<br>      (conv): Conv(<br>        (conv): Conv2d(12, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(32, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>    )<br>    (1): Conv(<br>      (conv): Conv2d(32, 64, kernel_size&#x3D;(3, 3), stride&#x3D;(2, 2), padding&#x3D;(1, 1), bias&#x3D;False)<br>      (bn): BatchNorm2d(64, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>      (act): SiLU()<br>    )<br>    (2): C3(<br>      (cv1): Conv(<br>        (conv): Conv2d(64, 32, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(32, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv2): Conv(<br>        (conv): Conv2d(64, 32, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(32, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv3): Conv(<br>        (conv): Conv2d(64, 64, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(64, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (m): Sequential(<br>        (0): Bottleneck(<br>          (cv1): Conv(<br>            (conv): Conv2d(32, 32, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(32, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>          (cv2): Conv(<br>            (conv): Conv2d(32, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(32, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>        )<br>      )<br>    )<br>    (3): Conv(<br>      (conv): Conv2d(64, 128, kernel_size&#x3D;(3, 3), stride&#x3D;(2, 2), padding&#x3D;(1, 1), bias&#x3D;False)<br>      (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>      (act): SiLU()<br>    )<br>    (4): C3(<br>      (cv1): Conv(<br>        (conv): Conv2d(128, 64, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(64, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv2): Conv(<br>        (conv): Conv2d(128, 64, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(64, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv3): Conv(<br>        (conv): Conv2d(128, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (m): Sequential(<br>        (0): Bottleneck(<br>          (cv1): Conv(<br>            (conv): Conv2d(64, 64, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(64, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>          (cv2): Conv(<br>            (conv): Conv2d(64, 64, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(64, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>        )<br>        (1): Bottleneck(<br>          (cv1): Conv(<br>            (conv): Conv2d(64, 64, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(64, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>          (cv2): Conv(<br>            (conv): Conv2d(64, 64, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(64, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>        )<br>        (2): Bottleneck(<br>          (cv1): Conv(<br>            (conv): Conv2d(64, 64, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(64, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>          (cv2): Conv(<br>            (conv): Conv2d(64, 64, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(64, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>        )<br>      )<br>    )<br>    (5): Conv(<br>      (conv): Conv2d(128, 256, kernel_size&#x3D;(3, 3), stride&#x3D;(2, 2), padding&#x3D;(1, 1), bias&#x3D;False)<br>      (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>      (act): SiLU()<br>    )<br>    (6): C3(<br>      (cv1): Conv(<br>        (conv): Conv2d(256, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv2): Conv(<br>        (conv): Conv2d(256, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv3): Conv(<br>        (conv): Conv2d(256, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (m): Sequential(<br>        (0): Bottleneck(<br>          (cv1): Conv(<br>            (conv): Conv2d(128, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>          (cv2): Conv(<br>            (conv): Conv2d(128, 128, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>        )<br>        (1): Bottleneck(<br>          (cv1): Conv(<br>            (conv): Conv2d(128, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>          (cv2): Conv(<br>            (conv): Conv2d(128, 128, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>        )<br>        (2): Bottleneck(<br>          (cv1): Conv(<br>            (conv): Conv2d(128, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>          (cv2): Conv(<br>            (conv): Conv2d(128, 128, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>        )<br>      )<br>    )<br>    (7): Conv(<br>      (conv): Conv2d(256, 512, kernel_size&#x3D;(3, 3), stride&#x3D;(2, 2), padding&#x3D;(1, 1), bias&#x3D;False)<br>      (bn): BatchNorm2d(512, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>      (act): SiLU()<br>    )<br>    (8): SPP(<br>      (cv1): Conv(<br>        (conv): Conv2d(512, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv2): Conv(<br>        (conv): Conv2d(1024, 512, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(512, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (m): ModuleList(<br>        (0): MaxPool2d(kernel_size&#x3D;5, stride&#x3D;1, padding&#x3D;2, dilation&#x3D;1, ceil_mode&#x3D;False)<br>        (1): MaxPool2d(kernel_size&#x3D;9, stride&#x3D;1, padding&#x3D;4, dilation&#x3D;1, ceil_mode&#x3D;False)<br>        (2): MaxPool2d(kernel_size&#x3D;13, stride&#x3D;1, padding&#x3D;6, dilation&#x3D;1, ceil_mode&#x3D;False)<br>      )<br>    )<br>    (9): C3(<br>      (cv1): Conv(<br>        (conv): Conv2d(512, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv2): Conv(<br>        (conv): Conv2d(512, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv3): Conv(<br>        (conv): Conv2d(512, 512, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(512, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (m): Sequential(<br>        (0): Bottleneck(<br>          (cv1): Conv(<br>            (conv): Conv2d(256, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>          (cv2): Conv(<br>            (conv): Conv2d(256, 256, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>        )<br>      )<br>    )<br>    (10): Conv(<br>      (conv): Conv2d(512, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>      (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>      (act): SiLU()<br>    )<br>    (11): Upsample(scale_factor&#x3D;2.0, mode&#x3D;’nearest’)<br>    (12): Concat()<br>    (13): C3(<br>      (cv1): Conv(<br>        (conv): Conv2d(512, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv2): Conv(<br>        (conv): Conv2d(512, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv3): Conv(<br>        (conv): Conv2d(256, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (m): Sequential(<br>        (0): Bottleneck(<br>          (cv1): Conv(<br>            (conv): Conv2d(128, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>          (cv2): Conv(<br>            (conv): Conv2d(128, 128, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>        )<br>      )<br>    )<br>    (14): Conv(<br>      (conv): Conv2d(256, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>      (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>      (act): SiLU()<br>    )<br>    (15): Upsample(scale_factor&#x3D;2.0, mode&#x3D;’nearest’)<br>    (16): Concat()<br>    (17): C3(<br>      (cv1): Conv(<br>        (conv): Conv2d(256, 64, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(64, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv2): Conv(<br>        (conv): Conv2d(256, 64, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(64, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv3): Conv(<br>        (conv): Conv2d(128, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (m): Sequential(<br>        (0): Bottleneck(<br>          (cv1): Conv(<br>            (conv): Conv2d(64, 64, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(64, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>          (cv2): Conv(<br>            (conv): Conv2d(64, 64, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(64, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>        )<br>      )<br>    )<br>    (18): Conv(<br>      (conv): Conv2d(128, 128, kernel_size&#x3D;(3, 3), stride&#x3D;(2, 2), padding&#x3D;(1, 1), bias&#x3D;False)<br>      (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>      (act): SiLU()<br>    )<br>    (19): Concat()<br>    (20): C3(<br>      (cv1): Conv(<br>        (conv): Conv2d(256, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv2): Conv(<br>        (conv): Conv2d(256, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv3): Conv(<br>        (conv): Conv2d(256, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (m): Sequential(<br>        (0): Bottleneck(<br>          (cv1): Conv(<br>            (conv): Conv2d(128, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>          (cv2): Conv(<br>            (conv): Conv2d(128, 128, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(128, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>        )<br>      )<br>    )<br>    (21): Conv(<br>      (conv): Conv2d(256, 256, kernel_size&#x3D;(3, 3), stride&#x3D;(2, 2), padding&#x3D;(1, 1), bias&#x3D;False)<br>      (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>      (act): SiLU()<br>    )<br>    (22): Concat()<br>    (23): C3(<br>      (cv1): Conv(<br>        (conv): Conv2d(512, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv2): Conv(<br>        (conv): Conv2d(512, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (cv3): Conv(<br>        (conv): Conv2d(512, 512, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>        (bn): BatchNorm2d(512, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>        (act): SiLU()<br>      )<br>      (m): Sequential(<br>        (0): Bottleneck(<br>          (cv1): Conv(<br>            (conv): Conv2d(256, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>          (cv2): Conv(<br>            (conv): Conv2d(256, 256, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)<br>            (bn): BatchNorm2d(256, eps&#x3D;0.001, momentum&#x3D;0.03, affine&#x3D;True, track_running_stats&#x3D;True)<br>            (act): SiLU()<br>          )<br>        )<br>      )<br>    )<br>    (24): Detect(<br>      (m): ModuleList(<br>        (0): Conv2d(128, 21, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1))<br>        (1): Conv2d(256, 21, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1))<br>        (2): Conv2d(512, 21, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1))<br>      )<br>    )<br>  )<br>)<br>Parameter name: ema      Parameter value:None<br>Parameter name: updates          Parameter value:None<br>Parameter name: optimizer        Parameter value:None<br>Parameter name: wandb_id         Parameter value:None</p><h5 id="各层参数的形状"><a href="#各层参数的形状" class="headerlink" title="各层参数的形状"></a>各层参数的形状</h5><p>model.0.conv.conv.weight         torch.Size([32, 12, 3, 3])<br>model.0.conv.conv.bias   torch.Size([32])<br>model.1.conv.weight      torch.Size([64, 32, 3, 3])<br>model.1.conv.bias        torch.Size([64])<br>model.2.cv1.conv.weight          torch.Size([32, 64, 1, 1])<br>model.2.cv1.conv.bias    torch.Size([32])<br>model.2.cv2.conv.weight          torch.Size([32, 64, 1, 1])<br>model.2.cv2.conv.bias    torch.Size([32])<br>model.2.cv3.conv.weight          torch.Size([64, 64, 1, 1])<br>model.2.cv3.conv.bias    torch.Size([64])<br>model.2.m.0.cv1.conv.weight      torch.Size([32, 32, 1, 1])<br>model.2.m.0.cv1.conv.bias        torch.Size([32])<br>model.2.m.0.cv2.conv.weight      torch.Size([32, 32, 3, 3])<br>model.2.m.0.cv2.conv.bias        torch.Size([32])<br>model.3.conv.weight      torch.Size([128, 64, 3, 3])<br>model.3.conv.bias        torch.Size([128])<br>model.4.cv1.conv.weight          torch.Size([64, 128, 1, 1])<br>model.4.cv1.conv.bias    torch.Size([64])<br>model.4.cv2.conv.weight          torch.Size([64, 128, 1, 1])<br>model.4.cv2.conv.bias    torch.Size([64])<br>model.4.cv3.conv.weight          torch.Size([128, 128, 1, 1])<br>model.4.cv3.conv.bias    torch.Size([128])<br>model.4.m.0.cv1.conv.weight      torch.Size([64, 64, 1, 1])<br>model.4.m.0.cv1.conv.bias        torch.Size([64])<br>model.4.m.0.cv2.conv.weight      torch.Size([64, 64, 3, 3])<br>model.4.m.0.cv2.conv.bias        torch.Size([64])<br>model.4.m.1.cv1.conv.weight      torch.Size([64, 64, 1, 1])<br>model.4.m.1.cv1.conv.bias        torch.Size([64])<br>model.4.m.1.cv2.conv.weight      torch.Size([64, 64, 3, 3])<br>model.4.m.1.cv2.conv.bias        torch.Size([64])<br>model.4.m.2.cv1.conv.weight      torch.Size([64, 64, 1, 1])<br>model.4.m.2.cv1.conv.bias        torch.Size([64])<br>model.4.m.2.cv2.conv.weight      torch.Size([64, 64, 3, 3])<br>model.4.m.2.cv2.conv.bias        torch.Size([64])<br>model.5.conv.weight      torch.Size([256, 128, 3, 3])<br>model.5.conv.bias        torch.Size([256])<br>model.6.cv1.conv.weight          torch.Size([128, 256, 1, 1])<br>model.6.cv1.conv.bias    torch.Size([128])<br>model.6.cv2.conv.weight          torch.Size([128, 256, 1, 1])<br>model.6.cv2.conv.bias    torch.Size([128])<br>model.6.cv3.conv.weight          torch.Size([256, 256, 1, 1])<br>model.6.cv3.conv.bias    torch.Size([256])<br>model.6.m.0.cv1.conv.weight      torch.Size([128, 128, 1, 1])<br>model.6.m.0.cv1.conv.bias        torch.Size([128])<br>model.6.m.0.cv2.conv.weight      torch.Size([128, 128, 3, 3])<br>model.6.m.0.cv2.conv.bias        torch.Size([128])<br>model.6.m.1.cv1.conv.weight      torch.Size([128, 128, 1, 1])<br>model.6.m.1.cv1.conv.bias        torch.Size([128])<br>model.6.m.1.cv2.conv.weight      torch.Size([128, 128, 3, 3])<br>model.6.m.1.cv2.conv.bias        torch.Size([128])<br>model.6.m.2.cv1.conv.weight      torch.Size([128, 128, 1, 1])<br>model.6.m.2.cv1.conv.bias        torch.Size([128])<br>model.6.m.2.cv2.conv.weight      torch.Size([128, 128, 3, 3])<br>model.6.m.2.cv2.conv.bias        torch.Size([128])<br>model.7.conv.weight      torch.Size([512, 256, 3, 3])<br>model.7.conv.bias        torch.Size([512])<br>model.8.cv1.conv.weight          torch.Size([256, 512, 1, 1])<br>model.8.cv1.conv.bias    torch.Size([256])<br>model.8.cv2.conv.weight          torch.Size([512, 1024, 1, 1])<br>model.8.cv2.conv.bias    torch.Size([512])<br>model.9.cv1.conv.weight          torch.Size([256, 512, 1, 1])<br>model.9.cv1.conv.bias    torch.Size([256])<br>model.9.cv2.conv.weight          torch.Size([256, 512, 1, 1])<br>model.9.cv2.conv.bias    torch.Size([256])<br>model.9.cv3.conv.weight          torch.Size([512, 512, 1, 1])<br>model.9.cv3.conv.bias    torch.Size([512])<br>model.9.m.0.cv1.conv.weight      torch.Size([256, 256, 1, 1])<br>model.9.m.0.cv1.conv.bias        torch.Size([256])<br>model.9.m.0.cv2.conv.weight      torch.Size([256, 256, 3, 3])<br>model.9.m.0.cv2.conv.bias        torch.Size([256])<br>model.10.conv.weight     torch.Size([256, 512, 1, 1])<br>model.10.conv.bias       torch.Size([256])<br>model.13.cv1.conv.weight         torch.Size([128, 512, 1, 1])<br>model.13.cv1.conv.bias   torch.Size([128])<br>model.13.cv2.conv.weight         torch.Size([128, 512, 1, 1])<br>model.13.cv2.conv.bias   torch.Size([128])<br>model.13.cv3.conv.weight         torch.Size([256, 256, 1, 1])<br>model.13.cv3.conv.bias   torch.Size([256])<br>model.13.m.0.cv1.conv.weight     torch.Size([128, 128, 1, 1])<br>model.13.m.0.cv1.conv.bias       torch.Size([128])<br>model.13.m.0.cv2.conv.weight     torch.Size([128, 128, 3, 3])<br>model.13.m.0.cv2.conv.bias       torch.Size([128])<br>model.14.conv.weight     torch.Size([128, 256, 1, 1])<br>model.14.conv.bias       torch.Size([128])<br>model.17.cv1.conv.weight         torch.Size([64, 256, 1, 1])<br>model.17.cv1.conv.bias   torch.Size([64])<br>model.17.cv2.conv.weight         torch.Size([64, 256, 1, 1])<br>model.17.cv2.conv.bias   torch.Size([64])<br>model.17.cv3.conv.weight         torch.Size([128, 128, 1, 1])<br>model.17.cv3.conv.bias   torch.Size([128])<br>model.17.m.0.cv1.conv.weight     torch.Size([64, 64, 1, 1])<br>model.17.m.0.cv1.conv.bias       torch.Size([64])<br>model.17.m.0.cv2.conv.weight     torch.Size([64, 64, 3, 3])<br>model.17.m.0.cv2.conv.bias       torch.Size([64])<br>model.18.conv.weight     torch.Size([128, 128, 3, 3])<br>model.18.conv.bias       torch.Size([128])<br>model.20.cv1.conv.weight         torch.Size([128, 256, 1, 1])<br>model.20.cv1.conv.bias   torch.Size([128])<br>model.20.cv2.conv.weight         torch.Size([128, 256, 1, 1])<br>model.20.cv2.conv.bias   torch.Size([128])<br>model.20.cv3.conv.weight         torch.Size([256, 256, 1, 1])<br>model.20.cv3.conv.bias   torch.Size([256])<br>model.20.m.0.cv1.conv.weight     torch.Size([128, 128, 1, 1])<br>model.20.m.0.cv1.conv.bias       torch.Size([128])<br>model.20.m.0.cv2.conv.weight     torch.Size([128, 128, 3, 3])<br>model.20.m.0.cv2.conv.bias       torch.Size([128])<br>model.21.conv.weight     torch.Size([256, 256, 3, 3])<br>model.21.conv.bias       torch.Size([256])<br>model.23.cv1.conv.weight         torch.Size([256, 512, 1, 1])<br>model.23.cv1.conv.bias   torch.Size([256])<br>model.23.cv2.conv.weight         torch.Size([256, 512, 1, 1])<br>model.23.cv2.conv.bias   torch.Size([256])<br>model.23.cv3.conv.weight         torch.Size([512, 512, 1, 1])<br>model.23.cv3.conv.bias   torch.Size([512])<br>model.23.m.0.cv1.conv.weight     torch.Size([256, 256, 1, 1])<br>model.23.m.0.cv1.conv.bias       torch.Size([256])<br>model.23.m.0.cv2.conv.weight     torch.Size([256, 256, 3, 3])<br>model.23.m.0.cv2.conv.bias       torch.Size([256])<br>model.24.m.0.weight      torch.Size([21, 128, 1, 1])<br>model.24.m.0.bias        torch.Size([21])<br>model.24.m.1.weight      torch.Size([21, 256, 1, 1])<br>model.24.m.1.bias        torch.Size([21])<br>model.24.m.2.weight      torch.Size([21, 512, 1, 1])<br>model.24.m.2.bias        torch.Size([21])</p><h5 id="输入为640-640-3时，各层输出形状"><a href="#输入为640-640-3时，各层输出形状" class="headerlink" title="输入为640* 640* 3时，各层输出形状"></a>输入为640* 640* 3时，各层输出形状</h5><h6 id="Layer-type-depth-idx-Output-Shape-Param"><a href="#Layer-type-depth-idx-Output-Shape-Param" class="headerlink" title="Layer (type:depth-idx)                        Output Shape              Param"></a>Layer (type:depth-idx)                        Output Shape              Param</h6><p>Model                                         [1, 25200, 7]             –<br>├─Sequential: 1-1                             –                        –<br>│    └─Focus: 2-1                             [1, 32, 320, 320]         –<br>│    │    └─Conv: 3-1                         [1, 32, 320, 320]         (3,488)<br>│    └─Conv: 2-2                              [1, 64, 160, 160]         –<br>│    │    └─Conv2d: 3-2                       [1, 64, 160, 160]         (18,496)<br>│    │    └─SiLU: 3-3                         [1, 64, 160, 160]         –<br>│    └─C3: 2-3                                [1, 64, 160, 160]         –<br>│    │    └─Conv: 3-4                         [1, 32, 160, 160]         (2,080)<br>│    │    └─Sequential: 3-5                   [1, 32, 160, 160]         (10,304)<br>│    │    └─Conv: 3-6                         [1, 32, 160, 160]         (2,080)<br>│    │    └─Conv: 3-7                         [1, 64, 160, 160]         (4,160)<br>│    └─Conv: 2-4                              [1, 128, 80, 80]          –<br>│    │    └─Conv2d: 3-8                       [1, 128, 80, 80]          (73,856)<br>│    │    └─SiLU: 3-9                         [1, 128, 80, 80]          –<br>│    └─C3: 2-5                                [1, 128, 80, 80]          –<br>│    │    └─Conv: 3-10                        [1, 64, 80, 80]           (8,256)<br>│    │    └─Sequential: 3-11                  [1, 64, 80, 80]           (123,264)<br>│    │    └─Conv: 3-12                        [1, 64, 80, 80]           (8,256)<br>│    │    └─Conv: 3-13                        [1, 128, 80, 80]          (16,512)<br>│    └─Conv: 2-6                              [1, 256, 40, 40]          –<br>│    │    └─Conv2d: 3-14                      [1, 256, 40, 40]          (295,168)<br>│    │    └─SiLU: 3-15                        [1, 256, 40, 40]          –<br>│    └─C3: 2-7                                [1, 256, 40, 40]          –<br>│    │    └─Conv: 3-16                        [1, 128, 40, 40]          (32,896)<br>│    │    └─Sequential: 3-17                  [1, 128, 40, 40]          (492,288)<br>│    │    └─Conv: 3-18                        [1, 128, 40, 40]          (32,896)<br>│    │    └─Conv: 3-19                        [1, 256, 40, 40]          (65,792)<br>│    └─Conv: 2-8                              [1, 512, 20, 20]          –<br>│    │    └─Conv2d: 3-20                      [1, 512, 20, 20]          (1,180,160)<br>│    │    └─SiLU: 3-21                        [1, 512, 20, 20]          –<br>│    └─SPP: 2-9                               [1, 512, 20, 20]          –<br>│    │    └─Conv: 3-22                        [1, 256, 20, 20]          (131,328)<br>│    │    └─ModuleList: 3-23                  –                        –<br>│    │    └─Conv: 3-24                        [1, 512, 20, 20]          (524,800)<br>│    └─C3: 2-10                               [1, 512, 20, 20]          –<br>│    │    └─Conv: 3-25                        [1, 256, 20, 20]          (131,328)<br>│    │    └─Sequential: 3-26                  [1, 256, 20, 20]          (655,872)<br>│    │    └─Conv: 3-27                        [1, 256, 20, 20]          (131,328)<br>│    │    └─Conv: 3-28                        [1, 512, 20, 20]          (262,656)<br>│    └─Conv: 2-11                             [1, 256, 20, 20]          –<br>│    │    └─Conv2d: 3-29                      [1, 256, 20, 20]          (131,328)<br>│    │    └─SiLU: 3-30                        [1, 256, 20, 20]          –<br>│    └─Upsample: 2-12                         [1, 256, 40, 40]          –<br>│    └─Concat: 2-13                           [1, 512, 40, 40]          –<br>│    └─C3: 2-14                               [1, 256, 40, 40]          –<br>│    │    └─Conv: 3-31                        [1, 128, 40, 40]          (65,664)<br>│    │    └─Sequential: 3-32                  [1, 128, 40, 40]          (164,096)<br>│    │    └─Conv: 3-33                        [1, 128, 40, 40]          (65,664)<br>│    │    └─Conv: 3-34                        [1, 256, 40, 40]          (65,792)<br>│    └─Conv: 2-15                             [1, 128, 40, 40]          –<br>│    │    └─Conv2d: 3-35                      [1, 128, 40, 40]          (32,896)<br>│    │    └─SiLU: 3-36                        [1, 128, 40, 40]          –<br>│    └─Upsample: 2-16                         [1, 128, 80, 80]          –<br>│    └─Concat: 2-17                           [1, 256, 80, 80]          –<br>│    └─C3: 2-18                               [1, 128, 80, 80]          –<br>│    │    └─Conv: 3-37                        [1, 64, 80, 80]           (16,448)<br>│    │    └─Sequential: 3-38                  [1, 64, 80, 80]           (41,088)<br>│    │    └─Conv: 3-39                        [1, 64, 80, 80]           (16,448)<br>│    │    └─Conv: 3-40                        [1, 128, 80, 80]          (16,512)<br>│    └─Conv: 2-19                             [1, 128, 40, 40]          –<br>│    │    └─Conv2d: 3-41                      [1, 128, 40, 40]          (147,584)<br>│    │    └─SiLU: 3-42                        [1, 128, 40, 40]          –<br>│    └─Concat: 2-20                           [1, 256, 40, 40]          –<br>│    └─C3: 2-21                               [1, 256, 40, 40]          –<br>│    │    └─Conv: 3-43                        [1, 128, 40, 40]          (32,896)<br>│    │    └─Sequential: 3-44                  [1, 128, 40, 40]          (164,096)<br>│    │    └─Conv: 3-45                        [1, 128, 40, 40]          (32,896)<br>│    │    └─Conv: 3-46                        [1, 256, 40, 40]          (65,792)<br>│    └─Conv: 2-22                             [1, 256, 20, 20]          –<br>│    │    └─Conv2d: 3-47                      [1, 256, 20, 20]          (590,080)<br>│    │    └─SiLU: 3-48                        [1, 256, 20, 20]          –<br>│    └─Concat: 2-23                           [1, 512, 20, 20]          –<br>│    └─C3: 2-24                               [1, 512, 20, 20]          –<br>│    │    └─Conv: 3-49                        [1, 256, 20, 20]          (131,328)<br>│    │    └─Sequential: 3-50                  [1, 256, 20, 20]          (655,872)<br>│    │    └─Conv: 3-51                        [1, 256, 20, 20]          (131,328)<br>│    │    └─Conv: 3-52                        [1, 512, 20, 20]          (262,656)<br>│    └─Detect: 2-25                           [1, 25200, 7]             –</p><h6 id="│-│-└─ModuleList-3-53-–-18-879"><a href="#│-│-└─ModuleList-3-53-–-18-879" class="headerlink" title="│    │    └─ModuleList: 3-53                  –                        (18,879)"></a>│    │    └─ModuleList: 3-53                  –                        (18,879)</h6>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/04/03/hello-world/"/>
      <url>/2024/04/03/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>YOLOv5网络结构详解</title>
      <link href="/2023/04/06/understanding-of-yolo-network-structure-1/"/>
      <url>/2023/04/06/understanding-of-yolo-network-structure-1/</url>
      
        <content type="html"><![CDATA[<h3 id="yolo网络结构详解"><a href="#yolo网络结构详解" class="headerlink" title="yolo网络结构详解"></a>yolo网络结构详解</h3><p>先来放上整个网络结构的示意图，图中所示为yolov5l的网络结构图</p><p><img src="/2023/04/06/understanding-of-yolo-network-structure-1/yolov5.jpeg" alt="yolov5l整体网络结构(部分参数不正确，以文中介绍为准)"></p><hr><h4 id="一、Backbone结构"><a href="#一、Backbone结构" class="headerlink" title="一、Backbone结构"></a>一、Backbone结构</h4><h5 id="1、Focus模块"><a href="#1、Focus模块" class="headerlink" title="1、Focus模块"></a>1、Focus模块</h5><p>​进入backbone之前，对原始输入图像进行切片操作，每隔一个像素取一个值，图像高宽减半，通道数变为原来4倍，信息基本没有丢失，以yolov5s为例，输入图像640 *640 *3，经过Focus结构，变为  320 *320 *12，如图：640 *640 *3</p><p><img src="/2023/04/06/understanding-of-yolo-network-structure-1/gzh\blog\myBlog\source_posts\understanding-of-yolo-network-structure-1\Focus.png" alt="focus示意图"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 展现在网络结构中就是先对输入图像进行切片操作，原始输入640* 640 *3，</span></span><br><span class="line"><span class="comment"># 切片后变为320* 320* 12</span></span><br><span class="line"><span class="comment"># 然后在经过两次卷积，640*640*3--&gt;320*320*12--&gt;320*320*64--&gt;160*160*128</span></span><br><span class="line">(<span class="number">0</span>): Focus(</span><br><span class="line">   (conv): Conv(</span><br><span class="line">     (conv): Conv2d(<span class="number">12</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">(<span class="number">1</span>): Conv(</span><br><span class="line">   (conv): Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line"><span class="comment"># 实现Focus模块的代码</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Focus</span>(nn.Module):</span><br><span class="line">    <span class="comment"># Focus wh information into c-space</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, c1, c2, k=<span class="number">1</span>, s=<span class="number">1</span>, p=<span class="literal">None</span>, g=<span class="number">1</span>, act=<span class="literal">True</span></span>):  <span class="comment"># ch_in, ch_out, kernel, stride, padding, groups</span></span><br><span class="line">        <span class="built_in">super</span>(Focus, self).__init__()</span><br><span class="line">        self.conv = Conv(c1 * <span class="number">4</span>, c2, k, s, p, g, act)</span><br><span class="line">        <span class="comment"># self.contract = Contract(gain=2)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):  <span class="comment"># x(b,c,w,h) -&gt; y(b,4c,w/2,h/2)</span></span><br><span class="line">        <span class="comment"># 对于4维张量x，...表示省略的维度，::2表示步长为2的切片。</span></span><br><span class="line">        <span class="comment"># 步长为2的切片后，在通道维度拼接起来，图像高宽减半，通道数变为原来4倍</span></span><br><span class="line">        <span class="keyword">return</span> self.conv(torch.cat([x[..., ::<span class="number">2</span>, ::<span class="number">2</span>], x[..., <span class="number">1</span>::<span class="number">2</span>, ::<span class="number">2</span>], x[..., ::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>], x[..., <span class="number">1</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>]], <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># return self.conv(self.contract(x))</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="2、BottleNeck模块"><a href="#2、BottleNeck模块" class="headerlink" title="2、BottleNeck模块"></a>2、BottleNeck模块</h5><p>下面两种BottleNeck模块分别用在模型的Backbone和neck部分，BottleNeck1用于Backbone，BottleNeck2用于neck，BottleNeck结构不改变图像高宽和通道数量。</p><p><img src="/2023/04/06/understanding-of-yolo-network-structure-1/gzh\blog\myBlog\source_posts\understanding-of-yolo-network-structure-1\BottleNeck.png" alt="BottleNeck示意图"></p><p>由一个1* 1卷积和3* 3卷积，再加上残差连接组成</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Bottleneck</span>(nn.Module):</span><br><span class="line">    <span class="comment"># Standard bottleneck</span></span><br><span class="line">    <span class="comment"># 残差链接块，shortcut表示是否包含捷径路线,即BottleNeck1和即BottleNeck2</span></span><br><span class="line">    <span class="comment"># 由1*1卷积、3*3卷积和残差连接组成</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, c1, c2, shortcut=<span class="literal">True</span>, g=<span class="number">1</span>, e=<span class="number">0.5</span></span>):  <span class="comment"># ch_in, ch_out, shortcut, groups, expansion</span></span><br><span class="line">        <span class="built_in">super</span>(Bottleneck, self).__init__()</span><br><span class="line">        c_ = <span class="built_in">int</span>(c2 * e)  <span class="comment"># hidden channels</span></span><br><span class="line">        self.cv1 = Conv(c1, c_, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.cv2 = Conv(c_, c2, <span class="number">3</span>, <span class="number">1</span>, g=g)</span><br><span class="line">        self.add = shortcut <span class="keyword">and</span> c1 == c2            <span class="comment"># 确保残差连接shortcut前后的通道数一致，保证能够可以相加</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 逐元素相加</span></span><br><span class="line">        <span class="keyword">return</span> x + self.cv2(self.cv1(x)) <span class="keyword">if</span> self.add <span class="keyword">else</span> self.cv2(self.cv1(x))</span><br><span class="line"><span class="comment">### 输入为160*160*64时，输出为160*160*64</span></span><br></pre></td></tr></table></figure><p>第一次进入BottleNeck1，图像形状为160* 160* 64，输出160* 160* 64</p><h5 id="3、C3模块"><a href="#3、C3模块" class="headerlink" title="3、C3模块"></a>3、C3模块</h5><p>如图，C3模块由1* 1卷积和BottleNeck模块组成，构成了yolov5的核心组成部分。整体来说，C3模块先对输入图像做通道数减半的1*1卷积并分支，其中一个分支经过若干个BottleNeck, 然后两个分支在通道维concat，concat后得到的输出通道就和输入通道数一样了，最后再来一个通道数不变的</p><p><img src="/2023/04/06/understanding-of-yolo-network-structure-1/gzh\blog\myBlog\source_posts\understanding-of-yolo-network-structure-1\C3.png" alt="C3模块示意图"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">C3</span>(nn.Module):</span><br><span class="line">    <span class="comment"># CSP Bottleneck with 3 convolutions</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, c1, c2, n=<span class="number">1</span>, shortcut=<span class="literal">True</span>, g=<span class="number">1</span>, e=<span class="number">0.5</span></span>):  <span class="comment"># ch_in, ch_out, number, shortcut, groups, expansion</span></span><br><span class="line">        <span class="built_in">super</span>(C3, self).__init__()</span><br><span class="line">        c_ = <span class="built_in">int</span>(c2 * e)  <span class="comment"># hidden channels</span></span><br><span class="line">        self.cv1 = Conv(c1, c_, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.cv2 = Conv(c1, c_, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.cv3 = Conv(<span class="number">2</span> * c_, c2, <span class="number">1</span>)  <span class="comment"># act=FReLU(c2)</span></span><br><span class="line">        self.m = nn.Sequential(*[Bottleneck(c_, c_, shortcut, g, e=<span class="number">1.0</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)])</span><br><span class="line">        <span class="comment"># self.m = nn.Sequential(*[CrossConv(c_, c_, 3, 1, g, 1.0, shortcut) for _ in range(n)])</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 先对输入图像x进行cv1卷积，再来n个Bottleneck块，再和经过cv2卷积的输入图像x在通道维拼接，再经过cv3卷积</span></span><br><span class="line">        <span class="keyword">return</span> self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), dim=<span class="number">1</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;&#x27;&#x27;&#x27;</span></span><br><span class="line">经过C3模块，图像由之前的<span class="number">160</span>*<span class="number">160</span>*<span class="number">128</span>--&gt;(cv1)<span class="number">160</span>*<span class="number">160</span>*<span class="number">64</span>--&gt;(<span class="number">3</span>个BottleNeck)<span class="number">160</span>*<span class="number">160</span>*<span class="number">64</span></span><br><span class="line">  --&gt;(cv2)<span class="number">160</span>*<span class="number">160</span>*<span class="number">64</span>--&gt;</span><br><span class="line">    concat--&gt;<span class="number">160</span>*<span class="number">160</span>*<span class="number">128</span>--&gt;(cv3)<span class="number">160</span>*<span class="number">160</span>*<span class="number">128</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;&#x27;&#x27;&#x27;</span></span><br><span class="line">图像经过一个C3模块，大小和通道数都不发生改变</span><br></pre></td></tr></table></figure><h5 id="4、后续网络走向"><a href="#4、后续网络走向" class="headerlink" title="4、后续网络走向"></a>4、后续网络走向</h5><ul><li>经过第一个C3模块，输出图像为160* 160* 128，然后一个卷积，变为80* 80* 256，此时得到的特征图称为P3</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">3</span>): Conv(</span><br><span class="line">  (conv): Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (act): SiLU(inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><ul><li><p>然后第二个C3模块，输入图像是80* 80* 256，输出图像也为80* 80* 256，然后一个卷积，变为40* 40* 512，此时得到的特征图称为P4</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 第二个C3模块有9个BottleNeck</span><br><span class="line">    (5): Conv(</span><br><span class="line">      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))</span><br><span class="line">      (act): SiLU(inplace=True)</span><br></pre></td></tr></table></figure></li><li><p>然后第三个C3模块，输入图像是40* 40* 512，输出图像也是40* 40* 512，经过一个卷积，变为20* 20* 1024，此时得到的特征图称为P5</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第三个C3模块有9个BOttleNeck</span></span><br><span class="line">    (<span class="number">7</span>): Conv(</span><br><span class="line">      (conv): Conv2d(<span class="number">512</span>, <span class="number">1024</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (act): SiLU(inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></li><li><p>然后第四个C3模块（3个BottleNeck），输出图像是20* 20* 1024，输出图像也为20* 20* 1024，与之前不同的是这里并没有一个卷积使其图像高宽减半，通道数加倍，而是接一个<strong>SPPF</strong>模块</p></li></ul><h5 id="5、SPPF"><a href="#5、SPPF" class="headerlink" title="5、SPPF"></a>5、<strong>SPPF</strong></h5><p>​SSPF模块将经过CBS的x、一次池化后的y1、两次池化后的y2和3次池化后的self.m(y2)先进行拼接，然后再CBS提取特征。 仔细观察不难发现，虽然SSPF对特征图进行了多次池化，但是特征图尺寸并未发生变化，通道数更不会变化，所以后续的4个输出能够在channel维度进行融合。这一模块的主要作用是对高层特征进行提取并融合，在融合的过程中作者多次运用最大池化，尽可能多的去提取高层次的语义特征。</p><p><img src="/2023/04/06/understanding-of-yolo-network-structure-1/gzh\blog\myBlog\source_posts\understanding-of-yolo-network-structure-1\SSPF.jpg" alt="SSPF示意图"></p><p>​第三个C3模块结束后，进入SSPF模块，输入为20* 20* 20* 1024，先经过一个卷积cv1，       变为20* 20* 512，然后顺序经过kernel_size为5、9、13的最大池化层并依次输出，图像大小和高宽均不发生变化，最后将这四个输出在通道维concat起来，四个20* 20 <em>512，拼接起来得到20 * 20</em> 2048，然后经过cv2，变为20* 20* 1024</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">(8): SPP(</span><br><span class="line">  (cv1): Conv(</span><br><span class="line">    (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">    (act): SiLU(inplace=True)</span><br><span class="line">  )</span><br><span class="line">  (cv2): Conv(</span><br><span class="line">    (conv): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">    (act): SiLU(inplace=True)</span><br><span class="line">  )</span><br><span class="line">  (m): ModuleList(</span><br><span class="line">    (0): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)</span><br><span class="line">    (1): MaxPool2d(kernel_size=9, stride=1, padding=4, dilation=1, ceil_mode=False)</span><br><span class="line">    (2): MaxPool2d(kernel_size=13, stride=1, padding=6, dilation=1, ceil_mode=False)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>​至此，整个BackBone结构完毕，接下来分别是从第二个C3模块结束、第三个C3模块结束、SPPF模块结束引出的连接BackBone和Head(检测层)的Neck</p><h4 id="二、Neck结构"><a href="#二、Neck结构" class="headerlink" title="二、Neck结构"></a>二、Neck结构</h4><p>按照整体网络结构图中由下至上的顺序</p><h5 id="1、SPPF模块引出的neck"><a href="#1、SPPF模块引出的neck" class="headerlink" title="1、SPPF模块引出的neck"></a>1、SPPF模块引出的neck</h5><p>​经过SPPF模块后，输出图像为20* 20* 1024，先经过一个卷积，通道数减半，变为20* 20* 512（记为<strong>n1</strong>,后面会用到），然后最近邻上采样，高宽加倍变为40* 40* 512，然后和第三个C3模块结束后的输出concat后变为40* 40* 1024，然后经过neck部分左下的C3模块，这里的C3会使输出通道数减半，高宽不变，输出40* 40* 512</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">10</span>): Conv(</span><br><span class="line">      (conv): Conv2d(<span class="number">1024</span>, <span class="number">512</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">11</span>): Upsample(scale_factor=<span class="number">2.0</span>, mode=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line">    (<span class="number">12</span>): Concat()</span><br><span class="line">    (<span class="number">13</span>): C3(</span><br><span class="line">      (cv1): Conv(</span><br><span class="line">        (conv): Conv2d(<span class="number">1024</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (cv2): Conv(</span><br><span class="line">        (conv): Conv2d(<span class="number">1024</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (cv3): Conv(</span><br><span class="line">        (conv): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      <span class="comment"># 这里BottleNeck中的卷积是256，图中有误</span></span><br><span class="line">      (m): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Bottleneck(</span><br><span class="line">          (cv1): Conv(</span><br><span class="line">            (conv): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (cv2): Conv(</span><br><span class="line">            (conv): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">1</span>): Bottleneck(</span><br><span class="line">          (cv1): Conv(</span><br><span class="line">            (conv): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (cv2): Conv(</span><br><span class="line">            (conv): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">2</span>): Bottleneck(</span><br><span class="line">          (cv1): Conv(</span><br><span class="line">            (conv): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (cv2): Conv(</span><br><span class="line">            (conv): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>​得到输出40* 40* 512后，经过一个卷积，通道数减半，得到40* 40* 256（记为<strong>n2</strong>), 然后一个最近邻上采样，得到80* 80* 256，然后和第二个C3模块引出的neck拼接在一起，得到80* 80* 512</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">14</span>): Conv(</span><br><span class="line">  (conv): Conv2d(<span class="number">512</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">)</span><br><span class="line">(<span class="number">15</span>): Upsample(scale_factor=<span class="number">2.0</span>, mode=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line">(<span class="number">16</span>): Concat()</span><br></pre></td></tr></table></figure><p>​然后经过neck中左上角的C3模块，和左下角的C3模块一样，输出的图像高宽不变，通道数减半，关键是进入C3分支的两个卷积的输出通道都缩小了1&#x2F;2（相比BackBone里的C3模块），输入为80* 80* 512，得到输出&#x3D;&#x3D;80* 80* 256（记为<strong>n3</strong>)，同时也是<strong>head1</strong>, 进入检测层&#x3D;&#x3D;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">17</span>): C3(</span><br><span class="line">  (cv1): Conv(</span><br><span class="line">    (conv): Conv2d(<span class="number">512</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">  (cv2): Conv(</span><br><span class="line">    (conv): Conv2d(<span class="number">512</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">  (cv3): Conv(</span><br><span class="line">    (conv): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">  (m): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Bottleneck(</span><br><span class="line">      (cv1): Conv(</span><br><span class="line">        (conv): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (cv2): Conv(</span><br><span class="line">        (conv): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">1</span>): Bottleneck(</span><br><span class="line">      (cv1): Conv(</span><br><span class="line">        (conv): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (cv2): Conv(</span><br><span class="line">        (conv): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">2</span>): Bottleneck(</span><br><span class="line">      (cv1): Conv(</span><br><span class="line">        (conv): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (cv2): Conv(</span><br><span class="line">        (conv): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (act): SiLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>​通过左上角的C3模块后，得到80* 80* 256的输出，然后进入一个卷积，通道数不变，高宽减半，输出为40* 40* 256，然后和<strong>n2</strong>的40* 40* 256concat，得到40* 40* 512，然后进入右上角的C3模块，右上角的C3模块和BackBone里的一样，不改变图像高宽和通道数，通过右上角的C3模块，得到输出为&#x3D;&#x3D;40* 40* 512，也即<strong>head2</strong>，进入检测层&#x3D;&#x3D;</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">(20): C3(</span><br><span class="line">      (cv1): Conv(</span><br><span class="line">        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">        (act): SiLU(inplace=True)</span><br><span class="line">      )</span><br><span class="line">      (cv2): Conv(</span><br><span class="line">        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">        (act): SiLU(inplace=True)</span><br><span class="line">      )</span><br><span class="line">      (cv3): Conv(</span><br><span class="line">        (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">        (act): SiLU(inplace=True)</span><br><span class="line">      )</span><br><span class="line">      (m): Sequential(</span><br><span class="line">        (0): Bottleneck(</span><br><span class="line">          (cv1): Conv(</span><br><span class="line">            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">            (act): SiLU(inplace=True)</span><br><span class="line">          )</span><br><span class="line">          (cv2): Conv(</span><br><span class="line">            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">            (act): SiLU(inplace=True)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">        (1): Bottleneck(</span><br><span class="line">          (cv1): Conv(</span><br><span class="line">            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">            (act): SiLU(inplace=True)</span><br><span class="line">          )</span><br><span class="line">          (cv2): Conv(</span><br><span class="line">            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">            (act): SiLU(inplace=True)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">        (2): Bottleneck(</span><br><span class="line">          (cv1): Conv(</span><br><span class="line">            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">            (act): SiLU(inplace=True)</span><br><span class="line">          )</span><br><span class="line">          (cv2): Conv(</span><br><span class="line">            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">            (act): SiLU(inplace=True)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>​通过右上角的C3模块后，输出为40* 40* 512，然后先经过一个卷积，使图像高宽减半、通道数不变，得到20* 20* 512，然后和<strong>n1</strong>的20* 20* 512concat得到20* 20* 1024，然后进入neck的右下角的C3模块，这个C3模块和BackBone里的一样，不改变图像高宽和通道数，通过C3模块后得到输出为&#x3D;&#x3D;20* 20* 1024，也即<strong>head3</strong>，进入检测层&#x3D;&#x3D;</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">(21): Conv(</span><br><span class="line">  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))</span><br><span class="line">  (act): SiLU(inplace=True)</span><br><span class="line">)</span><br><span class="line">(22): Concat()</span><br><span class="line">(23): C3(</span><br><span class="line">  (cv1): Conv(</span><br><span class="line">    (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">    (act): SiLU(inplace=True)</span><br><span class="line">  )</span><br><span class="line">  (cv2): Conv(</span><br><span class="line">    (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">    (act): SiLU(inplace=True)</span><br><span class="line">  )</span><br><span class="line">  (cv3): Conv(</span><br><span class="line">    (conv): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">    (act): SiLU(inplace=True)</span><br><span class="line">  )</span><br><span class="line">  (m): Sequential(</span><br><span class="line">    (0): Bottleneck(</span><br><span class="line">      (cv1): Conv(</span><br><span class="line">        (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">        (act): SiLU(inplace=True)</span><br><span class="line">      )</span><br><span class="line">      (cv2): Conv(</span><br><span class="line">        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        (act): SiLU(inplace=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Bottleneck(</span><br><span class="line">      (cv1): Conv(</span><br><span class="line">        (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">        (act): SiLU(inplace=True)</span><br><span class="line">      )</span><br><span class="line">      (cv2): Conv(</span><br><span class="line">        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        (act): SiLU(inplace=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (2): Bottleneck(</span><br><span class="line">      (cv1): Conv(</span><br><span class="line">        (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">        (act): SiLU(inplace=True)</span><br><span class="line">      )</span><br><span class="line">      (cv2): Conv(</span><br><span class="line">        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        (act): SiLU(inplace=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="三、Head结构"><a href="#三、Head结构" class="headerlink" title="三、Head结构"></a>三、Head结构</h4><p>​经过前面的BackBone结构和Neck结构，得到head1（80* 80* 256）、head2（40* 40* 512）、head3（20* 20* 1024），3个head分别通过3个卷积变为80* 80* 21，40* 40* 21、20* 20* 21。为什么是21？因为我的分类类别数nc&#x3D;2，这里的输出通道数应为3*（nc+5）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">    (<span class="number">24</span>): Detect(</span><br><span class="line">      (m): ModuleList(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">256</span>, <span class="number">21</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">1</span>): Conv2d(<span class="number">512</span>, <span class="number">21</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">2</span>): Conv2d(<span class="number">1024</span>, <span class="number">21</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">)&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Detect</span>(nn.Module):</span><br><span class="line">    stride = <span class="literal">None</span>  <span class="comment"># strides computed during build，特征图的缩放步长</span></span><br><span class="line">    export = <span class="literal">False</span>  <span class="comment"># onnx export，ONNX动态量化</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, nc=<span class="number">80</span>, anchors=(<span class="params"></span>), ch=(<span class="params"></span>)</span>):  <span class="comment"># detection layer</span></span><br><span class="line">        <span class="built_in">super</span>(Detect, self).__init__()</span><br><span class="line">        self.nc = nc  <span class="comment"># number of classes</span></span><br><span class="line">        self.no = nc + <span class="number">5</span>  <span class="comment"># number of outputs per anchor，每个类别的预测置信度+（预测类别+预测坐标）</span></span><br><span class="line">        self.nl = <span class="built_in">len</span>(anchors)  <span class="comment"># number of detection layers # nl: 表示预测层数，yolov5是3层预测</span></span><br><span class="line">        <span class="comment"># na: 表示anchors的数量，除以2是因为[10,13, 16,30, 33,23]这个长度是6，对应3个anchor</span></span><br><span class="line">        self.na = <span class="built_in">len</span>(anchors[<span class="number">0</span>]) // <span class="number">2</span>  <span class="comment"># number of anchors</span></span><br><span class="line">        <span class="comment"># grid: 表示初始化grid列表大小，下面会计算grid，grid就是每个格子的x，y坐标（整数，比如0-19），</span></span><br><span class="line">        <span class="comment"># 左上角为(1,1),右下角为(input.w/stride,input.h/stride)</span></span><br><span class="line">        self.grid = [torch.zeros(<span class="number">1</span>)] * self.nl  <span class="comment"># init grid</span></span><br><span class="line">        <span class="comment"># print(&quot;self.grid: &quot;, self.grid)</span></span><br><span class="line">        a = torch.tensor(anchors).<span class="built_in">float</span>().view(self.nl, -<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># shape(nl,na,2)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用register_buffer方法注册anchors和anchor_grid为模块的缓冲区（buffer），</span></span><br><span class="line">        <span class="comment"># 这样在模型进行训练时，这些参数将被包含在模型的状态中，并且在推理过程中不会被修改。</span></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;anchors&#x27;</span>, a)  <span class="comment"># shape(nl,na,2)</span></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;anchor_grid&#x27;</span>, a.clone().view(self.nl, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>))  <span class="comment"># shape(nl,1,na,1,1,2)</span></span><br><span class="line">        <span class="comment"># ch=(128,256,512),最后的3个1*1卷积</span></span><br><span class="line">        <span class="comment"># 每一张进行三次预测，每一个预测结果包含nc+5个值</span></span><br><span class="line">        <span class="comment"># (n, 255, 80, 80),(n, 255, 40, 40),(n, 255, 20, 20) --&gt; ch=(255, 255, 255)</span></span><br><span class="line">        <span class="comment"># 255 -&gt; (nc+5)*3 ===&gt; 为了提取出预测框的位置信息以及预测框尺寸信息</span></span><br><span class="line">        self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, <span class="number">1</span>) <span class="keyword">for</span> x <span class="keyword">in</span> ch)  <span class="comment"># output conv</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x = x.copy()  # for profiling</span></span><br><span class="line">        z = []  <span class="comment"># inference output</span></span><br><span class="line">        self.training |= self.export</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;self.nl&quot;</span>, self.nl)</span><br><span class="line">        <span class="comment"># 首先进行for循环，每次i的循环，产生一个z。</span></span><br><span class="line">        <span class="comment"># 维度重排列：(n, 255, , ) -&gt; (n, 3, nc+5, ny, nx) -&gt; (n, 3, ny, nx, nc+5)，</span></span><br><span class="line">        <span class="comment"># 三层分别预测了80*80、40*40、20*20次。</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.nl):</span><br><span class="line">            x[i] = self.m[i](x[i])  <span class="comment"># conv，3个output 1*1 conv</span></span><br><span class="line">            bs, _, ny, nx = x[i].shape  <span class="comment"># x(bs,255,20,20) to x(bs,3,20,20,85)</span></span><br><span class="line">            <span class="comment"># print(&quot;ny,nx :&quot;, ny, nx)</span></span><br><span class="line">            <span class="comment"># print(&quot;x[i]: &quot;, x[i].shape)   </span></span><br><span class="line">            <span class="comment"># 维度重排列: bs, 先验框组数, 检测框行数, 检测框列数, 属性数5 + 分类数</span></span><br><span class="line">            x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>).contiguous()      <span class="comment"># .contiguous()确保张量在储存中是连续的</span></span><br><span class="line">            <span class="comment"># print(&quot;x[i]: &quot;, x[i].shape)</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.training:  <span class="comment"># inference</span></span><br><span class="line">                <span class="keyword">if</span> self.grid[i].shape[<span class="number">2</span>:<span class="number">4</span>] != x[i].shape[<span class="number">2</span>:<span class="number">4</span>]:</span><br><span class="line">                    self.grid[i] = self._make_grid(nx, ny).to(x[i].device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># -------------------按损失函数的回归方式来转换坐标---------------------</span></span><br><span class="line">                y = x[i].sigmoid()</span><br><span class="line">                <span class="comment"># 对坐标进行解码，计算预测框的中心坐标。</span></span><br><span class="line">                y[..., <span class="number">0</span>:<span class="number">2</span>] = (y[..., <span class="number">0</span>:<span class="number">2</span>] * <span class="number">2.</span> - <span class="number">0.5</span> + self.grid[i]) * self.stride[i]  <span class="comment"># xy</span></span><br><span class="line">                <span class="comment"># 计算预测框的宽度和高度。</span></span><br><span class="line">                y[..., <span class="number">2</span>:<span class="number">4</span>] = (y[..., <span class="number">2</span>:<span class="number">4</span>] * <span class="number">2</span>) ** <span class="number">2</span> * self.anchor_grid[i]  <span class="comment"># wh</span></span><br><span class="line">                z.append(y.view(bs, -<span class="number">1</span>, self.no))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x <span class="keyword">if</span> self.training <span class="keyword">else</span> (torch.cat(z, <span class="number">1</span>), x)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_grid</span>(<span class="params">nx=<span class="number">20</span>, ny=<span class="number">20</span></span>):</span><br><span class="line">        yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)])</span><br><span class="line">        <span class="keyword">return</span> torch.stack((xv, yv), <span class="number">2</span>).view((<span class="number">1</span>, <span class="number">1</span>, ny, nx, <span class="number">2</span>)).<span class="built_in">float</span>()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>​分类和 bbox 检测等都是在同一个卷积的不同通道中完成，预测结果在通道维得到。</p>]]></content>
      
      
      <categories>
          
          <category> deep learing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> yolo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
